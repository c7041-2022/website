[
  {
    "objectID": "lab10-sample-size.html#objectives",
    "href": "lab10-sample-size.html#objectives",
    "title": "Lab 10 Sample size",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nEvaluate principles to increase sampling precision in experimental design\nEvaluate power of sampling\nDemonstrate power tools in R, like {pwr}\n\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab10-sample-size.html#start-a-script",
    "href": "lab10-sample-size.html#start-a-script",
    "title": "Lab 10 Sample size",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab10-sample-size.html#experiment-planning-tools",
    "href": "lab10-sample-size.html#experiment-planning-tools",
    "title": "Lab 10 Sample size",
    "section": "3 Experiment planning tools",
    "text": "3 Experiment planning tools\n\nBefore carrying out a time- and fund-consuming experiment, it is useful to get an idea of what to expect from the results. How big an effect are you expecting? What are the chances that you would detect it? What sample size would you need to have a reasonable chance of succeeding? How narrow a confidence interval around the estimated effect would you be happy with? In this lab we will show how R can be used to address some of these questions."
  },
  {
    "objectID": "lab10-sample-size.html#random-sampling-warm-up",
    "href": "lab10-sample-size.html#random-sampling-warm-up",
    "title": "Lab 10 Sample size",
    "section": "4 Random sampling warm-up",
    "text": "4 Random sampling warm-up\n\nTo begin, let’s get some practice sampling (randomly) categorical and Gaussian-distributed data from a population. The intention is to use sample\n\n\nRandomly sample 20 observations from a population having two groups of individuals, “infected” and “uninfected”, in equal proportions. Summarize the results in a frequency table.\nRepeat the previous step five times to convince yourself that the outcome varies from sample to sample.\nSample 18 individuals from a population having two groups of individuals, “mated” and “unmated”, where the proportion mated in the population is 0.7. Summarize the results in a frequency table.\nRepeat the previous step five times to convince yourself that the outcome varies from sample to sample.\nSample 30 observations from a Gaussian-distributed population having mean 0 and standard deviation 2. Plot the results in a histogram.\nRepeat the following 5 times and calculate the mean each time: sample 30 observations from a Gaussian-distributed population having mean 0 and standard deviation 2. Convince yourself that the sample mean is different each time.\n\n\n\n4.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\n# 1.\n\n# run this several times in your own script \n# and see how the output changes\nsample(c(\"infected\",\"uninfected\"), \n       prob = c(.5,.5),\n       size = 20, \n       replace = TRUE)\n\n [1] \"uninfected\" \"uninfected\" \"infected\"   \"infected\"   \"infected\"  \n [6] \"uninfected\" \"uninfected\" \"uninfected\" \"infected\"   \"uninfected\"\n[11] \"uninfected\" \"infected\"   \"infected\"   \"infected\"   \"infected\"  \n[16] \"infected\"   \"infected\"   \"infected\"   \"infected\"   \"infected\"  \n\n# 3. \nz <- sample(c(\"mated\",\"unmated\"), size = 18, replace = TRUE, prob = c(.7,.3))\n\ntable(z)\n\nz\n  mated unmated \n     13       5 \n\n# 5. \n\nz <- rnorm(30, mean = 0, sd = 2)\nhist(z, right = FALSE, col = \"goldenrod\", las = 1)\n\n\n\n\n\n\n# 6. \nz <- rnorm(1000, mean = 0, sd = 2)\n\nhist(z, right = FALSE, col = \"goldenrod\", las = 1)"
  },
  {
    "objectID": "lab10-sample-size.html#plan-for-precision",
    "href": "lab10-sample-size.html#plan-for-precision",
    "title": "Lab 10 Sample size",
    "section": "5 Plan for precision",
    "text": "5 Plan for precision\nConsider an experiment to estimate mate preference of females of a species of jumping spider. Each independent trial involves presenting a female spider with two tethered males. One of the males is from her own species, and the other is from its sister species. To avoid pseudoreplication, females are tested only once and males are replaced between tests. You want to estimate p, the proportion of female spiders that choose males of their own species. Before carrying out the experiment, it is useful to generate data under different scenarios to get a sense of the sample size you would need to estimate preference with sufficient precision.\n\n\n5.1 Estimate weak or no preference\nWe’ll start with the case of weak or no preference: Imagine that females choose males essentially randomly (p = 0.5), with half choosing the male from her own species and the other half picking the male of the other species. How much data would you need to demonstrate this (and convince your skeptical supervisory committee)? One idea is to collect data and use it to test the null hypothesis of no preference. If the null hypothesis is true, you should fail to reject it. However, this won’t be very convincing to your committee. Failing to reject a null hypothesis is inconclusive by itself. Maybe your test won’t have much power.\nA second idea is to plan your sample size so as to obtain a narrow confidence interval (i.e. having high precision) for the strength of preference. If, at the end of your experiment, you end up with an estimate of p close to 0.5 AND your 95% confidence interval for p is relatively narrow, you’ll be in a strong position to say that the true preference really is weak, even if you can’t say it is exactly 0.5. What sample size is necessary to achieve a reasonably narrow confidence interval in this case? Investigate this question by simulating data."
  },
  {
    "objectID": "lab10-sample-size.html#questions",
    "href": "lab10-sample-size.html#questions",
    "title": "Lab 10 Sample size",
    "section": "6 Questions",
    "text": "6 Questions\n\nRandomly sample n = 10 females from a population having equal numbers of “successes” (females who choose males of her own species) and “failures” (females who choose males of the other species). What was the proportion of successes in your sample?\nUsing the data from step 1, calculate an approximate 95% confidence interval for the population proportion of successes. Use the Agresti-Coull method in the binom package in R, which you will need to install if you haven’t already done so.\n\n\n\n# if(!require(\"binom\")) install.packages(\"binom\")\nlibrary(binom)\n\n\nTo obtain the 95% confidence interval, use the binom.confint function explained below. The argument x is the number of “successes” in your generated sample (number of females who chose males of her own species) and n is the sample size (number of females tested).\n\n\n# gets the confidence interval\nn <- 50 # number of trials\nx <- 27 # number of successes\nmyCI <- binom.confint(x, n, method = \"ac\")  \n\nprint(myCI)   # shows the results\n\n         method  x  n mean     lower     upper\n1 agresti-coull 27 50 0.54 0.4039603 0.6703319\n\nmyCI$lower    # lower limit of confidence interval\n\n[1] 0.4039603\n\nmyCI$upper    # upper limit\n\n[1] 0.6703319\n\n\n\nObtain the 95% confidence interval for the proportion using your data from step 1. What was the span of your confidence interval (upper limit minus lower limit)? Can you be confident that the true population proportion is 0.5 or very close to it?\n\n\nRepeat steps 1 and 2 five times and keep a record of the confidence intervals you obtained. What was the lowest value for the span of the confidence interval from the 5 samples?\n\n\n\nYou can speed up the effort if you create a for loop in R that automatically repeats steps 1 and 2 as many times as you decide. A loop that repeats ten times would look something like the following. The “i” in this loop is a counter, starting at 1 and increasing by 1 each time the commands in the loop are executed. Don’t forget to include a command inside the loop to print each result.\n\n\n\nfor(i in 1:10) {\n#  [paste in your R commands for steps 1 and 2 here]\n}\n\n\n\nIncrease the sample size to n = 20 and run the loop from step 4 again. How much narrower are the confidence interval spans? Are the spans adequate?\n\n\n\nBy modifying the sample size and re-running the loop a bunch of times, find a sample size (ballpark, no need to be exact at this point) that usually produces a confidence interval having a span no greater than 0.2. This would be the span of a confidence interval that had, e.g., a lower limit of 0.4 and an upper limit of 0.6. Surely this would be convincing evidence that the mate preference really was weak.\n\nBy this point you might wish to speed things up by saving the results of each iteration to a vector or data frame rather than print the results to the screen. This will make it possible to increase the number of iterations (say, to 100 times instead of just 10) for a given value of n.\n\n\nGiven the results of step 6, you would now have some design options before you. Is the sample size n that your simulation indicated was needed to generate a confidence interval of span 0.2 realistic? In other words, would an experiment with so many female spiders (and so many males) be feasible? If the answer is yes, great, get started on your experiment! If the answer is no, the sample size required is unrealistically large, then you have some decisions to make:\n\n\nForget all about doing the experiment. (Consider a thesis based on theory instead.)\nRevise your concept of what represents a “narrow” confidence interval. Maybe a confidence interval for p spanning, say, 0.3 to 0.7 (a span of 0.4) would be good enough to allow you to conclude that the preference was “not strong”. This would not require as big a sample size as a narrower interval.\n\n\n\nRepeat the above procedures to find a sample size that usually gives a confidence interval having a span of 0.4 or less.\n\n\n\n6.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\nlibrary(binom)\n\n# 1. 10 females from a population having equal numbers \n# of successes (1) and failures (0)\n\nx <- sample(c(1, 0), size = 10, c(0.5, 0.5), replace = TRUE)\nx\n\n [1] 0 1 1 0 0 0 0 0 0 1\n\n##  [1] 1 1 0 0 0 1 1 1 1 0\n\nsum(x)/10\n\n[1] 0.3\n\n## [1] 0.6\n\n# 2. \n\nmyCI <- binom.confint(sum(x), length(x), method = \"ac\")\nmyCI\n\n         method x  n mean     lower     upper\n1 agresti-coull 3 10  0.3 0.1033384 0.6076747\n\n##          method x  n mean     lower     upper\n## 1 agresti-coull 6 10  0.6 0.3116041 0.8328894\n\nmySpan <- myCI$upper - myCI$lower\nmySpan\n\n[1] 0.5043363\n\n## [1] 0.5212853\n\n# 3 & 4. \n\n# initialize empty vector \"span\"\nspan <- vector()\n\nfor(i in 1:5){\n    x <- sample(c(1,0), size = 10, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), 10, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nmin(span)\n\n[1] 0.4747451\n\n## [1] 0.4747451\n\n# 5.\n\nn <- 20       # sample size each time\nnruns <- 100  # number of runs\n\nspan <- vector()\n\nfor(i in 1:nruns){\n    x <- sample(c(1,0), size = n, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), n, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nhist(span, right = FALSE, col = \"goldenrod\", las = 1)\n\n\n\n\n\n\n# 6.\n\nn <- 93       # sample size each time\nnruns <- 100  # number of runs\n\nspan <- vector()\n\nfor(i in 1:nruns){\n    x <- sample(c(1,0), size = n, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), n, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nhist(span, right = FALSE, col = \"goldenrod\", las = 1)\n\n\n\n\n\n\n# 8.\n\nn <- 21       # sample size each time\nnruns <- 100  # number of runs\n\nspan <- vector()\n\nfor(i in 1:nruns){\n    x <- sample(c(1,0), size = n, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), n, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nhist(span, right = FALSE, col = \"goldenrod\", las = 1)"
  },
  {
    "objectID": "lab10-sample-size.html#plan-for-power",
    "href": "lab10-sample-size.html#plan-for-power",
    "title": "Lab 10 Sample size",
    "section": "7 Plan for power",
    "text": "7 Plan for power\n\nAssume that the preference p really is different from 0.5, and use null hypothesis significance testing to detect it. What strength of preference would we like to be able to detect in our experiment? To pick an extreme case, if the true proportion of females in the population choosing a male from her own species is 0.51 rather than 0.50, you would need an enormous sample size to detect it. But we don’t really care about such a small effect. Let’s start instead with the more realistic proportion p = 0.7. What sample size would be needed to detect it with reasonably high probability?\n\n\nSample 20 females from a population in which the true fraction of “successes” is 0.7\n\n\n\nApply the binomial test to your sample, to test the null hypothesis that the population proportion is 0.5. The binomial test calculates the exact 2-tailed probability of a result as extreme or more extreme as that observed if the null hypothesis is true. The method is implemented in R in the following command,\n\n\n# z <- binom.test(x, n, p = 0.5)\n\n\nwhere x is the observed number of successes in your sample from step 1, and n is the sample size. z here is an object that stores the result. To see the results of the test enter print(z) or just z in the command line. If you just want to see the resulting P-value of the test, enter this instead:\n\n# z$p.value\n\n\nDid you reject the null hypothesis?\n\n\nCreate a loop to repeat steps 1 and 2 ten times. In what fraction of iterations was the null hypothesis rejected?\n\n\n\nBy modifying the sample size and re-running the loop multiple times, find a sample size (ballpark, no need to be exact at this point) that usually results in the null hypothesis being rejected. Compare your results to those from the confidence interval simulation above.\n\n\n\nIs the sample size you determined feasible in an experiment? If the answer is yes, great! If the answer is no, because the sample size required is too large, then you have some decisions to make. You could decide not to run the experiment after all. Or, you could revise your aims. Perhaps your committee would be happy if you if you could detect a preference of 0.8 instead of 0.7.\n\n\n\n7.1 Suggested solutions\n\n\n# 1. Sample 20 females from a population in which the true fraction of \"successes\" is 0.7\n\nx <- sample(c(\"success\",\"failure\"), size = 20, c(0.7,0.3), replace = TRUE)\nnsuccess <- length(which(x == \"success\"))\n\n\n# 2. Apply the binomial test\n\nz <- binom.test(nsuccess, 20, 0.5)\nz$p.value\n\n[1] 0.04138947\n\n# 3. Repeat 10 times\n\nresult <- vector()\n\nfor(i in 1:10){\n    x <- sample(c(\"success\",\"failure\"), size = 20, c(0.7,0.3), replace = TRUE)\n    nsuccess <- length(which(x == \"success\"))\n    z <- binom.test(nsuccess, 20, 0.5)\n    result[i] <- z$p.value\n    }\n\nwhich(result <= 0.05)\n\n[1] 1 3 4 5 7 9\n\n# 4. Repeating 100 times shows that a sample size of n = 50 females \n#   seems to reject Ho roughly 80% of the time\n\nn <- 50 \n\nresult <- vector()\n\nfor(i in 1:100){\n    x <- sample(c(\"success\",\"failure\"), size = n, c(0.7,0.3), replace = TRUE)\n    nsuccess <- length(which(x == \"success\"))\n    z <- binom.test(nsuccess, n, 0.5)\n    result[i] <- z$p.value\n    }\n\nlength(which(result <= 0.05))/100\n\n[1] 0.66"
  },
  {
    "objectID": "lab10-sample-size.html#power-tools-in-r",
    "href": "lab10-sample-size.html#power-tools-in-r",
    "title": "Lab 10 Sample size",
    "section": "8 Power tools in R",
    "text": "8 Power tools in R\nSimulating random samples on the computer, as we did above, is a great way to investigate power and sample size requirements. It works in any situation and can mimic, even complicated study designs. However, a number of quantitative tools have been developed for mainly simple designs that do the work for you.\n\n8.1 Try the {pwr} package\n\nLoad the pwr library and use it to do some of the calculations for you. See the [Sample size and power page here for tips](https://dsgarage.netlify.app/misc-r/04-sample-size-power/.\n\n\nUse the pwr package to calculate the approximate minimum sample size needed to detect a preference of 0.6 with a power of 0.80 (i.e., the null hypothesis would be rejected in 80% of experiments). The null hypothesis is that the population proportion p of females who would choose the male from her own population is 0.5. The goal is to design an experiment that has a high probability of rejecting the null hypothesis when p is 0.6.\n\n\n\nRepeat the above procedure for a preference of 0.7, 0.8, and 0.9.\n\n\n\n\n8.2 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\n# You might need to install {pwr}\nlibrary(pwr)\n\n# 1.\n\nh <- ES.h(0.5, 0.6)\n\nz <- pwr.p.test(h, power = 0.8)\nz$n\n\n[1] 193.5839\n\n# 2.\n\n# Repeat for range values of pref\n\npref <- c(0.6, 0.7, 0.8, 0.9)\n\nfor(i in 1:length(pref)){\n  h <- ES.h(0.5, pref[i])\n  z <- pwr.p.test(h, power = 0.8)\n  print(z$n)\n}\n\n[1] 193.5839\n[1] 46.34804\n[1] 18.95431\n[1] 9.127904"
  },
  {
    "objectID": "lab10-sample-size.html#plan-a-2-x-2-experiment",
    "href": "lab10-sample-size.html#plan-a-2-x-2-experiment",
    "title": "Lab 10 Sample size",
    "section": "9 Plan a 2 x 2 experiment",
    "text": "9 Plan a 2 x 2 experiment\n\nIn an experiment on the great tit, two eggs were removed from 30 nests, which caused the attending females to lay one more egg. 35 un-manipulated nests served as controls. The response variable was incidence of malaria in female great tits at the end of the experiment. The results of the experiment are tabulated below.\n\n\n\n\n\nSurvivors\n\n\n\n\nImagine that you are considering repeating this experiment on a different species of songbird. What are the chances of detecting an effect? What sample sizes should you plan?\n\n\nRandomly sample 30 females from a control population in which the fraction of infected birds is 0.2 (the fraction in the tit data). Sample also 30 females from an experimental population in which the fraction of infected birds is 0.5 (the fraction in the tit data). Combined the samples into a data frame. Include a variable indicating treatment.\n\n\n\nDisplay the 2 x 2 frequency table from your random sample. Is there an association?\n\n\n\nRepeat steps 1-3 three times to convince yourself that the answer is different each time.\n\n\n\nUsing the tools in pwr calculate the sample size needed to achieve 80% power in this design.\n\n\n\n9.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\nlibrary(pwr)\n\n\n# 1. \n\nx <-\n  sample(\n    c(\"malaria\", \"no\"),\n    size = 30,\n    replace = TRUE,\n    prob = c(0.2, 0.8)\n  )\n\ny <-\n  sample(\n    c(\"malaria\", \"no\"),\n    size = 30,\n    replace = TRUE,\n    prob = c(0.5, 0.5)\n  )\n\nz <- rep(c(\"control\", \"treat\"), c(30, 30))\n\nmydata <- cbind.data.frame(\n  response = c(x, y),\n  treatment = z,\n  stringsAsFactors = FALSE\n)\n\n# 2.\n\ntable(mydata)\n\n         treatment\nresponse  control treat\n  malaria       7    18\n  no           23    12\n\n# 3. Repeat above\n\n# 4. \n\ncontrol <- c(0.2,0.8)\ntreatment <- c(0.5,0.5)\n\nprobs <- cbind.data.frame(treatment = treatment,\n                          control = control,\n                          stringsAsFactors = FALSE)\n\n# Cohen's effect size \"w\"\nw <- ES.w2(probs/sum(probs))           \n\nz <- pwr.chisq.test(w, df = 1, power = 0.80)\nz$N\n\n[1] 79.36072"
  },
  {
    "objectID": "lab10-sample-size.html#plan-a-2-treatment-experiment",
    "href": "lab10-sample-size.html#plan-a-2-treatment-experiment",
    "title": "Lab 10 Sample size",
    "section": "10 Plan a 2-treatment experiment",
    "text": "10 Plan a 2-treatment experiment\n\nImagine that you are considering a two-treatment experiment for a numeric response variable. The treatments consist of two grazing regimes and the response variable is the number of plant species in plots at the end of the experiment. How many replicate plots should you set up? As usual, we will investigate only the case of equal sample sizes in the two groups.\n\nWe’ll assume that the number of plant species in plots has a Gaussian distribution in both treatment and control. We’ll round the numbers so that they are integers.\n\n\nRandomly sample 20 measurements from a Gaussian distribution having a mean of 10 and a variance of 10 (so the standard deviation is the square root of 10). Call this the “control” group. Let’s round the numbers so that they are integers.\n\n\n# << YOUR CODE HERE FOR RANDOM SAMPLE\n\n# control <- round(control, 0)\n\n\n\nRepeat step 1 for a second sample, this time from a Gaussian distribution having a mean of 15 but the same sample variance, 10. (This is a bit unrealistic, as we would expect the variance in numbers of species to be higher as the mean increases, but never mind for now). Call this the “treatment” group. In other words, we will investigate the power of this experiment to detect a 1.5-fold change in the mean number of species from control to treatment.\n\n\n\nAssemble the samples into a data frame in “long” format, with a second variable indicating which measurements are from the control group and which are from the treatment group. Create a histogram for each of the two samples and compare the distributions by eye.\n\n\n\nUsing the power.t.test command in the basic R stats package, determine the power of the above design – probability that the experiment will detect a significant difference between the treatment and control means based on random samples.\n\n\n\nUsing the same command, determine the sample size that would be necessary to achieve a power of 0.80.\n\n\n\n10.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\nlibrary(pwr)\nlibrary(ggplot2)\n\n# 1.\n\nx1 <- rnorm(20, mean = 10, sd = sqrt(10))\nx1 <- round(x1,0)\n\n\n# 2.\n\nx2 <- rnorm(20, mean = 15, sd = sqrt(10))\nx2 <- round(x2,0)\n\n\n# 3.\n\nnspecies <- c(x1, x2)\n\ntreatment <- rep(c(\"control\", \"treatment\"), c(20,20))\n\nmydata <- cbind.data.frame(nspecies, treatment, stringsAsFactors = FALSE)\n\nggplot(mydata, aes(x = nspecies)) + \n        geom_histogram(fill = \"goldenrod\", col = \"black\", \n        boundary = 0, closed = \"left\", binwidth = 2) +\n    labs(x = \"Number of species\", y = \"Frequency\") + \n    theme(aspect.ratio = 0.5) + \n    facet_wrap( ~ treatment, ncol = 1, scales = \"free_y\")+\n    theme_classic()\n\n\n\n\n\n\n# 4.\n\nz <- power.t.test(n = 20, delta = 5, sd = 10)\nz$power\n\n[1] 0.3377084\n\n# 5.\n\nz <- power.t.test(delta = 5, sd = 10, power = 0.80)\nz$n\n\n[1] 63.76576"
  },
  {
    "objectID": "lab10-sample-size.html#harper-adams-data-science",
    "href": "lab10-sample-size.html#harper-adams-data-science",
    "title": "Lab 10 Sample size",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab11-linear-mods.html#harper-adams-data-science",
    "href": "lab11-linear-mods.html#harper-adams-data-science",
    "title": "Lab 11 Linear models",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab12-mixed-effects.html#harper-adams-data-science",
    "href": "lab12-mixed-effects.html#harper-adams-data-science",
    "title": "Lab 12 Mixed effects",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab13-likelihood.html#harper-adams-data-science",
    "href": "lab13-likelihood.html#harper-adams-data-science",
    "title": "Lab 13 Likelihood",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab14-GLM.html#harper-adams-data-science",
    "href": "lab14-GLM.html#harper-adams-data-science",
    "title": "Lab 14 GLM",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab15-model-selection.html#harper-adams-data-science",
    "href": "lab15-model-selection.html#harper-adams-data-science",
    "title": "Lab 15 Model selection",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The material in this module is designed to be experienced in an intensive one week format followed by an assessment meant to showcase reproducible statistical analysis skills. For enrolled students, the work will be supported with several live sessions during the main week of delivery.\n\n\n\n\nDay\nTopics\nLabs\n\n\n\n\nMon\nam\npm\n 00 Module overview\n 01 Introduction part 1 \n 02 Graph basics \n 03 Describing data \n 04 Estimation \n 05 Probability \nLab 01 Graphing I\nLab 02 Describing data\nLab 03 Frequency data\n\n\nTues\nam\npm\n 06 Hypothesis testing \n 07 Analyzing proportion data \n 08 Discrete data \n 09 Contingency tables \n 10 Gaussian distribution \n 11 One-sample tests \nLab 04 Contingency analysis\nLab 05 The Gaussian dist\nLab 06 2 group tests\n\n\nWed\nam\npm\n 12 Two sample tests  vid1  vid2\n 13 Testing assumptions  vid1  vid2\n 14 Experimental design I \n 15 ANOVA  vid1  vid2  vid3\n 16 Correlation \nLab 07 ANOVA\nLab 08 Cor and reg\nLab 09 Graphing II\n\n\nThurs\nam\npm\n 17 Regression  vid1  vid2\n 18 Data and reproducibility \n 19 Graphing II \n 20 Experimental design II \nLab 10 Sample size\nLab 11 Linear models\nLab 12 Mixed effects\n\n\nFri\nam\npm\n 21 Linear models  vid1  vid2\n 22 Mixed effects models \n 23 Likelihood \n 24 Generalized linear model \n 25 Model selection \nLab 13 Likelihood\nLab 14 GLM\nLab 15 Model selection"
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab09-graph2.html#objectives",
    "href": "lab09-graph2.html#objectives",
    "title": "Lab 09 Graphing II",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nEvaluate data using graphical exploration\nGraph and evaluate frequency distributions\nCompare frequency distributions\nPractice good practice in graph design and data exploration\nPractice {ggplot2} tools and syntax\n\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab09-graph2.html#start-a-script",
    "href": "lab09-graph2.html#start-a-script",
    "title": "Lab 09 Graphing II",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab09-graph2.html#displaying-data-using-graphs-and-tables",
    "href": "lab09-graph2.html#displaying-data-using-graphs-and-tables",
    "title": "Lab 09 Graphing II",
    "section": "3 Displaying data using graphs and tables",
    "text": "3 Displaying data using graphs and tables\nThe purpose of this lab is to tour the table and graphics capabilities of R, and to explore the best methods for displaying patterns in data. We will do this by exploring some real datasets."
  },
  {
    "objectID": "lab09-graph2.html#data-mammal-body-mass",
    "href": "lab09-graph2.html#data-mammal-body-mass",
    "title": "Lab 09 Graphing II",
    "section": "4 DATA: Mammal body mass**",
    "text": "4 DATA: Mammal body mass**\nThese data were published as a data paper in Ecology and deposited in the Ecological Archives (F. A. Smith, S. K. Lyons, S. K. M. Ernest, K. E. Jones, D. M. Kaufman, T. Dayan, P. A. Marquet, J. H. Brown, and J. P. Haskell. 2003. Body mass of late Quaternary mammals. Ecology 84: 3403.)\nMost of the variables are categorical, with multiple named categories. “Continent” includes mammals on islands (“Insular” category) whereas “Oceanic” refers to marine mammals. Body mass (in grams) is the sole numeric variable. The “status” variable indicates whether species is currently present in the wild (extant), extinct as of late Pleistocene (extinct), extinct within the last 300 years (historical), or an introduced species (introduction).\n\n\n4.1 Read and examine the data\nThe data file we will work with is biogeogMammals.csv.\n\nRead the contents of the file to a data frame.\nUse the head() function to view the first few lines of the data frame on the screen. You’ll see that every row represents the data for a different mammal species.\n\n\n\n\n4.2 Frequency tables\n\nWhich continent has the greatest number of mammal species? Which has the least? Make a table (e.g., using table()) of the frequency of cases on each continent.\n\n2.You’ll notice in the frequency table for the variable “continent” that there’s a typo in the data. One case is shown as having the continent Af rather than AF. Fix this using the command line in R and recalculate the frequency table.\n3.How many extinct mammals are recorded in the data file? Use a frequency table to find out. Create a two-way frequency table (contingency table) showing the status of mammal species on each continent.\n4.Judging by eye, which continent has the greatest number of extinctions relative to the number of extant species?\n\n\n\n4.3 Suggested solutions\nAll lines below beginning with double hashes are R output\n\n\n# Load the packages you might need\n\n# Load data\n# NB your file path may be different than mine\nmammals <- read.csv(\"data/biogeogMammals.csv\")\nhead(mammals)\n\n  continent status        order  family      genus       species mass.grams\n1        AF extant Artiodactyla Bovidae      Addax nasomaculatus    70000.3\n2        AF extant Artiodactyla Bovidae  Aepyceros      melampus    52500.1\n3        AF extant Artiodactyla Bovidae Alcelaphus    buselaphus   171001.5\n4        AF extant Artiodactyla Bovidae Ammodorcas       clarkei    28049.8\n5        AF extant Artiodactyla Bovidae Ammotragus        lervia    48000.0\n6        AF extant Artiodactyla Bovidae Antidorcas   marsupialis    39049.9\n\n# Number of mammal species on each continent\ntable(mammals$continent)\n\n\n     Af      AF     AUS      EA Insular     NAm Oceanic      SA \n      1    1033     346    1033    1484     779      78     977 \n\n# Fix \"Af\"\nwhich(mammals$continent==\"Af\")\n\n[1] 322\n\nmammals$continent[322]<-\"AF\" # replace the wrong value\n\ntable(mammals$continent) # now the table is fixed\n\n\n     AF     AUS      EA Insular     NAm Oceanic      SA \n   1034     346    1033    1484     779      78     977 \n\n# How many extinct mammals?\n# The table shows that 242 species of mammal are listed as extinct\nz <- table(mammals$status)\nz\n\n\n      extant      extinct   historical introduction \n        5388          242           84           17 \n\n# Extinction status by continent (contingency table)\n# The table shows that Australia (AUS) has the greatest number of extinct species\n# relative to the total number.\ntable(mammals$continent, mammals$status)\n\n         \n          extant extinct historical introduction\n  AF        1017      13          4            0\n  AUS        261      45         23           17\n  EA        1027       0          6            0\n  Insular   1405      29         50            0\n  NAm        700      78          1            0\n  Oceanic     78       0          0            0\n  SA         900      77          0            0"
  },
  {
    "objectID": "lab09-graph2.html#example-graphing-frequency-distributions",
    "href": "lab09-graph2.html#example-graphing-frequency-distributions",
    "title": "Lab 09 Graphing II",
    "section": "5 EXAMPLE: Graphing frequency distributions",
    "text": "5 EXAMPLE: Graphing frequency distributions\n\n\nPlot the number of mammal species on each continent using a simple bar graph. Include a label for the y axis.\nThe plot categories are listed in alphabetical order by default, which is arbitrary and makes the visual display less efficient than other possibilities. Redo the bar graph with the continents appearing in order of decreasing numbers of species.\nGenerate a histogram of the body masses of mammal species. How informative is that?!\nCreate a new variable in the mammal data frame: the log (base 10) of body mass. (See “Transform and add a variable” on the R tips “Data” page if you need help with this.)\nGenerate a histogram of log body mass. Is this more informative? Morphological data commonly require a log-transformation to analyze.\nRedo the previous histogram but use a bin width of 2 units. How much detail is lost?\nRedo the histogram but try a bin width of of 1; then try 0.5; and then 0.1. Which bin width is superior?\nRedo the histogram, but display probability density instead of frequency.\nHow does the frequency distribution of log body mass depart from a normal distribution? Answer by visual examination of the histogram you just created. Now answer by examining a normal quantile plot instead. Which display is more informative?\nOptional: redraw the histogram of log body mass and superimpose a normal density curve to assess help detect deviations from normality.\n\n\n\n5.1 Suggested solutions\n\n# Load the packages you might need\nlibrary(ggplot2)\n\n# Bar plot of mammal species by continent\nbarplot(\n  table(mammals$continent),\n  col = \"goldenrod\",\n  cex.names = 0.8,\n  ylim = c(0, 1600),\n  las = 1\n)\n\n\n\n\n\n\n# Barplot sorted by frequency\nbarplot(\n  sort(table(mammals$continent),\n       decreasing = TRUE),\n  col = \"goldenrod\",\n  cex.names = 0.8,\n  las = 1,\n  ylim = c(0, 1600),\n  ylab = \"Frequency\"\n)\n\n\n\n\n\n\n# ggplot methods\nggplot(mammals, aes(x = continent)) +\n  geom_bar(stat = \"count\", fill = \"goldenrod\") +\n  labs(x = \"Continent\", y = \"Frequency\") +\n  theme_classic()\n\n\n\n\n\n\n\n# To order by category in ggplot, first make a new factor variable\n\nmammals$continent_ordered <- factor(mammals$continent,\n                                    levels = names(sort(table(mammals$continent),\n                                                        decreasing = TRUE)))\n\nggplot(mammals, aes(x = continent_ordered)) +\n  geom_bar(stat = \"count\", fill = \"goldenrod\") +\n  labs(x = \"Continent\", y = \"Frequency\") +\n  theme_classic()\n\n\n\n\n\n\n# Histogram of body masses\nhist(mammals$mass.grams, col=\"goldenrod\", \n      right = FALSE, \n      las = 1, \n      xlab = \"Body mass (g)\", \n      main = \"\")\n\n\n\n\n\n\n# Add a new variable, log10 of body mass\nmammals$logmass <- log10(mammals$mass.grams)\n\nhist(mammals$logmass, \n      col=\"goldenrod\", \n      right = FALSE, \n      las = 1,\n      xlab = \"Log10 body mass\", \n      main = \"\", \n      breaks = seq(0, 8.5, by = 0.5))\n\n\n\n\n\nSame but using ggplot. You’ll see a Warning: Removed rows containing non-finite values. These are rows with missing data on mass. Use the argument na.rm = TRUE in geom_histogram to get rid of the warning.\n\nggplot(mammals, aes(x = logmass)) + \n    geom_histogram(fill = \"goldenrod\", \n                  col = \"black\", \n                  binwidth = 0.5, \n                  boundary = 0) + \n    labs(x = \"log10 body mass\", y = \"Frequency\") + \n    theme_classic()\n\n\n\n\n\n\n\n# Plot density instead\nhist(mammals$logmass, \n      col=\"goldenrod\", \n      right = FALSE, \n      las = 1, \n      prob = TRUE,\n      xlab = \"Log10 body mass\", \n      main = \"\", \n      breaks = seq(0, 8.5, by = 0.5))\n\n\n\n\n\n\n# with ggplot\nggplot(mammals, aes(x = logmass)) + \n    geom_histogram(fill = \"goldenrod\", \n                  col = \"black\", \n                  binwidth = 0.5, \n                  boundary = 0, \n                  aes(y = ..density..)) + \n    labs(x = \"log10 body mass\", y = \"Density\") + \n  theme_classic()\n\n\n\n## Warning: Removed 1372 rows containing non-finite values (stat_bin).\n\n\n\n# Normal quantile plot\nqqnorm(mammals$logmass)\n# adds the straight line for comparison through 1st and 3rd quartiles\nqqline(mammals$logmass, col = 'red') \n\n\n\n\n\n\n# Histogram with best-fit normal curve superimposed.\n# The curve function is fussy about the name of the variable: must be \"x\"\nx <- mammals$logmass\n\nhist(\n  x,\n  col = \"goldenrod\",\n  right = FALSE,\n  las = 1,\n  prob = TRUE,\n  xlab = \"Log10 body mass\",\n  main = \"\",\n  breaks = seq(0, 8.5, by = 0.5)\n)\n\nm <- mean(x, na.rm = TRUE)\ns <- sd(x, na.rm = TRUE)\n\ncurve(\n  dnorm(x, mean = m, sd = s),\n  col = \"blue\",\n  lwd = 3,\n  lty = 3,\n  add = TRUE\n)"
  },
  {
    "objectID": "lab09-graph2.html#example-comparing-frequency-distributions",
    "href": "lab09-graph2.html#example-comparing-frequency-distributions",
    "title": "Lab 09 Graphing II",
    "section": "6 EXAMPLE Comparing frequency distributions",
    "text": "6 EXAMPLE Comparing frequency distributions\n\n\nUse a box plot to compare the distribution of body sizes (log scale most revealing) of mammals having different extinction status. Are extinct mammals similar to, larger than, or smaller than, extant mammals?\nExamine the previous box plot. How do the shapes of the body size distributions compare between extinct and extant mammals?\nRedo the previous box plot but make box width proportional to the square root of sample size. Add a title to the plot.\nDraw a violin plot to compare the frequency distribution of log body sizes of mammals having different extinction status. Which do you find is more revealing about the shapes of the body size distributions: box plot or violin plot?\nUse multiple histograms to compare the frequency distribution of log body sizes of mammals having different extinction status. Stack the panels one above the other. In this plot, how easy is it to visualize differences among treatments in the distributions compared to your previous plots?\nMake a table of the median log body mass of each extinction-status group of mammals. Are the values consistent with the plotted distributions?\n\n\n\nSuggested solutions\nAll lines below beginning with double hashes are R output\n\n# Box plot to compare the distribution of body sizes\n# Extinct mammals tend to have large mass compared to extant mammals.\n# The frequency distributions for these two groups also have opposite skew.\n\nboxplot(logmass ~ status, data = mammals, ylab = \"log10 body mass\", \n        col = \"goldenrod1\", las = 1)\n\n\n\n\n\n\n# or ggplot method\nggplot(mammals, aes(x = status, y = logmass)) + \n    geom_boxplot(fill = \"goldenrod1\", notch = FALSE) + \n    labs(x = \"Status\", y = \"Log10 body mass\") + \n    theme_classic()\n\n\n\n\n\n\n# Violin plot\nggplot(mammals, aes(x = status, y = logmass)) + \n    geom_violin(fill = \"goldenrod1\") + \n    labs(x = \"Status\", y = \"Log10 body mass\") + \n    stat_summary(fun.y = mean,  geom = \"point\", color = \"black\") +\n  theme_classic()\n\n\n\n\n\n\n# Multiple histograms\nggplot(mammals, aes(x = logmass)) + \n    geom_histogram(fill = \"goldenrod1\", col = \"black\", \n             binwidth = 0.2, boundary = 0) +\n    labs(x = \"log10 body mass\", y = \"Frequency\") + \n    facet_wrap(~status, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\n  theme_classic()"
  },
  {
    "objectID": "lab09-graph2.html#data-fly-sex-and-longevity",
    "href": "lab09-graph2.html#data-fly-sex-and-longevity",
    "title": "Lab 09 Graphing II",
    "section": "7 DATA: Fly sex and longevity",
    "text": "7 DATA: Fly sex and longevity\n\nThe data are from L. Partridge and M. Farquhar (1981), Sexual activity and the lifespan of male fruit flies, Nature 294: 580-581. The experiment placed male fruit flies with varying numbers of previously-mated or virgin females to investigate how mating activity affects male lifespan. The data are in the file fruitflies.csv.\n\n\n7.1 Inspect the data\nOpen the data file in a spreadsheet program to have a look at it. View the first few lines of the data frame on the screen, and familiarize yourself with the variable names.\nOur goal here is to find a plot type that clearly and efficiently visualizes the patterns in the data, especially the differences among groups.\n\n\n\n7.2 Analysis\n\n\nRead the data file into a new data frame.\nUse a strip chart to examine the distribution of longevities in the treatment groups. Try the jitter method to reduce overlap between points. If needed, adjust the size or rotation of the treatment labels so that they all fit on the graph. What pattern of differences between treatments in longevity is revealed?\nCompare the strip chart to a box plot of the same data. Is the pattern in the data as clear in both types of plot?\nThe variable thorax stands for thorax length, which was used as a measure of body size. The measurement was included in case body size also affected longevity. Produce a scatter plot of thorax length and longevity. Make longevity the response variable (i.e., plot it on the vertical axis). Is there a relationship?\nRedraw the scatter plot but this time use different symbols or colors for the different treatment groups. Add a legend to identify the symbols. Describe the pattern of differences between treatments.\nAdd scatterplot smoothers or linear regressions to the previous figure, one for each group. Do the differences in longevity among the treatments stand out when variation in body size is incorporated?\n\n\n\n\n7.3 Suggested solutions\n\n\n# Read and inspect data\n# NB your file path may be different than mine\nx <- read.csv(\"data/fruitflies.csv\")\n\nhead(x)\n\n  Npartners          treatment longevity.days thorax.mm\n1         8 8 pregnant females             35      0.64\n2         8 8 pregnant females             37      0.68\n3         8 8 pregnant females             49      0.68\n4         8 8 pregnant females             46      0.72\n5         8 8 pregnant females             63      0.72\n6         8 8 pregnant females             39      0.76\n\n# Strip chart\nstripchart(longevity.days ~ treatment, \n          data = x, \n          vertical = TRUE, \n          method = \"jitter\",\n          pch = 16, \n          col = \"blue\", \n          cex.axis=0.7, \n          ylab=\"Longevity (days)\")\n\n\n\n\n\n\n# Strip chart using ggplot\nggplot(x, aes(x=treatment, y=longevity.days)) +\n  geom_jitter(color = \"blue\", size = 3, width = 0.15) +\n  labs(x = \"Treatment\", y = \"Longevity (days)\") + \n  theme_classic()\n\n\n\n\n\n\n# Box plot\nboxplot(\n  longevity.days ~ treatment,\n  data = x,\n  cex.axis = .7,\n  ylab = \"Longevity (days)\",\n  boxwex = 0.5,\n  col = \"goldenrod1\"\n)\n\n\n\n\n\n\n# Box plot using ggplot\nggplot(x, aes(x=treatment, y=longevity.days)) +\n  geom_boxplot(fill = \"goldenrod1\", width = 0.5) +\n  labs(x = \"Treatment\", y = \"Longevity (days)\") + \n  theme_classic()\n\n\n\n\n\n\n# Scatter plot\nplot(\n  longevity.days ~ thorax.mm,\n  data = x,\n  pch = 16,\n  col = \"blue\",\n  las = 1,\n  xlab = \"Thorax length (mm)\",\n  ylab = \"Longevity (days)\"\n)\n\n\n\n\n\n\n# Scatter plot with ggplot\nggplot(x, aes(x = thorax.mm, y = longevity.days)) + \n    geom_point(size = 3, col = \"blue\") + \n    labs(x = \"Thorax length (mm)\", y = \"Longevity (days)\") + \n    theme_classic()\n\n\n\n\n\n\n# Not run - just here to show code\n\n# Scatter plot with separate colors for each group \nplot(\n  longevity.days ~ thorax.mm,\n  data = x,\n  pch = as.numeric(factor(treatment)),\n  col = as.numeric(factor(treatment)),\n  las = 1,\n  xlab = \"Thorax length (mm)\",\n  ylab = \"Longevity (days)\"\n)\n\n# NB the locator(1) function allows you to click ON YOUR PLOT\n# to place the legend... neat eh?\n# legend(\n#   locator(1),\n#   legend = as.character(levels(factor(x$treatment))),\n#   pch = 1:length(levels(factor(x$treatment))),\n#   col = 1:length(levels(factor(x$treatment)))\n# )\n\nlegend(\n  y = 90, x = .65,\n  legend = as.character(levels(factor(x$treatment))),\n  pch = 1:length(levels(factor(x$treatment))),\n  col = 1:length(levels(factor(x$treatment)))\n)\n\n\n\n\n\n# Scatter plot with separate colors for each group using ggplot\nggplot(x, aes(x = thorax.mm, y = longevity.days, colour = treatment, \n            shape = treatment)) + \n    geom_point(size = 2) + \n    labs(x = \"Thorax length (mm)\", y = \"Longevity (days)\") + \n    theme_classic()\n\n\n\n\n\n\n# Add lines; shown for ggplot method only\nggplot(x, aes(x=thorax.mm, y=longevity.days, colour = treatment, \n            shape = treatment)) + \n    geom_point(size = 2) +\n    geom_smooth(method = lm, size = 1, se = FALSE) +\n    labs(x = \"Thorax length (mm)\", y = \"Longevity (days)\") + \n    theme_classic()\n\n\n\n## `geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "lab09-graph2.html#harper-adams-data-science",
    "href": "lab09-graph2.html#harper-adams-data-science",
    "title": "Lab 09 Graphing II",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  }
]