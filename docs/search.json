[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "mysite",
    "section": "",
    "text": "Hello world"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to C7041. This module is an introduction to statistical analysis and experimental design reviewing basic statistics and introducing linear models, the modern standard for creating and consuming scientific evidence. There is an emphasis on practical applications using a series of data examples and lab exercises, along with lectures on selected topics and readings. A prerequisite is a familiarity with the R language and simple statistics, like that covered in the R Stats Bootcamp."
  },
  {
    "objectID": "index.html#c7041-resources",
    "href": "index.html#c7041-resources",
    "title": "Home",
    "section": "C7041 Resources",
    "text": "C7041 Resources\n\nR Stats Bootcamp\nFree Textbook option: Diez et al., 2019. OpenIntro Statistics 4ed\nAlternative Textbook option (not free): Whitlock & Schluter, 2020. The Analysis of Biological Data 3ed"
  },
  {
    "objectID": "index.html#harper-adams-data-science",
    "href": "index.html#harper-adams-data-science",
    "title": "Home",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab01-graph1.html#objectives",
    "href": "lab01-graph1.html#objectives",
    "title": "Lab 01 Graphs I",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nReview some common graph formats and when they are used\nMake graphs in R, such as histograms, bar charts, box plots, and scatter plots\nCritique basic graphs to improve readability and accurate communication\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab01-graph1.html#start-a-script",
    "href": "lab01-graph1.html#start-a-script",
    "title": "Lab 01 Graphs I",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab01-graph1.html#learning-the-tools",
    "href": "lab01-graph1.html#learning-the-tools",
    "title": "Lab 01 Graphs I",
    "section": "3 Learning the tools",
    "text": "3 Learning the tools\n\n3.1 {ggplot2} package\nThe function ggplot() allows us to graph most kinds of data relatively simply. The syntax is slightly odd but very flexible. We’ll practice specific commands for several types of plots below.\nTo begin, remember to install ggplot2 if need to and then load ggplot2 with the library() function:\n\n# Neat trick for checking whether a package is installed\n# and installing it if it is not, loading it if it is...\n\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n# The \"hard\" way\n# install.packages(\"ggplot2\", dep = T) # run if needed...\n\nlibrary(ggplot2)\n\n\nGraphing in {ggplot2} is slightly different than with the base R plot functions (like plot(), boxplot(), hist(), etc.). To make a graph with ggplot(), you need to specify at least two elements in your command.\n\nFirst part: Use the function ggplot() itself, to specify which data frame you want to use and also which variables are to be plotted.\nSecond part: Tell R what kind of graph to make using a geom() function.\n\nThe odd part is that these two parts are put together with a + sign. It’s simplest to see this with an example. We’ll draw a histogram with ggplot() in the next section.\n\n\n\n3.2 Histograms\nA histogram represents the frequency distribution of a numerical variable in a sample.\nLet’s see how to make a basic histogram using the age data from the Titanic data set. Make sure you have loaded the data (using read.csv()) into a data frame called titanicData.\n\n# Example code to load the data\n# NB you will probably have to supply your own file path\ntitanic <- read.csv(\"data/titanic.csv\")\n\n# Code to make a simple histogram of age:\n\nggplot(titanic, aes(x=age)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (stat_bin).\n\n\n\n\n# There are a couple of informative warnings, but we can ignore them\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## Warning: Removed 680 rows containing non-finite values (stat_bin).\n\n\nggplot()\nNotice that there are two functions called here, put together in a single command with a plus sign. The first function is ggplot(), and it has two input arguments. Listed first is titanicData; this is the name of the data frame containing the variables that we want to graph. The second input to ggplot is an aes() function. In this case, the aes() function tells R that we want Age to be the x-variable (i.e. the variable that is displayed along the x-axis). (The “aes” stands for “aesthetics”,” but if you’re like me it won’t necessarily help you remember it any better)\ngeom_histogram()\nThe second function in this command is geom_histogram(). This is the part that tells R that the “geometry” of our plot should be a histogram.\nThis is not the most beautiful graph in the world, but it conveys the information. Later, we’ll see a couple of options that can make a ggplot graph look a little better.\n\n\n\n3.3 Bar graphs\nA bar graph plots the frequency distribution of a categorical variable.\nIn ggplot(), the syntax for a bar graph is very similar to that for a histogram. For example, here is a bar graph for the categorical variable Sex in the titanic data set. Aside from specifying a different variable for x, we use a different geom function here, geom_bar.\n\nggplot(titanic, aes(x=sex)) + geom_bar(stat=\"count\")\n\n\n\n\n\n\n\n3.4 Boxplots\nA boxplot is a convenient way of showing the “frequency distribution”central tendency” of data for a numerical variable in one or more groups. Here’s the code to draw a boxplot for age in the titanic data set, separately for each sex:\n\nggplot(titanic, aes(x=sex, y=age)) + geom_boxplot()\n\nWarning: Removed 680 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nNotice that the y variable here is age, and x is the categorical variable sex that winds up on the x-axis. See the result below, and look at where the variables are. The other new feature here is the new geom function, geom_boxplot().\nHere the thick bar in the middle of each boxplot is the median of that group. The upper and lower bounds of the box extend from the first to the third quartile. The vertical lines are called whiskers, and they cover most of the range of the data (except when data points are pretty far from the median, when they are plotted as individual dots, as on the male boxplot).\n\n\n\n3.5 Scatterplots\nThe last graphical style that we will mention here is the scatter plot, which shows the relationship between two numerical variables.\nWe will use a dataset showing the relationship between the ornamentation of father guppies and the sexual attractiveness of their sons, using the file guppy-attractiveness.csv.\n\n# Example code to load the data\n# NB you will probably have to supply your own file path\nguppy <-read.csv(\"data/guppy-attractiveness.csv\", header =T)\n\nhead(guppy)\n\n  fatherOrnamentation sonAttractiveness\n1                0.35             -0.32\n2                0.03             -0.03\n3                0.14              0.11\n4                0.10              0.28\n5                0.22              0.31\n6                0.23              0.18\n\n\n\nTo make a scatter plot of the variables fatherOrnamentation and sonAttractiveness with ggplot, you need to specify the x and y variables, and use geom_point():\n\n\nggplot(guppy,  \n      aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n  geom_point()"
  },
  {
    "objectID": "lab01-graph1.html#better-looking-graphics-with-options",
    "href": "lab01-graph1.html#better-looking-graphics-with-options",
    "title": "Lab 01 Graphs I",
    "section": "4 Better looking graphics with options",
    "text": "4 Better looking graphics with options\n\n4.1 Sexing up the sexual selection scatterplot\nThe code we tried above for {ggplot2} does not begin to scratch the surface of what ggplot and R are capable of. Not only are there far more choices about the kinds of plots available, but there are many, many options for customizing the look and feel of each graph. You can choose the font, the font size, the colors, the style of the axes labels, etc., and you can customize the legends and axes legends nearly as much as you want.\nLet’s dig a little deeper into just a few options that you can add to any of the graphs to improve them. For example, you can change the text of the x-axis label or the y-axis label by using xlab or ylab. Let’s do that for the scatterplot, to make the labels a little nicer to read for humans.\n\n\n# better axes\nggplot(guppy,   \n    aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n    geom_point() +\n    xlab(\"Father ornamentation\") + \n    ylab(\"Son attractiveness\")\n\n\n\n\n\nThe can see the labels included in quotes inside the xlab and ylab arguments now appear on your graph.\nIt can also be nice to remove the default gray background, to make what some feel is a cleaner graph. Try adding + theme_classic() to the end of one of your lines of code making a graph, to see whether you prefer the result to the default design. Also, you could add a title by adding the labs function and setting the main argument to a character string + labs(title = \"your title\").\n\n\nggplot(guppy,   \n    aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n    geom_point() +\n    xlab(\"Father ornamentation\") + \n    ylab(\"Son attractiveness\") +\n    theme_classic() +\n    labs(title = \"Ooooo, now that is classic!\")"
  },
  {
    "objectID": "lab01-graph1.html#getting-help",
    "href": "lab01-graph1.html#getting-help",
    "title": "Lab 01 Graphs I",
    "section": "5 Getting help",
    "text": "5 Getting help\nThe help pages in R (using help() with the name of the function you would like help with) are the main source of help, but the amount of detail takes getting used to. For example, to explore the options for ggplot(), enter the following into the R Console.\n\nhelp(ggplot)\nThis will cause the contents of the help page for this function to appear in the Help window in RStudio. These manual pages are often technical, but it is useful with a little practice."
  },
  {
    "objectID": "lab01-graph1.html#exercises",
    "href": "lab01-graph1.html#exercises",
    "title": "Lab 01 Graphs I",
    "section": "6 Exercises",
    "text": "6 Exercises\n\nMake a script in RStudio that collects all your R code required to answer the following questions. Include answers to the qualitative questions using comments.\n\n)\n\n6.1\nFor each of the following pairs of graphs, identify features that communicate better on one version than the other.\nSurvivorship as a function of sex for passengers of the RMS Titanic\n\n\n\n\n\n\nEar length in male humans as a function of age\n\n\n\n\n6.2\nLet’s use the data from countries.csv to practice making some graphs.\nRead the data from the file “countries.csv” in the Data folder.\nUse ggplot() (Make sure that you have run library(ggplot2)).\nMake a histogram to show the frequency distribution of values for measles_immunization_oneyearolds, a numerical variable. (This variable gives the percentage of 1-year-olds that have been vaccinated against measles.) Describe the pattern that you see.\nMake a bar graph to show the numbers of countries in each of the continents. (The categorical variable continent indicates the continent to which countries belong.)\nDraw a scatterplot that shows the relationship between the two numerical variables life_expectancy_at_birth_male and life_expectancy_at_birth_female.\n\n\n\n6.3\nThe ecological footprint is a widely-used measure of the impact a person has on the planet. It measures the area of land (in hectares) required to generate the food, shelter, and other resources used by a typical person and required to dispose of that person’s wastes. Larger values of the ecological footprint indicate that the typical person from that country uses more resources.\nThe countries data set has two variables for many countries showing the ecological footprint of an average person in each country. ecological_footprint_2000 and ecological_footprint_2012 show the ecological footprints for the years 2000 and 2012, respectively.\nMake a scatterplot to plot the relationship between the ecological footprint of 2000 and of 2012.\nDescribe the relationship between the footprints for the two years. Does the value of ecological footprint of 2000 seem to predict anything about its value in 2012?\nFrom this graph, does the ecological footprint tend to go up or down in the years between 2000 and 2012? Did the countries with high or low ecological footprint change the most over this time? (Hint: you can add a one-to-one line to your graph by adding + geom_abline(intercept = 0, slope = 1) to your ggplot() command. This will make it easier to see when your points are above or below the “line of equivalence”).\n\n\n\n6.4\nUse the countries data again. Plot the relationship between continent and female life expectancy at birth. Describe the patterns that you see.\n\n\n\n6.5 Muchhala (2006) measured the length of the tongues of eleven different species of South American bats, as well as the length of their palates (to get an indication of the size of their mouths). All of these bats use their tongues to feed on nectar from flowers. Data from the article are given in the file BatTongues.csv. In this file, both Tongue Length and Palette Length are given in millimeters.\nImport the data and inspect it using summary(). You can call the data set whatever you like, but in one of the later steps we’ll assume it is called bat_tongues. Each value for tongue length and palate length is a species mean, calculated from a sample of individuals per species.\nDraw a scatter plot to show the association between palate length and tongue length, with tongue length as the response variable. Describe the association: is it positive or negative? Is it strong or weak?\nAll of the data points that went into this graph have been double checked and verified. With that in mind, what conclusion can you draw from the outlier (the point with an unusual value) on the scatter plot?\nLet’s figure out which species is the outlier. To do this, we’ll use a useful function called filter() from the package {dplyr}. Use library() to load dplyr to your R session.\nThe function filter() gives us the row (or rows) of a data frame that has a certain property. Looking at the graph, we can tell that the point we are interested in has a very long tongue length, at least over 80 mm long! The following command will pull out the rows of the data frame bat_tongues that have tongue_length greater than 80 mm:\n\nbat_tongues <- read.csv('data/BatTongues.csv')\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# useful\nfilter(bat_tongues,tongue_length>80)\n\n           species palate_length tongue_length\n1 Anoura fistulata          12.4          85.2\n\n\n The unusual species is Anoura fistulata See a photo here. This species has an outrageously long tongue, which it uses to collect nectar from a particular flower (can you guess what feature of the flower has led to the evolution of such a long tongue?). See the article by Muchhala (2006) to learn more about the biology of this strange bat.\n\n\n\n6.6 Pick one of the plots you made using R today. What could be improved about this graph to make it a more effective presentation of the data?"
  },
  {
    "objectID": "lab01-graph1.html#harper-adams-data-science",
    "href": "lab01-graph1.html#harper-adams-data-science",
    "title": "Lab 01 Graphs I",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab02-descriptives.html#objectives",
    "href": "lab02-descriptives.html#objectives",
    "title": "Lab 02 Describing data",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nInvestigate sampling error; see that larger samples have less sampling error\nVisualize confidence intervals\nCalculate basic summary statistics using R\nCalculate confidence intervals for the mean with R\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab02-descriptives.html#start-a-script",
    "href": "lab02-descriptives.html#start-a-script",
    "title": "Lab 02 Describing data",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab02-descriptives.html#missing-data",
    "href": "lab02-descriptives.html#missing-data",
    "title": "Lab 02 Describing data",
    "section": "3 Missing data",
    "text": "3 Missing data\nSometimes we do not have all variables measured on all individuals in the data set. When this happens, we need a space holder in our data files so that R knows that the data is missing. The standard way of doing this in R is to put NA in the location that the data would have gone. NA is short for “not available”.\nFor example, in the Titanic data set, we do not know the age of several passengers. Let’s look at it. Load the Titanic data set:\n\n# Load data\n# NB your file path may be different than mine\ntitanicData <- read.csv(\"data/titanic.csv\" )\n\n\nHave R print out the vector of the age variable, which you can do easily by just typing its name:\n\n# print the variable age\ntitanicData$age\n\n   [1] 29.0000  2.0000 30.0000 25.0000  0.9167 47.0000 63.0000 39.0000 58.0000\n  [10] 71.0000 47.0000 19.0000      NA      NA      NA 50.0000 24.0000 36.0000\n  [19] 37.0000 47.0000 26.0000 25.0000 25.0000 19.0000 28.0000 45.0000 39.0000\n  [28] 30.0000 58.0000      NA 45.0000 22.0000      NA 41.0000 48.0000      NA\n  [37] 44.0000 59.0000 60.0000 45.0000      NA 53.0000 58.0000 36.0000 33.0000\n  [46]      NA      NA 36.0000 36.0000 14.0000 11.0000 49.0000      NA 36.0000\n  [55]      NA 46.0000 47.0000 27.0000 31.0000      NA      NA      NA      NA\n  [64] 27.0000 26.0000      NA      NA 64.0000 37.0000 39.0000 55.0000      NA\n  [73] 70.0000 69.0000 36.0000 39.0000 38.0000      NA 27.0000 31.0000 27.0000\n  [82]      NA 31.0000 17.0000      NA      NA  4.0000 27.0000 50.0000 48.0000\n  [91] 49.0000 48.0000 39.0000 23.0000 53.0000 36.0000      NA      NA 30.0000\n [100] 24.0000 19.0000 28.0000 23.0000 64.0000 60.0000      NA 49.0000      NA\n [109] 44.0000 22.0000 60.0000 48.0000 37.0000 35.0000 47.0000 22.0000 45.0000\n [118] 49.0000      NA 71.0000 54.0000 38.0000 19.0000 58.0000 45.0000 23.0000\n [127] 46.0000 25.0000 21.0000 48.0000 49.0000 45.0000 36.0000      NA 55.0000\n [136] 52.0000 24.0000      NA      NA      NA 16.0000 44.0000 51.0000 42.0000\n [145] 35.0000 35.0000 38.0000 35.0000      NA 50.0000 49.0000 46.0000      NA\n [154] 58.0000 41.0000      NA 42.0000 40.0000      NA      NA      NA 42.0000\n [163] 55.0000 50.0000 16.0000      NA 29.0000 21.0000 30.0000 15.0000 30.0000\n [172]      NA      NA      NA 46.0000 54.0000 36.0000 28.0000      NA 65.0000\n [181] 33.0000 44.0000 37.0000      NA 55.0000 47.0000 36.0000 58.0000 31.0000\n [190] 23.0000 19.0000 64.0000      NA 64.0000 22.0000 28.0000      NA      NA\n [199] 22.0000      NA      NA 18.0000 17.0000 52.0000 46.0000 56.0000      NA\n [208]      NA 43.0000 31.0000      NA      NA 33.0000      NA 27.0000 55.0000\n [217] 54.0000      NA 61.0000 48.0000 18.0000 13.0000 21.0000      NA      NA\n [226]      NA 34.0000 40.0000 36.0000 50.0000 39.0000 56.0000 28.0000 56.0000\n [235] 56.0000 24.0000 18.0000      NA 24.0000 23.0000 45.0000 40.0000  6.0000\n [244] 57.0000      NA 32.0000 62.0000 54.0000 43.0000 52.0000      NA 62.0000\n [253] 67.0000 63.0000 61.0000 46.0000 52.0000 39.0000 18.0000 48.0000      NA\n [262] 49.0000 39.0000 17.0000 46.0000      NA 31.0000      NA 61.0000 47.0000\n [271] 64.0000 60.0000 60.0000 55.0000 54.0000 21.0000 57.0000 45.0000 31.0000\n [280] 50.0000 50.0000 27.0000 20.0000 51.0000      NA 21.0000      NA      NA\n [289] 36.0000      NA      NA      NA      NA      NA      NA      NA      NA\n [298]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [307] 40.0000      NA      NA 32.0000      NA      NA      NA      NA      NA\n [316]      NA 33.0000      NA      NA      NA      NA      NA 30.0000 28.0000\n [325] 18.0000      NA 34.0000 32.0000 57.0000 18.0000 23.0000 36.0000 28.0000\n [334] 51.0000 32.0000 19.0000 28.0000 36.0000  4.0000  1.0000 12.0000 34.0000\n [343] 19.0000 23.0000 26.0000      NA 27.0000 15.0000 45.0000 40.0000 20.0000\n [352] 25.0000 36.0000 25.0000      NA 42.0000 26.0000 26.0000  0.8333 31.0000\n [361]      NA 19.0000 54.0000 44.0000 52.0000 30.0000 30.0000      NA      NA\n [370] 29.0000      NA 29.0000 27.0000 24.0000 35.0000 31.0000  8.0000 22.0000\n [379] 30.0000      NA 20.0000      NA 21.0000 49.0000  8.0000 28.0000 18.0000\n [388]      NA 28.0000 22.0000 25.0000 18.0000 32.0000 18.0000      NA 42.0000\n [397] 34.0000  8.0000      NA      NA 23.0000 21.0000 19.0000      NA      NA\n [406]      NA 38.0000      NA 38.0000 35.0000 35.0000 38.0000 24.0000 16.0000\n [415] 26.0000 45.0000 24.0000 21.0000 22.0000      NA 34.0000 30.0000 50.0000\n [424] 30.0000 23.0000  1.0000 44.0000 28.0000  6.0000 30.0000      NA 43.0000\n [433] 45.0000  7.0000 24.0000 24.0000 49.0000 48.0000      NA 34.0000 32.0000\n [442] 21.0000 18.0000 53.0000 23.0000 21.0000      NA 52.0000 42.0000 36.0000\n [451] 21.0000 41.0000      NA      NA 33.0000 17.0000      NA      NA      NA\n [460]      NA      NA      NA 23.0000 34.0000      NA 22.0000      NA      NA\n [469] 45.0000      NA      NA 31.0000 30.0000 26.0000      NA 34.0000 26.0000\n [478] 22.0000  1.0000  3.0000      NA      NA      NA 25.0000      NA 48.0000\n [487]      NA 57.0000      NA      NA      NA  2.0000      NA 27.0000 19.0000\n [496] 30.0000 20.0000 45.0000      NA 46.0000 41.0000 13.0000 19.0000 30.0000\n [505] 48.0000 71.0000 54.0000      NA      NA 64.0000 32.0000 18.0000  2.0000\n [514] 32.0000  3.0000 26.0000 19.0000      NA 20.0000 29.0000 39.0000 22.0000\n [523]      NA 24.0000      NA 28.0000      NA 50.0000 20.0000 40.0000 42.0000\n [532] 21.0000 32.0000 34.0000      NA      NA 33.0000  2.0000  8.0000 36.0000\n [541] 34.0000 30.0000 28.0000 23.0000  0.8333 25.0000  3.0000 50.0000      NA\n [550] 21.0000      NA      NA 25.0000 18.0000 20.0000 30.0000 59.0000 30.0000\n [559] 35.0000 22.0000      NA 25.0000 41.0000 25.0000 14.0000 50.0000 22.0000\n [568]      NA 27.0000 29.0000 27.0000 30.0000 22.0000 35.0000 30.0000 28.0000\n [577] 23.0000      NA 12.0000 40.0000 36.0000 28.0000 32.0000 29.0000  4.0000\n [586]  2.0000      NA      NA 36.0000 33.0000      NA      NA      NA 32.0000\n [595]      NA      NA 26.0000      NA 30.0000 24.0000      NA 18.0000 42.0000\n [604] 13.0000 16.0000 35.0000 16.0000 25.0000 18.0000 20.0000 30.0000 26.0000\n [613] 40.0000 24.0000 41.0000 18.0000  0.8333 23.0000 20.0000 25.0000 35.0000\n [622] 17.0000 32.0000 20.0000 39.0000 39.0000  6.0000  2.0000 17.0000 38.0000\n [631]  9.0000 26.0000 11.0000  4.0000 20.0000 26.0000 25.0000 18.0000 24.0000\n [640] 35.0000 40.0000 38.0000  5.0000  9.0000  3.0000 13.0000 23.0000  5.0000\n [649]      NA 45.0000 23.0000 17.0000 27.0000 23.0000 20.0000 32.0000 33.0000\n [658]  3.0000      NA      NA      NA 18.0000 40.0000 26.0000 15.0000 45.0000\n [667] 18.0000 27.0000 22.0000 19.0000 26.0000 22.0000 20.0000 32.0000 21.0000\n [676] 18.0000 26.0000  6.0000      NA      NA  9.0000 40.0000 32.0000      NA\n [685] 26.0000 18.0000 20.0000      NA 29.0000 22.0000 22.0000 35.0000 21.0000\n [694] 20.0000 19.0000 18.0000 18.0000 38.0000      NA 30.0000 17.0000 21.0000\n [703] 21.0000 21.0000      NA      NA 24.0000 33.0000 33.0000 28.0000 16.0000\n [712] 37.0000 28.0000      NA 24.0000 21.0000      NA 32.0000 29.0000 26.0000\n [721] 18.0000 20.0000 19.0000 24.0000 24.0000 36.0000 31.0000 31.0000 30.0000\n [730] 22.0000      NA 43.0000 35.0000 27.0000 19.0000 30.0000 36.0000  3.0000\n [739]  9.0000 59.0000 19.0000 44.0000 17.0000      NA 45.0000 22.0000 19.0000\n [748] 29.0000 30.0000 34.0000 28.0000  0.3333 27.0000 25.0000 24.0000 22.0000\n [757] 21.0000 17.0000      NA      NA 26.0000 33.0000  1.0000  0.1667 25.0000\n [766] 36.0000 36.0000 30.0000      NA 23.0000 26.0000 19.0000 65.0000      NA\n [775] 42.0000 43.0000 32.0000 19.0000 30.0000 24.0000 23.0000      NA 24.0000\n [784] 24.0000 23.0000 22.0000      NA 18.0000 16.0000 45.0000      NA      NA\n [793]      NA 47.0000  5.0000      NA      NA      NA      NA      NA      NA\n [802]      NA      NA      NA      NA      NA 21.0000 18.0000  9.0000 48.0000\n [811] 16.0000      NA      NA 25.0000      NA      NA 22.0000 16.0000      NA\n [820] 33.0000      NA  9.0000 41.0000 38.0000 40.0000 43.0000 14.0000 16.0000\n [829]  9.0000 10.0000  6.0000 11.0000 40.0000 32.0000      NA 20.0000 37.0000\n [838] 28.0000 19.0000      NA      NA      NA      NA      NA      NA      NA\n [847]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [856]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [865]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [874]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [883]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [892]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [901]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [910]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [919]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [928]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [937]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [946]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [955]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [964]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [973]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [982]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [991]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1000]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1009]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1018]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1027]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1036]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1045]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1054]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1063]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1072]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1081]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1090]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1099]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1108]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1117]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1126]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1135]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1144]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1153]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1162]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1171]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1180]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1189]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1198]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1207]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1216]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1225]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1234]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1243]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1252]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1261]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1270]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1279]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1288]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1297]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1306]      NA      NA      NA      NA      NA      NA      NA      NA\n\n\n\nIf you look through the results, you will see that most individuals have numbers in this list, but some have NA. These NA values are the people for which we do not have age information - missing data.\nBy the way, the titanic.csv file simply has nothing in the places where there is missing data. When R loaded it, it replaced the empty spots with NA automatically."
  },
  {
    "objectID": "lab02-descriptives.html#measures-of-location",
    "href": "lab02-descriptives.html#measures-of-location",
    "title": "Lab 02 Describing data",
    "section": "4 Measures of location",
    "text": "4 Measures of location\nFor this lab we will use R to give some basic descriptive statistics for numerical data.\n\n4.1 mean()\nWe have already seen in lab 1 how to calculate the mean of a vector of data using mean(). Unfortunately, if there are missing data we need to tell R how to deal with it.\nA (somewhat annoying) quirk of R is that if we try to take the mean of a list of numbers that include missing data, we get an NA for the result!\n\n# mean with missing data - doh!\nmean(titanicData$age)\n\n[1] NA\n\n\n\n\n\n4.2 na.rm = TRUE\nTo get the mean of all the numbers that we do have, we have to add an argument to the mean() function, na.rm = TRUE.\n\n# tell R to exclude NAs\nmean(titanicData$age, na.rm = TRUE)\n\n[1] 31.19418\n\n\n\nThis tells R to remove (“rm”) the NAs (“na”) before calculating the mean. It turns out that the mean age of passengers that we have information for was about 31.2.\nna.rm = TRUE can be added to many functions in R, including median(), as we shall see next.\n\n\n\n4.3 median()\nThe median of a series of numbers is the “middle” number – half of the numbers in the list are greater the median and half are below it. It can be calculated in R by using median().\n\n# calculate median\nmedian(titanicData$age, na.rm = TRUE)\n\n[1] 30\n\n\n\n\n\n4.4 summary()\nA handy function that will return both the mean and median at the same time, along with other information such as the first and third quartiles, is summary().\n\n# automatic summary information\nsummary(titanicData$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1667 21.0000 30.0000 31.1942 41.0000 71.0000     680 \n\n\n\nFrom left to right, this output gives us\n\nthe smallest (minimum) value in the list (“Min.”)\nthe first quartile (“1st Qu.” - the value where 25% of the values are smaller)\nthe median\nthe mean\nthe third quartile (“3rd Qu.” - 75% of values are smaller)\nthe largest (maximum) value (“Max.”)\nthe number of individuals with missing values (”NA’s”)\n\n\nThe first quartile is the value in the data that is larger than a quarter of the data points. The third quartile is larger than 3/4 of the data. These are also called the 25th percentile and the 75th percentile, respectively. (You may remember these from boxplots, where the top and bottom of the box mark the 75th and 25th percentiles, respectively.)"
  },
  {
    "objectID": "lab02-descriptives.html#measures-of-variability",
    "href": "lab02-descriptives.html#measures-of-variability",
    "title": "Lab 02 Describing data",
    "section": "5 Measures of variability",
    "text": "5 Measures of variability\nR can also calculate measures of the variability of a sample. In this section we’ll learn how to calculate the variance, standard deviation, coefficient of variation and interquartile range of a set of data.\n\n\n5.1 var()\nTo calculate the variance of a list of numbers, use var().\n\n# variance is big here!\nvar(titanicData$age, na.rm = TRUE)\n\n[1] 217.4895\n\n\n\nNote that var(), as well as sd() below, have the same need for na.rm = TRUE when analyzing data that include missing values.\n\n\n\n5.2 sd()\nThe standard deviation can be calculated by sd().\n\n\n# standard deviation\nsd(titanicData$age, na.rm = TRUE)\n\n[1] 14.74753\n\n\n\nOf course, the standard deviation is the same as the square root of the variance.\n\nsqrt(var(titanicData$age, na.rm = TRUE))\n\n[1] 14.74753\n\n\n\n\n\n5.3 Coefficient of variation\nThe variance of a measure is positively correlated with it’s mean. We can standardize the measure of variation according to the mean (so that variation can be compared between variables with different means) using the coefficient of variation, the `c.v.’.\n\\(c.v. = (standard\\ deviation) / mean * 100\\)\nThere is no standard function in R to calculate the coefficient of variation. You can do this yourself, though, directly from the definition:\n\n# calculate the cv\nsd(titanicData$age, na.rm = TRUE) / mean(titanicData$age, na.rm = TRUE) * 100\n\n[1] 47.27653\n\n\n\n\n\n5.4 IQR()\nThe interquartile range (or IQR) is the difference between the third quartile and the first quartile; in other words the range covered by the middle half of the data. It can be calculated easily with IQR().\n\n# Interquartile range\nIQR(titanicData$age, na.rm = TRUE)\n\n[1] 20\n\n\n\nNote that this is the same as we could calculate by the results from summary() above. The third quartile is 41 and the first quartile is 21, so the difference is 41 – 21 = 20.\n\n\n\n5.5 Confidence intervals (CI) of the mean\nThe confidence interval for an estimate tells us a range of values that is likely to contain the true value of the parameter. For example, in 95% of random samples from a population, the 95% confidence interval of the mean will contain the true value of the population mean.\nR does not have a built-in function to calculate only the confidence interval of the mean, but the function that calculates t-tests will give us this information. The function t.test() has many results in its output. By adding $conf.int to this function we can slice out only the confidence interval for the mean. By default it gives us the low and high values for the 95% confidence interval of a variable.\n\n# 95 CI\nt.test(titanicData$age)$conf.int\n\n[1] 30.04312 32.34524\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\nAs the result above shows, the 95% confidence interval of the mean of age in the titanicData data set is from about 30.0 to 32.3. R also tells us that it used a 95% confidence level for its calculation. (The confidence interval is not so useful in this case, because we actually have information for nearly all the individuals on the Titanic.)\nTo calculate confidence intervals with a different level of confidence, we can add the option conf.level to the t.test function. For example, for a 99% confidence interval we can use the following.\n\n\n# Specify confidence interval with conf.level argument\nt.test(titanicData$age, conf.level = 0.99)$conf.int\n\n[1] 29.67976 32.70861\nattr(,\"conf.level\")\n[1] 0.99"
  },
  {
    "objectID": "lab02-descriptives.html#sampling-exercise",
    "href": "lab02-descriptives.html#sampling-exercise",
    "title": "Lab 02 Describing data",
    "section": "6 Sampling exercise",
    "text": "6 Sampling exercise\n\n6.1 Distribution of sample means\nGo to the web and open the page at Gaussian sampling exercise. This page contains some interactive visualizations that let you play around with sampling to see the distribution of sampling means. Click the button that says “Tutorial” near the bottom of the page and follow along with the instructions.\n\n\n\n6.2 Confidence intervals\nGo back to the web, and open the page at Confidence interval exercise. This page draws confidence intervals for the mean of a known population. Click “Tutorial” again, and follow along."
  },
  {
    "objectID": "lab02-descriptives.html#challenge-questions",
    "href": "lab02-descriptives.html#challenge-questions",
    "title": "Lab 02 Describing data",
    "section": "7 Challenge questions",
    "text": "7 Challenge questions\n\nMake a script in RStudio that collects all your R code required to answer the following questions. Include answers to the qualitative questions using comments.\n\n\n\n7.1\nUse the data file MancHeight.csv for this question - it contains height, gender and the number of siblings for university statistics students from Manchester. These data were collected using measuring tapes in units of cm. Open that file in R.\nPlot the distribution of heights in the class. Describe the shape of the distribution. Is it symmetric or skewed? Is it unimodal (one distinct peak), bimodal (two discernible peaks) or something else? ? Key point: A distribution is skewed if it is asymmetric. A distribution is skewed right if there is a long tail to the right, and skewed left if there is a long tail to the left.\nAre there any large or small outliers that look as though a student used the wrong units for their height measurement. (e.g., are there any that are more plausibly a height given in inches rather than the requested centimeters?) If so, and if this is not likely to be an accurate description of an individual in your class, use filter() from the package dplyr to create a new data set without those rows.\nUse R to calculate the mean height of all students in the class.\nUse sd() to calculate the standard deviation of height.\n\n\n\n7.2\nThe file caffeine.csv contains data on the amount of caffeine in a 16 oz. cup of coffee obtained from various vendors. For context, doses of caffeine over 25 mg are enough to increase anxiety in some people, and doses over 300 to 360 mg are enough to significantly increase heart rate in most people. A can of Red Bull contains 80mg of caffeine.\nWhat is the mean amount of caffeine in 16 oz. coffees?\nWhat is the 95% confidence interval for the mean?\nPlot the frequency distribution of caffeine levels for these data in a histogram. Comment on whether you think the amount of caffeine in a cup of coffee is relatively consistent from one vendor to another? What is the standard deviation of caffeine level? What is the coefficient of variation?\nThe file caffeineCosta.csv has data on six 16 oz. cups of Breakfast Blend coffee sampled on six different days from a Costa location. Calculate the mean (and the 95% confidence interval for the mean) for these data. Compare these results to the data taken on the broader sample of vendors in the first file. Describe the difference.\n\n\n\n7.3\nA confidence interval is a range of values that are likely to contain the true value of a parameter. Consider the caffeine.csv data again.\nCalculate the 99% confidence interval for the mean caffeine level.\nCompare this 99% confidence interval to the 95% confidence interval you calculate in question 2 above. Which confidence interval is wider (i.e., spans a broader range)? Why should this one be wider?\nLet’s compare the quantiles of the distribution of caffeine to this confidence interval. Approximately 95% of the data values should fall between the 2.5% and 97.5% quantiles of the distribution of caffeine. (Explain why this is true.) We can use R to calculate the 2.5% and 97.5% quantiles with a command like the following. (Replace “datavector” with the name of the vector of your caffeine data.)\nquantile(datavector, c(0.025, 0.975), na.rm =TRUE)\nAre these the same as the boundaries of the 95% confidence interval? If not, why not? Which should bound a smaller region, the quantile or the confidence interval of the mean?\n\n\n\n7.4\nReturn to the class data set Mancheight.csv. Find the mean value of sibs. Add one to this to find the mean number of children per family in the class.\nThe mean number of offspring per family twenty years ago was about 2. Is the value for this class similar, greater, or smaller? If different, think of reasons for the difference.\nAre the families represented in this class systematically different from the population at large? Is there a potential sampling bias?\nConsider the way in which the data were collected. How many families with zero children are represented? Why? What effect does this have on the estimated mean family size of all couples?\n\n\n\n7.5\nReturn to the data on countries of the world, in countries.csv. Plot the distributions for ecological_footprint_2000, cell_phone_subscriptions_per_100_people_2012, and life_expectancy_at_birth_female.\nFor each variable, plot a histogram of the distribution using hist(). Is the variable skewed? If so, in which direction?\nFor each variable, calculate the mean and median and add them to their respective plots using the function abline() using the v argument. Are the mean and median similar for each? Match the difference in mean and median to the direction of skew on the histogram. Do you see a pattern?"
  },
  {
    "objectID": "lab02-descriptives.html#harper-adams-data-science",
    "href": "lab02-descriptives.html#harper-adams-data-science",
    "title": "Lab 02 Describing data",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab03-frequency.html#objectives",
    "href": "lab03-frequency.html#objectives",
    "title": "Lab 03 Frequency data",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nCalculate a confidence interval for a proportion\nMake a hypothesis test about proportions\nFit frequency data to a model\nTest for a fit to a Poisson distribution\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab03-frequency.html#start-a-script",
    "href": "lab03-frequency.html#start-a-script",
    "title": "Lab 03 Frequency data",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab03-frequency.html#confidence-interval-for-a-proportion",
    "href": "lab03-frequency.html#confidence-interval-for-a-proportion",
    "title": "Lab 03 Frequency data",
    "section": "3 Confidence interval for a proportion",
    "text": "3 Confidence interval for a proportion\nTo calculate a confidence interval for an estimate of a proportion, we suggest using the Agresti-Coull method. This method is available in the R package “binom”, which you can install and load with the following:\n\n# If needed, install {binom}\n# install.packages(\"binom\", dependencies = TRUE)\nlibrary(binom)\n\nWarning: package 'binom' was built under R version 4.1.1\n\n\n\nLike any package, you only need to install it once, but you need to run the library() command in each session before you can use it. Once the package is installed, calculating a confidence interval for a proportion is fairly straightforward. The function you need is called binom.confint(). You need to specify:\n\nx: the number of successes\nn: the total sample size\nmethod: “ac” for “Agresti-Coull”\n\n\nFor example, if we have 87 data points and 30 of them are “successes”, we can find the 95% confidence interval for the proportion of successes with the following command.\n\nbinom.confint(x = 30, n = 87, method = \"ac\")\n\n         method  x  n      mean     lower     upper\n1 agresti-coull 30 87 0.3448276 0.2532164 0.4495625\n\n\n\nThis tells us that the observed proportion (called “mean” here) is 0.3448, and the lower and upper bounds of the 95% confidence interval are 0.2532 and 0.4496."
  },
  {
    "objectID": "lab03-frequency.html#the-binomial-tests",
    "href": "lab03-frequency.html#the-binomial-tests",
    "title": "Lab 03 Frequency data",
    "section": "4 The Binomial tests",
    "text": "4 The Binomial tests\nDoing a binomial test in R is much easieR (see what I did theRe!?) than doing one by hand. You use the binomial test to decide whther some observed count of data with 2 possible outcomes is different than expected (e.g. if you count 100 coin flips you might expect 50:50)\n\n\n4.1 binom.test()\nThe function binom.test() will do an exact binomial test. It requires three pieces of information in the input:\nx for the number of “successes” observed in the data,\nn for the total number of data points, and\np for the proportion given by the null hypothesis.\n\nFor example, if we have 18 toads that have been measured for a left-right preference and 14 are right-handed, we can test the null hypothesis of equal probabilities of left- and right-handedness with a binomial test. In this case, n = 18, x = 14, and p = 0.5.\n\nbinom.test(x = 14, n = 18, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  14 and 18\nnumber of successes = 14, number of trials = 18, p-value = 0.03088\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5236272 0.9359080\nsample estimates:\nprobability of success \n             0.7777778 \n\n\n\nIn this case, the output of the function gives quite a bit of information. One key element that we will be looking for is the P-value; in this case R tells us that the P-value is 0.03088. This is the P-value that corresponds to a two-tailed test.\nThe binom.test() function also gives an estimate of the proportion of successes (in this case 0.7777778). (It also gives an approximate 95% confidence interval for the proportion using a different method than the Agresti-Coull method that we recommend.)"
  },
  {
    "objectID": "lab03-frequency.html#goodness-of-fit-test",
    "href": "lab03-frequency.html#goodness-of-fit-test",
    "title": "Lab 03 Frequency data",
    "section": "5 Goodness of fit test",
    "text": "5 Goodness of fit test\nA chi-squared (\\(\\chi^2\\)) goodness-of-fit test compares the frequencies of values of a categorical variable to the frequencies predicted by a null hypothesis. For example, the file MandMlist.csv contains a list of all the colors of M&M candies from one package, under the variable “color”. Let’s ask whether this fits the proportions of colors advertised by the candy company.\n\n# Read in the data, NB your file path may be different\nMMlist <- read.csv(\"data/MandMlist.csv\")\n\n# MMlist$color contains the color of each of 55 M&Ms.\nMMlist$color\n\n [1] \"brown\"  \"blue\"   \"blue\"   \"green\"  \"yellow\" \"yellow\" \"orange\" \"yellow\"\n [9] \"orange\" \"yellow\" \"red\"    \"green\"  \"green\"  \"orange\" \"orange\" \"yellow\"\n[17] \"green\"  \"blue\"   \"red\"    \"brown\"  \"green\"  \"orange\" \"blue\"   \"yellow\"\n[25] \"green\"  \"yellow\" \"orange\" \"brown\"  \"green\"  \"blue\"   \"brown\"  \"red\"   \n[33] \"green\"  \"orange\" \"blue\"   \"blue\"   \"green\"  \"orange\" \"red\"    \"yellow\"\n[41] \"red\"    \"blue\"   \"brown\"  \"yellow\" \"blue\"   \"orange\" \"green\"  \"green\" \n[49] \"green\"  \"blue\"   \"orange\" \"yellow\" \"yellow\" \"orange\" \"red\"   \n\n\n\n\n5.1 table()\nSummarizing frequency data in a table is useful to see the data more concisely, and such a table is also necessary as input to the χ2 test function. We can summarize the counts (frequencies) of a categorical variable with table().\n\n# create table\nMMtable <- table(MMlist$color)\n\n# print table\nMMtable\n\n\n  blue  brown  green orange    red yellow \n    10      5     12     11      6     11 \n\n\nThis shows that in this list of M&M colors, 10 were blue, 5 were brown, etc.\n\n\n\n5.2 chisq.test()\nWe can use a χ2 goodness-of-fit test to compare the frequencies in a set of data to a null hypothesis that specifies the probabilities of each category. In R, this can be calculated using the function chisq.test().\nThe company says that the percentages are 24% blue, 14% brown, 16% green, 20% orange, 13% red and 13% yellow. This is our null hypothesis. Let’s test whether these proportions are consistent with the frequencies of the colors in this bag. R requires you to proivide the proportions expected by the null hypothesis in a vector, like this:\n\nexpected_proportions <- c(0.24, 0.14, 0.16, 0.20, 0.13, 0.13)\n\nThese expected proportions will use to calculate the expected frequencies.\n\nThe first thing we need to do is check whether the expected frequencies (expected by the null hypothesis) are large enough to justify using a χ2 goodness of fit test. (In other words, that no more than 25% of the expected frequencies are less than 5 and none is less than 1.)\nTo obtain these expected counts, multiply the vector of the expected proportions by the total sample size. We can find the total sample size by taking the sum() of the frequency table:\n\nsum(MMtable)\n\n[1] 55\n\n\n\nSo for this example, the expected frequencies from our null hypothesis are 55 multiplied by the list of expected proportions:\n\n55 * expected_proportions\n\n[1] 13.20  7.70  8.80 11.00  7.15  7.15\n\n\nAll of these expected values are greater than 5, so we have no problem with the assumptions of the χ2 goodness of fit test. (If there were a problem here, we’d have to combine categories. We’ll see an example of that in the next section.)\n\nThe function chisq.test() requires two arguments in its input: a frequency table from the data and a vector with the expected proportions from the null hypothesis. The name of the vector of expected proportions in this function is p. So here is the format of the R command to do a χ2 test:\n\nchisq.test(MMtable, p = expected_proportions)\n\n\n    Chi-squared test for given probabilities\n\ndata:  MMtable\nX-squared = 5.1442, df = 5, p-value = 0.3985\n\n\n“X–squared” here in the output means χ2. The χ2 value for this test is 5.1442. There are 5 degrees of freedom (df) for this test, and the P-value turned out to be 0.3985. We wouldn’t reject the null hypothesis with these data."
  },
  {
    "objectID": "lab03-frequency.html#fit-to-a-poisson-distribution",
    "href": "lab03-frequency.html#fit-to-a-poisson-distribution",
    "title": "Lab 03 Frequency data",
    "section": "6 Fit to a Poisson distribution",
    "text": "6 Fit to a Poisson distribution\nA χ2 goodness of fit test can be used to ask whether a frequency distribution of a variable is consistent with a specific distribution, like the Poisson distribution. This is a useful way to determine if the count of events in space or time are not “random” but instead are clumped or dispersed.\nIt turns out that this is not a straightforward process in R. In this section, we’ll highlight a couple of functions in R that will streamline doing the calculations by hand. We’ll use the data on the numbers of historical extinctions in the data file MassExtinctions.csv.\n\nMass extinctions Do extinctions occur randomly in Earth’s history, or are there some periods when extinctions occur at a rate higher than we would expect at random, i.e., is there evidence of “mass extinctions”? Fossil marine invertebrates constitute what is considered the best fossil record of extinctions, because they have structures that tend to preserve well. The data we examine here is a survey of the recorded number of extinctions of marine invertebrate families in 76 time blocks of similar duration through the fossil record (Raup & Seposki 1982).\n\n\n\n# Load the data, you will probably need to supply your own file path\nextinctData <- read.csv(\"data/MassExtinctions.csv\")\n\nnumber_of_extinctions <- extinctData$numberOfExtinctions\n\ntable(number_of_extinctions)\n\nnumber_of_extinctions\n 1  2  3  4  5  6  7  8  9 10 11 14 16 20 \n13 15 16  7 10  4  2  1  2  1  1  1  2  1 \n\n\n\nThe first row lists each possible number of extinctions in a time period, ranging from 1 to 20. The second row shows the number of time periods that had each number of extinctions. (so there were 13 time periods with one extinction, 15 time periods with 2 extinctions, etc.)\nThe mean number of extinctions per unit of time is not specified by the null hypothesis, so we need to estimate it from the data. This mean turns out to be 4.21 extinctions per time period.\n\nmean(number_of_extinctions)\n\n[1] 4.210526\n\n\n\nWe can use this mean to calculate the probability according to a Poisson distribution of getting each specific number of extinctions.\n\n\n6.1 dpois()\nFortunately, R has a function to calculate the probability of a given value under the Poisson distribution. This function is dpois(), which stands for “density of the Poisson”. dpois() requires two values for input, the mean (which is called lambda in this function) and x, the value of the outcome we are interested in (here, the number of extinctions). For example, the probability of getting x = 3 from a Poisson distribution with mean 4.21 can be found from the following:\n\ndpois(x = 3, lambda = 4.21)\n\n[1] 0.1846355\n\n\n\nWe see that the probability of getting exactly 3 successes from a Poisson distribution with mean 4.21 is about 18.5%.\ndpois() will accept a vector of xs as input, and return a vector of the probabilities of those xs. A convenient short hand to know here is that R will create a vector with a range of values with a colon. For example, 0:20 is a vector with the integers from 0 to 20.\n\n0:20\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\nWe’ll need the probabilities for all possible values of x from 0 to 20 (because this was the largest value observed in the data). We can create a vector with the Poisson probabilities of each possibility from 0 to 20 like this:\n\nexpected_probability <- dpois(x = 0:20, lambda = 4.21)\n\nexpected_probability\n\n [1] 1.484637e-02 6.250321e-02 1.315693e-01 1.846355e-01 1.943289e-01\n [6] 1.636249e-01 1.148102e-01 6.905011e-02 3.633762e-02 1.699793e-02\n[11] 7.156129e-03 2.738846e-03 9.608784e-04 3.111768e-04 9.357530e-05\n[16] 2.626347e-05 6.910575e-06 1.711384e-06 4.002736e-07 8.869220e-08\n[21] 1.866971e-08\n\nround(expected_probability, 8)\n\n [1] 0.01484637 0.06250321 0.13156926 0.18463553 0.19432889 0.16362493\n [7] 0.11481016 0.06905011 0.03633762 0.01699793 0.00715613 0.00273885\n[13] 0.00096088 0.00031118 0.00009358 0.00002626 0.00000691 0.00000171\n[19] 0.00000040 0.00000009 0.00000002\n\n\n\n\nNB! The odd numerical notation here is a shorthand for scientific notation. 1.48e-02 means 1.48 x 10-2, which is equal to 0.0148. The e stands for exponent.\n\n\nRemember that we asked R to output the probability of values from 0 to 20. So the first value in this output corresponds to the probability of getting a zero, the second to the probability of getting a 1, etc.\n\n\n6.2 Example chi squared test\nIn order to convert these probabilities into expected values for a χ2 test, we need to multiply them by the total sample size.\n\n# the expected number of extinctions\nround(length(number_of_extinctions) * expected_probability, 8)\n\n [1]  1.12832399  4.75024401  9.99926365 14.03229999 14.76899574 12.43549441\n [7]  8.72557191  5.24780825  2.76165909  1.29184275  0.54386580  0.20815227\n[13]  0.07302676  0.02364943  0.00711172  0.00199602  0.00052520  0.00013007\n[19]  0.00003042  0.00000674  0.00000142\n\n\n\nMost of these values would cause problems for the χ2 test because they are too small (less than 1). We need to combine categories in some reasonable way so that the expected frequencies match the assumptions of the χ2 test. Let’s combine the categories for 0 and 1 successes, and combine everything greater than or equal to 8 into one category. It is possible to do this all in R, but you can also do it by hand, using the expected frequencies we just calculated. Here is a table of the expected frequencies for these new combined categories, for both the observed and expected, summing over all the possibilities within each of these groupings. (E.g., the expected frequency for the category “0 or 1” is the sum of 1.128324 and 4.750244.)\n\n\n\n\n\nExpected frequencies\n\n\n\n\nNote that these expected values are large enough to match the conditions of the χ2 test.\n\n\n\n6.3 Do the test\nMake vectors (using c()) for the observed and expected values for these new groups:\n\nexpected_combined <- c(5.878568, 9.999264, 14.032300, 14.768996, 12.435494,  8.725572, 5.247808, 4.911998)\n\nobserved_combined <- c(13, 15, 16, 7, 10, 4, 2, 9)\n\nFrom these we can calculate the χ2 test statistic using chisq.test()$statistic.\n\nFirst we give it the list of observed values, then the expected values as \\(p\\). Because we are giving the list of expected frequencies rather the expected probabilities, we need to give it the option rescale.p = TRUE. Finally, by adding $statistic at the end, R will only give us the χ2 value as output. (We don’t yet want the full results of the test because R does not know to correct for the reduction in degrees of freedom caused by estimating the mean.)\n\nchisq.test(observed_combined, p = expected_combined, rescale.p = TRUE)$statistic\n\nWarning in chisq.test(observed_combined, p = expected_combined, rescale.p =\nTRUE): Chi-squared approximation may be incorrect\n\n\nX-squared \n 23.93919 \n\n\n\nThis correctly calculates our χ2 value as 23.939. The warning message is because one of the expected values is under 5, but because this occurred in less than 20% of the categories, it is fine to proceed.\nThe degrees of freedom for this test are the number of categories minus the number of parameters estimated from the data minus one. We ended up using 8 categories and estimated one parameter from the data, so we have df = 8 – 1 – 1 = 6.\n\n\n\n6.4 pchisq()\nTo calculate the correct P-value, we need to know probability of getting a χ2 value greater than 23.939 from a distribution that has 6 degrees of freedom. R has a function we can use for this, called pchisq(). We are interested here in the probability above the observed χ2 value, so we want the probability in the right (or upper) tail. To use this function, we need to specify three values: our observed χ2 (confusingly called \\(q\\) in this function), the degrees of freedom (called df, thank goodness), and an option that tells us to use the upper tail (lower.tail = FALSE).\n\npchisq(q = 23.939, df = 6, lower.tail = FALSE)\n\n[1] 0.0005359236\n\n\nThus, our P-value for this test of the fit of these data to a Poisson distribution is approximately 0.0005. We reject the null hypothesis that these extinction data follow a Poisson distribution."
  },
  {
    "objectID": "lab03-frequency.html#r-commands-summary",
    "href": "lab03-frequency.html#r-commands-summary",
    "title": "Lab 03 Frequency data",
    "section": "7 R commands summary",
    "text": "7 R commands summary\n\n\n\n\ncommands summary"
  },
  {
    "objectID": "lab03-frequency.html#exercises",
    "href": "lab03-frequency.html#exercises",
    "title": "Lab 03 Frequency data",
    "section": "8 Exercises",
    "text": "8 Exercises\n\nMake a script in RStudio that collects all your R code required to answer the following questions. Include answers to the qualitative questions using comments.\n\n\n\n8.1\nMany hospitals (and airplanes) have signs posted banning mobile phone use. These bans originated from studies on early versions of mobile phones. In one such experiment, out of 510 tests with cell phones operating at near-maximum power, six disrupted a piece of medical equipment enough to hinder interpretation of data or cause equipment to malfunction. A more recent study found zero instances of disruption of medical equipment out of 300 tests.\nFor the older data, use binom.confint() with the Agresti-Coull method to calculate the estimated proportion of equipment disruption. What is the 95% confidence interval for this proportion?\nFor the data on the newer cell phones, use R to calculate the estimate of the proportion and its 95% confidence interval.\n\n\n\n8.2\nIt is difficult to tell what other people are thinking, and it may even be impossible to find out what they are thinking by asking them. A series of studies shows that we do not always know how our own thought processes are carried out.\nA classic experiment by Wilson and Nisbett (1978) addressed this issue in a clever way. Participants were asked to decide which of four pairs of silk stockings were better, but the four stockings that they were shown side-by-side were in fact identical. Nearly all participants were able, however, to offer reasons that they had chosen one pair over the other.\nThe four pairs of stockings were presented to the participants randomly with respect to which pair was in which position. However, of the 52 subjects who selected a pair of stockings, 6 chose the pair on the far left, 9 chose the pair in the left-middle, 16 chose the pair in the right-middle, and 21 chose the pair on the far right. None admitted later that the position had any role in their selection. These data are in a file called stockings.csv.\n\n\nWhat are the expected frequencies for this scenario, under the null hypothesis that all four pairs of stockings are equally likely to be chosen?\n\n\nUse chisq.test() to test the null hypothesis that the selection of the stockings was independent of position.\n\nThe function chisq.test() can take the data either as a data frame, as above, or as a vector of the observed counts, as a parameter called x as input:\n\nchisq.test(x = c(6,9,16,21), p = c(0.25,0.25,0.25,0.25))\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(6, 9, 16, 21)\nX-squared = 10.615, df = 3, p-value = 0.014\n\n\nTry it using the specification of the counts, to see that you get the same answer as above.\n\n\n\n8.3\nMany people believe that the month in which a person is born predicts significant attributes of that person in later life. Such astrological beliefs have no scientific support, but are there circumstances in which birth month could have a strong effect on later life? One prediction is that elite athletes will disproportionately have been born in the months just after the age cutoff used to separate levels for young players of the sport. The prediction is that those athletes that are oldest within an age group will do better by being relatively older, and therefore will gain more confidence and attract more coaching attention than the relatively younger players in their same groups. As a result, they may be more likely to dedicate themselves to the sport and do well later. In the case of football, the cutoff for different age groups is generally August.\nThe birth months (by three month interval) of football players competing in the Under-20’s World Tournament are recorded in the data file football_birth_quarter.csv (from Barnsley et al. 1992). Plot these data. Do you see a pattern?\nThe numbers of people born calculated from a large sample is recorded in the file births.csv. Compare the distribution of birth months of the football players to what would be expected by chance, assuming that the birth data for the same in “births.csv” is a good approximation for the population from which recorded players are drawn. Use the appropriate statistical test. Describe the pattern.\n\n\n\n8.4\nIs cardiac arrest equally likely to occur throughout the year? Or are some weeks more likely than others to produce cardiac arrests? One way to look at this issue is to ask whether cardiac arrests occur according to a probability model in which such events are independent of each other and equally likely at all times. If so, then the number of cardiac arrests per week will follow a Poisson distribution.\nThe data file “cardiacArrest.csv” contains data on the number of cardiac arrests per week from one hospital over five years. It records the number of cardiac arrests that occurred to individuals outside of the hospital who were then brought in for treatment (from Skogvoll and Lindqvist 1999).\n\nCalculate a table of the observed frequencies of cardiac arrests per week.\nWhat is the mean number of cardiac arrests per week?\nFor the mean you just calculated, use dpois() to calculate the probability of 0 cardiac arrests in a week assuming a Poisson distribution. Multiply that probability by the number of data points to calculate the expected frequency of 0 in these data under the null hypothesis of a Poisson distribution.\nHere is a table of the expected frequencies under the null hypothesis. (The expected frequency for zero cardiac arrests should match your calculation above.) Are these frequencies acceptable for use in a χ2 goodness of fit test?\n\n\n\n\nNumber of cardiac arrests\nExpected\n\n\n\n\n0\n34.785295\n\n\n1\n70.103698\n\n\n2\n70.640891\n\n\n3\n47.454800\n\n\n4\n23.909219\n\n\n5\n9.636973\n\n\n6 or more\n4.469124\n\n\n\n\n\nCreate vectors in R for the observed and expected frequencies.\nCalculate the χ2 for this hypothesis test, using chisq.test()$statistic.\nHow many degrees of freedom should this χ2 goodness of fit test have?\nCalculate the P-value for this test, using pchisq().\nSummarize the results of this test. Does the frequency distribution of out-of-hospital cardiac events follow a Poisson distribution?"
  },
  {
    "objectID": "lab03-frequency.html#harper-adams-data-science",
    "href": "lab03-frequency.html#harper-adams-data-science",
    "title": "Lab 03 Frequency data",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab04-contingency.html#objectives",
    "href": "lab04-contingency.html#objectives",
    "title": "Lab 04 Contingency",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nDisplay associations between two categorical variables in contingency tables and mosaic plots\nCalculate odds ratios and their confidence intervals\nTest for the independence of two categorical variables\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab04-contingency.html#start-a-script",
    "href": "lab04-contingency.html#start-a-script",
    "title": "Lab 04 Contingency",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab04-contingency.html#categorical-variables",
    "href": "lab04-contingency.html#categorical-variables",
    "title": "Lab 04 Contingency",
    "section": "3 Categorical variables",
    "text": "3 Categorical variables\nIn this lab, we will learn how to look at, and test for, the association between two categorical variables. We’ll use the Titanic data set as an example, looking at the association between the sex of passengers and whether they survived the accident. Let’s load the data from the .csv file:\n\n# Load data\n# NB your file path may be different than mine\ntitanicData <- read.csv(\"data/titanic.csv\" )\n\nThe variables we’ll use are sex and survive. The variable survive contains a yes if the individual survived the sinking and a no for those that did not."
  },
  {
    "objectID": "lab04-contingency.html#frequency-table",
    "href": "lab04-contingency.html#frequency-table",
    "title": "Lab 04 Contingency",
    "section": "4 Frequency table",
    "text": "4 Frequency table\nA contingency table is an effective method to see the association between two categorical variables. Moreover, other R functions we will use in this exercise require a contingency table as input.\nA frequency table can be created using a function we saw in the last lab, called table(). To create a contingency table that shows the relationship between two categorical variables, we simply give table() the vectors that contain those two variables. Put the explanatory variable first, and then the response variable after a comma.\n\nsex_survive_table <- table(titanicData$sex, titanicData$survive)\nsex_survive_table\n\n        \n          no yes\n  female 156 307\n  male   708 142\n\n\n\nThis shows us that in the Titanic data set, there are 156 female passengers who did not survive, 307 females who did survive, and similar information for the males.\nIt is useful to keep this frequency table as a named object, as we have done here (sex_survive_table). We shall use this table several times in later analyses.\nSometimes for contingency analysis we have already summarized the counts for each case. In these cases it is useful to be able to create a data table directly from these counts. The following syntax will allow you to create a data table directly:\n\n# Make a table \"manually\"\"\n\nsex_survive_table_direct <- data.frame(no = c(156, 708),\n                                       yes = c(307, 142), \n                                       row.names = c(\"female\", \"male\"))\nsex_survive_table_direct\n\n        no yes\nfemale 156 307\nmale   708 142"
  },
  {
    "objectID": "lab04-contingency.html#mosaic-plots",
    "href": "lab04-contingency.html#mosaic-plots",
    "title": "Lab 04 Contingency",
    "section": "5 Mosaic plots",
    "text": "5 Mosaic plots\nWe often want show associations between categorical variables, and we have options, e.g. using a grouped bar or the he mosaic plot. Mosaic plots are nice because each combination of the variables is represented by a rectangle, and the size of the rectangle is proportional to the number of individuals in that combination.\nR has a function to calculate mosaic plots, with the sensible name mosaicplot(). In its most basic form, you just give it a frequency table as input.\n\n# ugly but functional default mosaic plot\nmosaicplot(sex_survive_table)\n\n\n\n\n\nThis shows the basic pattern. However, this plot can be greatly improved by adding a couple of extra options, to specify color and axes labels. We can add the option color = c(“darkred”, “gold”) to tell R which colors to use for the different response variables. This is a vector of color names that R assigns in the same order as the order of the categories of the variable plotted on the vertical axis (the response variable) starting with the topmost. (R has many named colors, including all the basics like “red”, “orange”, “blue”, etc.)\nWe would also like the axes to have good labels (rather than merely having the category level names). We can specify these with xlab and ylab as options. Let’s simply call the x-axis “Sex” and the y-axis “Survival”. Here’s what the command would now look like:\n\n# better but still slighty ugly\nmosaicplot(sex_survive_table, \n          color = c(\"blue4\", \"gold\"), \n          xlab = \"Sex\", \n          ylab = \"Survival\")\n\n\n\n\n\nIt is much easier now to see in the graph that the majority of females survived whereas the majority of males did not."
  },
  {
    "objectID": "lab04-contingency.html#odds-ratios",
    "href": "lab04-contingency.html#odds-ratios",
    "title": "Lab 04 Contingency",
    "section": "6 Odds ratios",
    "text": "6 Odds ratios\nOne of the ways to measure the strength of the association between two categorical variables is an odds ratio.\nIn R, the simplest way to estimate an odds ratio is to use the command fisher.test(). This function will also perform a Fisher’s exact test (more on that later). The input to this function is a contingency table like the one we calculated above. We’ll use the results in a couple of ways, so let’s save the results in an object. (Here we called it sex_survive_fisher)\n\nsex_survive_fisher <- fisher.test(sex_survive_table)\nsex_survive_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  sex_survive_table\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07759301 0.13384845\nsample estimates:\nodds ratio \n 0.1021212 \n\n\n\nThe output of this function has several parts, two of which we’ll want to look at now for estimating an odds ratio. We can see the specific elements of the output by using the $ character, in a similar way to how we specify variables inside of data frames.\nAdd $estimate after the results of the fisher.test() function call to get the odds ratio estimate. For example, if we want to know the odds ratio for survival as a function of sex for the Titanic data, we write:\n\nsex_survive_fisher$estimate\n\nodds ratio \n 0.1021212 \n\n\n\nThis shows that the odds ratio is about 0.10. Thus, the odds of a male surviving were only about a tenth of the odds of a female surviving. This is:\n(female death / female survival) / (male death / male survival)\n\nThe order of the values in the odds ratios is determined by the order of the values of each variable; by default R uses alphabetical order.\nThis fisher.test() function also calculates the 95% confidence interval for the odds ratio, and assigns it to an output variable called conf.int. We can see the 95% confidence interval for the odds ratio with a command like:\n\nsex_survive_fisher$conf.int\n\n[1] 0.07759301 0.13384845\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThus, the confidence interval for this odds ratio ranges from about 0.078 to about 0.134."
  },
  {
    "objectID": "lab04-contingency.html#χ2-contingency-test",
    "href": "lab04-contingency.html#χ2-contingency-test",
    "title": "Lab 04 Contingency",
    "section": "7 χ2 contingency test",
    "text": "7 χ2 contingency test\nA χ2 contingency analysis allows us to test the null hypothesis that two categorical variables are independent of each other.\nBecause this is simple to interpret and to calculate by hand, it is possibly the most common test used in science. However, the χ2 has assumptions requiring that all of the expected values are greater than 1 and that at least 80% are greater than 5. When doing such a test of independence on a computer, it may sometimes be better to use Fisher’s exact test, which doesn’t have this restriction.\nThe χ2 contingency test can be done with a function we have seen before, chisq.test(). If we give a frequency table as input, this function will calculate the χ2 test for us.\nBefore we do the test, though, we need to make sure that the assumptions of the χ2 test are met by our data. Fortunately, the chisq.test() function also provides a way for us to look at the expected values. If we give a frequency table as input, and then add $expected at the end of the function, it will show us the expected values for a test of independence, like this:\n\nchisq.test(sex_survive_table)$expected\n\n        \n               no      yes\n  female 304.6702 158.3298\n  male   559.3298 290.6702\n\n\n\nIn this case all the expected values are greater than 5, so we have no problem meeting this assumption. Therefore, it is appropriate to do a χ2 contingency test. Just give a frequency table as input to the chisq.test() function to do this test. We’ve added the option correct = FALSE to tell R to not do a Yate’s correction, which can be overly conservative.\n\nchisq.test(sex_survive_table, correct=FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  sex_survive_table\nX-squared = 327.7, df = 1, p-value < 2.2e-16\n\n\nThis output shows that the χ2 value for this test is 325.5, with 1 degree of freedom and a P-value less than 0.00000000000000022. So we can reject the null hypothesis of no association between sex and survival on the Titanic."
  },
  {
    "objectID": "lab04-contingency.html#fishers-exact-test",
    "href": "lab04-contingency.html#fishers-exact-test",
    "title": "Lab 04 Contingency",
    "section": "8 Fisher’s exact test",
    "text": "8 Fisher’s exact test\nAnother, more exact, option for testing for the independence of two categorical variables is Fisher’s exact test. This is a test that is tedious to calculate by hand, but R can do it in a flash. This test makes no approximations and sets no minimum threshold for the sizes of the expected values.\nTo implement Fisher’s exact test in R, use fisher.test(). This function is easy to use; just give it a frequency table as input.\n\nfisher.test(sex_survive_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  sex_survive_table\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07759301 0.13384845\nsample estimates:\nodds ratio \n 0.1021212 \n\n\n\nHere there is little uncertainty about the presence of an association between sex and survival; the P-value is less than 0.00000000000000022, which is very, very small and much, much less than 0.05 (This strange value is the smallest number that can be recorded using the default numerical precision in R).\nAlso given in the output here is information about the estimate of the odds ratio and its 95% confidence interval.\nfisher.test() is able to calculate contingency tests even when there are more than two possible values for each variable. In such cases, though, it cannot calculate the odds ratios."
  },
  {
    "objectID": "lab04-contingency.html#r-commands-summary",
    "href": "lab04-contingency.html#r-commands-summary",
    "title": "Lab 04 Contingency",
    "section": "9 R commands summary",
    "text": "9 R commands summary\n\n\n\n\nSurvivors"
  },
  {
    "objectID": "lab04-contingency.html#activity",
    "href": "lab04-contingency.html#activity",
    "title": "Lab 04 Contingency",
    "section": "10 Activity",
    "text": "10 Activity\n\n10.1 Sampling and contingency tables\nThe χ2 contingency test uses a test statistic called χ2. When the null hypothesis is true the distribution of possible values of that χ2 is approximately a χ2distribution. We’ll use an online exercise to visualize how well the χ2 distribution fits the distribution of the test statistic when the null hypothesis is true.\nOpen a web browser and navigate to this page\nThis page is based on a hypothetical example in which we compare the proportion of people who get sick after either receiving a vaccination or not. When you first open this app, you will see that the true probability of getting sick has been set to 60%, regardless of whether the person is vaccinated or not. Under these conditions, the null hypothesis of a contingency analysis is true: whether or not a person is vaccinated has no effect on whether they get sick. The two variables are independent.\nNotice the graph at the bottom right. Before we do anything else, this graph simply shows the χ2 distribution with one degree of freedom, as would be appropriate to use for a 2 x 2 contingency analysis as will be performed here.\n\nSTEP 1 Click the “MAKE ONE SAMPLE” button once. This will cause the computer to take one sample of n = 120 individuals from the population. A frequency table for that sample is shown on the right side of the page, and under that the χ2 and P-value from a χ2 contingency test of those data is shown. Also, a vertical blue bar appears on the graph at that value of χ2, and the area under the curve to the left of this χ2 is shown in red. The area under the curve to the right is the P-value.\nVisually compare the P-value to the area under the curve in the graph. Did your sample cause the test to reject the null hypothesis? Or did that sample give the correct answer about the population (i.e., that there is no difference in the sickness rates between vaccinated and unvaccinated people)?\n\nSTEP 2 The button “SIMULATE DISTRIBUTION” will take repeated samples from the population and calculate a χ2 statistic for each one. In the yellow box on the right, it will keep a tally of how many samples led to hypothesis tests that rejected the null hypothesis of independence. (Rejecting a true null hypothesis is a Type I error.) The program will also make a histogram of the χ2 values from all the tests to allow comparison with the χ2 distribution.\nCheck that probability of getting sick is still set to be the same for vaccinated and unvaccinated people in the population parameters. If so, the null hypothesis is in fact true. With a true null hypothesis, the distribution of calculated χ2 values (in the histogram) ought to match the theoretical distribution (shown by the thin curve). Do they match? (Note: you won’t get a proportion that exactly matches what we expect, by chance, because we haven’t been able to take an infinite number of samples.)\nThe calculations in this app assume a significance level of α = 0.05. What fraction of samples should we expect to result in a Type I error?\nNotice that there is a “FASTER!” button on the left. If you press this it will speed up the intervals between new samples taken in the simulation.\nRun the simulation until there are 1000 or more replicates. What Type I error rate do you observe?\n\nSTEP 3 Finally, let’s investigate the distribution of results that you get when the null hypothesis is NOT true. Before we go any further, think about what you would expect to see in the distribution of test statistics when the null hypothesis is false. Should the distribution of test statistics stay the same? Or will it shift towards the right or left?\nLet’s have the app simulate a case in which the null hypothesis is false. For example, let’s imagine that the vaccine actually works to some extent, and the probability of vaccinated people getting sick is only 0.35. Change the small white box in the table at the top of the page to 0.35 under “Vaccinated”, but leave the value of 0.6 under the “Unvaccinated” column. Now vaccination status and illness are NOT independent—the probability of getting sick differs between vaccination groups.\nTo see what the distribution of test statistics is now under this case, let’s simulate samples from this new population. Click “SIMULATE DISTRIBUTION” again. What is the shape of the distribution of test statistics in the histogram? Does it still match the theoretical χ2 distribution (shown by the curve)? Remember, the theoretical distribution is what we would get assuming that the null hypothesis is true. When the null hypothesis is false, we expect on average larger values of χ2.\nNow that the null hypothesis is false, a hypothesis test gets the correct answer when it rejects H0. The probability that a test correctly rejects a false null hypothesis is called the power of a test. The yellow box on the right tracks the power of the χ2 test. This is not a value that we can easily calculate ahead of time in a real setting. Power should be increased by having larger sample sizes or by having larger true deviations from the null hypothesis in the population. You can explore these features of power by changing the sample size (with the slider at the top right) or by making the difference between the vaccinated and unvaccinated groups larger or smaller.\n\n\n\n10.2 (optional) Collecting some data\nWe’ll do an experiment on ourselves. The point of the experiment needs to remain obscure until after the data is collected, so as to not bias the data collection process.\nAfter this paragraph, is reprinted the last paragraph of Darwin’s Origin of Species. You can download a page with only this one paragraph to print out. (e.g., consider printing a paper version if you are reading this electronically and are able to do so)\nPlease read through this paragraph, and circle every letter “t”. Please proceed at a normal reading speed. If you ever realize that you missed a “t” in a previous word, do not retrace your steps to encircle the “t”. You are not expected to get every “t”, so don’t slow down your reading to get the letter “t”s.\n\nIt is interesting to contemplate an entangled bank, clothed with many plants of many kinds, with birds singing on the bushes, with various insects flitting about, and with worms crawling through the damp earth, and to reflect that these elaborately constructed forms, so different from each other, and dependent on each other in so complex a manner, have all been produced by laws acting around us. These laws, taken in the largest sense, being Growth with Reproduction; inheritance which is almost implied by reproduction; Variability from the indirect and direct action of the external conditions of life, and from use and disuse; a Ratio of Increase so high as to lead to a Struggle for Life, and as a consequence to Natural Selection, entailing Divergence of Character and the Extinction of less-improved forms. Thus, from the war of nature, from famine and death, the most exalted object which we are capable of conceiving, namely, the production of the higher animals, directly follows. There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved.\n\nQuestion number 4 below will return to this exercise. Please don’t read Question 4 until you have completed this procedure."
  },
  {
    "objectID": "lab04-contingency.html#exercises",
    "href": "lab04-contingency.html#exercises",
    "title": "Lab 04 Contingency",
    "section": "11 Exercises",
    "text": "11 Exercises\n\n11.1\nOn April 18, 1940, most of the participants at a church supper in Oswego County, NY, developed gastroenteritis (a.k.a. food poisoning). In what has become a classic of epidemiology, researchers interviewed most of the people at the supper to discover which dishes they ate (CDC-EIS 2003). Their answers are in the file oswego.csv. In this file, the variable ill records whether that person got sick (Y for yes, N for no), and there are also columns for whether each person ate the baked ham, spinach, mashed potatoes, etc., for a total of 14 foods and drinks.\n\nLoad the data set into R from the .csv file.\nUsing table() and chisq.test()$expected, calculate the expected values for a χ2 contingency test of the relationship between fruit salad and illness. Would it be legitimate to use a χ2 contingency analysis to test this association? Why or why not? What test would be best to use?\nYou want to know which food is most strongly associated with illness. For the sake of simplicity, let’s imagine that we have ruled out all the other foods except for spinach, baked ham, vanilla ice cream and chocolate ice cream. Use fisher.test() to calculate an odds ratio for the illness for each these foods. Which is the most likely vehicle of the disease?\nUsing the food you think is the likely vehicle, what is the 95% confidence interval of the odds ratio for illness?\nFor the food you decided in part c is the most likely vehicle, draw a mosaic plot to illustrate how many people got sick as a function of whether they ate this food.\n\n(Researchers later determined that the person who had prepared the specific food that was associated with the gastroenteritis had a Staphylococcus infection, including a lesion on her hand [omg gross!]. The food in question had been left to sit overnight at room temperature, which allowed the Staphylococcus to grow to dangerous numbers before the supper.)\n\n\n\n11.2\nHuman names are often of obscure origin, but many have fairly obvious sources. For example, “Johnson” means “son of John,” “Smith” refers to an occupation, and “Whitlock” means “white-haired” (from “white locks”). In Lancashire, U.K., a fair number of people are named “Shufflebottom,” a name whose origins remain obscure.\nBefore children learn to walk, they move around in a variety of ways, with most infants preferring a particular mode of transportation. Some crawl on hands and knees, some belly crawl commando-style, and some shuffle around on their bottoms.\nA group of researchers decided to ask whether the name “Shufflebottom” might be associated with a propensity to bottom-shuffle. To test this, the compared the frequency of bottom-shufflers among infants with the last name “Shufflebottom” to the frequency of infants named “Walker.” (See Fox et al. 2002.)\nThey found that 9 out of 41 Walkers moved by bottom-shuffling, while 9 out of 43 Shufflebottoms did. You can find these data in the file shufflebottoms.csv.\nWhat is the odds ratio for the association between name and mode of movement? Give a 95% confidence interval for this odds ratio.\nBased on the confidence interval for the odds ratio, would you expect that a hypothesis test would find a significant association between name and movement type? Why or why not?\nIs there a significant difference between the name groups for mode of movement?\n\n\n\n11.3\nFalls are extremely dangerous for the elderly; in fact many deaths are associated with such falls. Some preventative measures are possible, and it would be very useful to have ways to predict which people are at greatest risks for falls.\nOne doctor noticed that some patients stopped walking when they started to talk, and she thought that the reason might be that it is a challenge for these people to do more than one thing at once. She hypothesized that this might be a cue for future risks, such as for falling, and this led to a study of 58 elderly patients in a nursing home (Lundin-Olsson et al. 1997).\nOf these 58 people, 12 stopped walking when talking, while the rest did not. Of the people who stopped walking to talk, 10 had a fall in the next six months. Of the other 46 who did not stop walking to talk, 11 had a fall in that same time period. These data are available in stopping_falls.csv.\nDraw a mosaic plot of the relationship between stopping to talk and falls.\nCarry out an appropriate hypothesis test of the relationship between “stops walking while talking” and falls.\nWhat is the odds ratio of this relationship? Give a 95% confidence interval. Which group has the higher odds of falling?\n\n\n\n11.4\nReturn to the page from the activities section where you circled the letter “t” in the paragraph from Darwin’s Origin of Species. (If you haven’t already done this, please stop reading here now and go back to do that first.)\nThe point of this exercise is to collect data on whether our brains perceive words merely as a collection of letters or if sometimes our brains process words as entities. The logic of this test is that, if words are perceived as units, rather than as collections of letters, then this should be especially true for common words. Hence we will look at the errors made in scanning this paragraph, and ask whether we are more (or less) likely to miss finding a “t” when it is part of a common word.\nCompare your results to the answer key that marks all the instances of the letter “t”. Note that the answer key marks all “t”s in red, but it also puts boxes around some words. The boxes are drawn around all instances of the letter “t” occurring in common words. “Common” is defined here as among the top-twenty words in terms of frequency of use in English; of these six contain one or more “t”s: the, to, it, that, with, and at. In this passage there are 94 “t”s, and 29 are in common words.\nCount how many mistakes you made finding “t”s in common words and in less common words.\nUse the appropriate test to ask whether the commonness of a word affects your probability of noticing the “t”s in it. You can create the table needed for the test by a command like:\ntCountTable = data.frame(Common = c (25, 4), Uncommon = c(55, 10), row.names = c(\"Found\", \"Not found\"))\n\nReplace 25 and 4 with the number of t’s in common words that you found or didn’t find, respectively. Replace 55 and 10 with the numbers of t’s you found or didn’t find in uncommon words."
  },
  {
    "objectID": "lab04-contingency.html#harper-adams-data-science",
    "href": "lab04-contingency.html#harper-adams-data-science",
    "title": "Lab 04 Contingency",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab05-Gaussian.html#objectives",
    "href": "lab05-Gaussian.html#objectives",
    "title": "Lab 05 The Gaussian",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nVisualize properties of the Gaussian distribution\nUnderstand the Central Limit Theorem\nCalculate sampling properties of sample means\nDecide whether a data set likely comes from a Gaussian distribution\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab05-Gaussian.html#start-a-script",
    "href": "lab05-Gaussian.html#start-a-script",
    "title": "Lab 05 The Gaussian",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab05-Gaussian.html#the-gaussian",
    "href": "lab05-Gaussian.html#the-gaussian",
    "title": "Lab 05 The Gaussian",
    "section": "3 The Gaussian",
    "text": "3 The Gaussian\nThis lab mainly focuses on exploring the nature of the Gaussian distribution. We will also learn a couple of tools that help us decide whether a particular data set is likely to have come from population with an approximately Gaussian distribution.\nMany statistical tests assume that the variable being analyzed has a Gaussian distribution. Fortunately, some of these tests are robust to this assumption: that is, they work reasonably well, especially when sample size is large (unless the assumption is grossly violated). Therefore it is often sufficient to be able to assess whether the data come from a distribution whose shape is approximately Gaussian (e.g., are the data similar enough to a bell curve?).\nA good way to start is to simply visualize the frequency distribution of the variable in the data set by drawing a histogram. Let’s use the age of passengers on the Titanic for our example.\n\n# Load data\n# NB your file path may be different than mine\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\ntitanicData <- read.csv(\"data/titanic.csv\" )\n\n\nRemember we can use ggplot() to draw histograms.\n\n# hist of passengers by age\nggplot(titanicData, aes(x = age)) +   \n  geom_histogram(binwidth = 10)\n\nWarning: Removed 680 rows containing non-finite values (stat_bin).\n\n\n\n\n\n\n\n3.1 hist() to assess distribution\nIf we are just drawing a histogram for ourselves to explore and better understand the data, it is possibly easier to use a function from base R, hist(). Give hist() a vector of data as input, and it will print a histogram in the plots window.\n\nhist(titanicData$age)\n\n\n\n\n\nLooking at this histogram, we see that the frequency distribution of the variable does not exactly resenble Gaussian. It is slightly asymmetric and there seems to be a second mode near 0. On the other hand, like the Gaussian distribution, this frequency distribution has a large mode near the center of the distribution, frequencies mainly fall off to either side, and there are no obvious outliers. This might be close enough to Gaussian that most methods assuming Gaussian would work fine.\n\n\n\n3.2 QQ plot\nAnother graphical technique that can help us visualize whether a variable is approximately Gaussian is the quantile plot (or a QQ plot). The QQ plot shows the data on the vertical axis ranked in order from smallest to largest (“sample quantiles” in the figure below). On the horizontal axis, it shows the expected value of an individual with the same quantile if the distribution were Gaussian (“theoretical quantiles” in the same figure). The QQ plot should follow more or less along a straight line if the data come from a Gaussian distribution (with some tolerance for sampling variation).\nQQ plots can be made in R using a function called qqnorm(). Simply give the vector of data as input and it will draw a QQ plot for you. qqline() will draw a line through that Q-Q plot to make the linear relationship easier to see.\n\n# qq plot\nqqnorm(titanicData$age, \n        pch = 16, cex = .6)  # Mere vanity\nqqline(titanicData$age, \n        col = \"red\", lwd = 2, lty = 1) # Mere vanity\n\n\n\n\n\nThis is what the resulting graph looks like for the Titanic age data. The dots do not land along a perfectly straight line. In particular the graph curves at the upper and lower end. However, this distribution definitely would be close enough to Gaussian to use most standard methods, such as the t-test.\nIt can be difficult to interpret QQ plots without experience. One of the goals of this lab is to develop some visual experience about what these graphs look like when the data is truly Gaussian. To do that, we will take advantage of a function built into R to generate random numbers drawn from a Gaussian distribution. This function is called rnorm().\n\n\n\n3.3rnorm()\nThe function rnorm() will return a vector of numbers, all drawn randomly from a Gaussian distribution. It takes three arguments:\n\nn: how many random numbers to generate (the length of the output vector)\nmean: the mean of the Gaussian distribution to sample from\nsd: the standard deviation of the Gaussian distribution\n\nFor example, the following command will give a vector of 20 random numbers drawn from a Gaussian distribution with mean 13 and standard deviation 4:\n\nset.seed(42) # For replicability\nrnorm(n = 20, mean = 13, sd = 4)\n\n [1] 18.483834 10.741207 14.452514 15.531450 14.617073 12.575502 19.046088\n [8] 12.621364 21.073695 12.749144 18.219479 22.146582  7.444557 11.884845\n[15] 12.466715 15.543802 11.862988  2.374178  3.238132 18.280453\n\n\n\nLet’s look at a QQ plot generated from 1000 numbers randomly drawn from a Gaussian distribution:\n\n# this is what Gaussian should look like\n# we know because the sample IS from a Gaussian distribution\nset.seed(42)\nGaussian_vector <- rnorm(n = 1000, mean = 13, sd = 4)\n\nqqnorm(Gaussian_vector)\nqqline(Gaussian_vector, col = 'red')\n\n\n\n\n\nThese points fall mainly along a straight line, but there is some wobble around that line even though these points were in fact randomly sampled from a known Gaussian distribution. With a QQ plot, we are looking for an overall pattern that is approximately a straight line, but we do not expect a perfect line. In the exercises, we’ll simulate several samples from a Gaussian distribution to try to build intuition about the kinds of results you might get.\nWhen data are not Gaussian distributed, the dots in the quantile plot will not follow a straight line, even approximately. For example, here is a histogram and a QQ plot for the population size of various counties, from the data in countries.csv. These data are very skewed to the right, and do not follow a Gaussian distribution at all.\n\ncountries <- read.csv(\"data/countries.csv\", header=T)\n\nhist(countries$total_population_in_thousands_2015, breaks=20,\n     main = 'Super skewed')\n\n\n\nqqnorm(countries$total_population_in_thousands_2015,\n       main = 'Where I\\'m from, we\\'d say that ain\\'t normal (Gaussian)')\nqqline(countries$total_population_in_thousands_2015, col = 'red')"
  },
  {
    "objectID": "lab05-Gaussian.html#transformations",
    "href": "lab05-Gaussian.html#transformations",
    "title": "Lab 05 The Gaussian",
    "section": "4 Transformations",
    "text": "4 Transformations\nWhen data are not Gaussian distributed, we can try to use a simple mathematical transformation on each data point to create a list of numbers that still convey the information about the original question but that may be better matched to the assumptions of our statistical tests. We may see more about such transformations later, but for now let’s learn how to do one of the most common data transformations, the log-transformation.\nWith a transformation, we apply the same mathematical function to each value of a given numerical variable for individual in the data set. With a log-transformation, we take the logarithm of each individual value for a numerical variable.\nWe can only use the log-transformation if all values are greater than zero. Also, it will only improve the fit of the Gaussian distribution to the data in cases when the frequency distribution of the data is right-skewed.\nTo take the log transformation for a variable in R is very simple. We simply use the function log(), and apply it to the vector of the numerical variable in question. For example, to calculate the log of age for all passengers on the Titanic, we use the command:\n\n# just first 100\nlog(titanicData$age[1:100])\n\n  [1]  3.36729583  0.69314718  3.40119738  3.21887582 -0.08697501  3.85014760\n  [7]  4.14313473  3.66356165  4.06044301  4.26267988  3.85014760  2.94443898\n [13]          NA          NA          NA  3.91202301  3.17805383  3.58351894\n [19]  3.61091791  3.85014760  3.25809654  3.21887582  3.21887582  2.94443898\n [25]  3.33220451  3.80666249  3.66356165  3.40119738  4.06044301          NA\n [31]  3.80666249  3.09104245          NA  3.71357207  3.87120101          NA\n [37]  3.78418963  4.07753744  4.09434456  3.80666249          NA  3.97029191\n [43]  4.06044301  3.58351894  3.49650756          NA          NA  3.58351894\n [49]  3.58351894  2.63905733  2.39789527  3.89182030          NA  3.58351894\n [55]          NA  3.82864140  3.85014760  3.29583687  3.43398720          NA\n [61]          NA          NA          NA  3.29583687  3.25809654          NA\n [67]          NA  4.15888308  3.61091791  3.66356165  4.00733319          NA\n [73]  4.24849524  4.23410650  3.58351894  3.66356165  3.63758616          NA\n [79]  3.29583687  3.43398720  3.29583687          NA  3.43398720  2.83321334\n [85]          NA          NA  1.38629436  3.29583687  3.91202301  3.87120101\n [91]  3.89182030  3.87120101  3.66356165  3.13549422  3.97029191  3.58351894\n [97]          NA          NA  3.40119738  3.17805383\n\n\nThis will return a vector of values, each of which is the log of age of a passenger."
  },
  {
    "objectID": "lab05-Gaussian.html#r-commands-summary",
    "href": "lab05-Gaussian.html#r-commands-summary",
    "title": "Lab 05 The Gaussian",
    "section": "5 R commands summary",
    "text": "5 R commands summary\n\n\n\n\nLab 05 commands"
  },
  {
    "objectID": "lab05-Gaussian.html#challenge-questions",
    "href": "lab05-Gaussian.html#challenge-questions",
    "title": "Lab 05 The Gaussian",
    "section": "6 Challenge questions",
    "text": "6 Challenge questions\n\nMake a script in RStudio that collects all your R code required to answer the following questions. Include answers to the qualitative questions using comments.\n\n\n\n6.1\nWe will use R’s random number generator for the Gaussian distribution to build intuition for how to view and interpret histograms and QQ plots. Remember, the lists of values generated by rnorm() come from a population that truly have a Gaussian distribution.\n\nGenerate a list of 10 random numbers from a Gaussian distribution with mean 15 and standard deviation 3, using the following command:\n\n\nGaussian_vector <- rnorm(n = 10, mean = 15, sd = 3)\n\n\n\nUse hist() to plot a histogram of these numbers from part a.\nPlot a QQ plot from the numbers in part a.\nRepeat a - c several times (at least a dozen times). For each, look at the histograms and QQ plots. Think about the ways in which these look different from the expectation of a Gaussian distribution (but remember that each of these samples comes from a true Gaussian population).\n\n\n\n\n6.2\nRepeat the procedures of Question 1, except this time have R sample 250 individuals for each sample. (You can use the same command as in Question 1, but now set n = 250.) Do the graphs and QQ plots from these larger samples look more like the Gaussian expectations than the smaller sample you already did? Why do you think that this is?\n\n\n\n6.3\nIn 1898, Hermon Bumpus collected house sparrows that had been caught in a severe winter storm in Chicago. He made several measurements on these sparrows, and his data are in the file bumpus.csv.\nBumpus used these data to observe differences between the birds that survived and those that died from the storm. This became one of the first direct and quantitative observations of natural selection on morphological traits. Here, let’s use these data to practice looking for fit of the Gaussian distribution.\n\nUse ggplot() to plot the distribution of total length (this is the length of the bird from beak to tail). Does the data look as though it comes from distribution that is approximately Gaussian?\nUse qqnorm() to plot a QQ plot for total length. Does the data fall approximately along a straight line in the QQ plot? If so, what does this imply about the fit of these data to a Gaussian distribution?\nCalculate the mean of total length and a 95% confidence interval for this mean.\n\n\n\n\n6.4\nThe file mammals.csv contains information on the body mass of various mammal species.\nPlot the distribution of body mass, and describe its shape. Does this look like it has a Gaussian distribution?\nTransform the body mass data with a log-transformation. Plot the distribution of log body mass. Describe the new distribution, and examine it for adherance to the Gaussian."
  },
  {
    "objectID": "lab05-Gaussian.html#harper-adams-data-science",
    "href": "lab05-Gaussian.html#harper-adams-data-science",
    "title": "Lab 05 The Gaussian",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab06-2-groups.html#objectives",
    "href": "lab06-2-groups.html#objectives",
    "title": "Lab 06 Two-group tests",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nUse strip charts, multiple histograms, and violin plots to view a numerical variable by group\nUse the paired t-test to test differences between group means with paired data\nTest for a difference between the means of two groups using the 2-sample t-test\nCalculate the 95% confidence for a mean difference (paired data) and the difference between means of two groups (2 independent samples of data)\nCompare the variances of two groups using Levene’s test\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab06-2-groups.html#start-a-script",
    "href": "lab06-2-groups.html#start-a-script",
    "title": "Lab 06 Two-group tests",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab06-2-groups.html#setup",
    "href": "lab06-2-groups.html#setup",
    "title": "Lab 06 Two-group tests",
    "section": "3 Setup",
    "text": "3 Setup\nWe’ll use the Titanic data set again this lab for our examples, so load the data to R.\n\ntitanicData <- read.csv(\"data/titanic.csv\" )\n\n\nAlso, we will be using ggplot() to make some graphs, so make sure that the package ggplot2 has been loaded. We also will need functions from the package car, so let’s load that as well.\n\n# If the libraries are not found when you load them...\n# you probably need to install them, don't you?\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.1.1\n\n\nLoading required package: carData\n\n\n\n\n3.1 Strip charts andgeom_jitter()\nA strip chart is a graphical technique to show the values of a numerical variable for all individuals according to their groups in a reasonably concise graph. Each individual is represented by a dot. To prevent nearby data points from obscuring each other, typically a strip chart adds “jitter”. That is, a little random variation is added to nudge each point to one side or the other to make it individually more visible.\n\nIn R, one can make strip charts with ggplot() using the geom function geom_jitter(). In the command below, we specify x = survive to indicate the categorical (group) variable and y = age to specify the numerical variable. If we want more or less jitter, we could use a larger or smaller value than 0.05 in the option position_jitter(0.05).\n\n# grouped data points\nggplot(titanicData, aes(x = survive, y = age)) + \n    geom_jitter(position = position_jitter(0.1)) + \n    theme_minimal()\n\nWarning: Removed 680 rows containing missing values (geom_point).\n\n\n\n\n\n\nOne can see from this strip chart that there is a weak tendency for survivors to be younger on average than non-survivors. Younger people had a higher chance of surviving the Titanic disaster than older people.\n\n\n\n3.2 Multiple histograms\nA multiple histogram plot is used to visualize the frequency distribution of a numerical variable separately for each of two or more groups. It allows easy comparison of the location and spread of the variable in the different groups, and it helps to assess whether the assumptions of relevant statistical methods are met.\nPlotting age again as a function of survive, we can write:\n\n# stacked histograms - yes please\nggplot(titanicData, aes(x = age)) +   # Specify data  \n    geom_histogram() +                # Makes the histograpm\n    facet_wrap(~ survive, ncol = 1)   # Stacks the 2 graphs\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nWe specified the data set titanicData for the function to use. Note that here the numerical variable is entered as the variable on the x axis (x = age). No y variable is specified because that is simply the count of individuals that have that age. The categorical variable is specified in the facet_wrap(~ survive). The “facets” here are the separate plots, and facet_wrap() tells R which variable to use to separate the data across the plots.\nWhat can we learn from the comparison of these groups? Notice, for example, that the very youngest people (less than 10 years old, say) are much more likely to be in the group that survived.\n\n\n\n3.3 Violin plots\nAnother good way to visualize the relationship between a group variable and a numerical variable is a violin plot.\nA violin plot can be made in R using ggplot, using the geom function geom_violin().\n\n# Best colour in the kingdom of R *horn of Chreub sounds*\nEd.col <- \"goldenrod\"\n\nggplot(titanicData, aes(x = survive, y = age, fill = survive)) + \n        geom_violin() +\n        xlab(\"Survival\") + \n        ylab(\"Age\") +\n        theme_classic() +\n        scale_fill_manual(values = c(Ed.col, \"red\")) +\n        stat_summary(fun = mean,  geom = \"point\", color = \"black\") + \n        theme(legend.position = \"none\") + \n        theme(aspect.ratio = 1)\n\nWarning: Removed 680 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 680 rows containing non-finite values (stat_summary).\n\n\n\n\n\n\nThis version has some extra options thrown in. The code is kind of complicated looking, but part of the point is to show what is possible.\nWhat is important here is that it uses geom_violin() to specify that a violin plot is wanted and that it specifies the categorial variable (here survive) and the numerical variable (here age). This graph also added the mean as a dot with the addition of stat_summary(fun.y=mean, geom=“point”, color=“black”).\nThe violin plot is wider (horizontally) where the density of data is highest. This feature makes it easy to visually compare different part of the distribution of data between the two groups."
  },
  {
    "objectID": "lab06-2-groups.html#two-sample-t-test",
    "href": "lab06-2-groups.html#two-sample-t-test",
    "title": "Lab 06 Two-group tests",
    "section": "4 Two-sample t-test",
    "text": "4 Two-sample t-test\nThe two-sample t-test is used to compare the means of two groups. This test can be performed in R using the function t.test(). As we shall see, t.test() actually performs a wide array of related calculations.\nAs with all other functions in these labs, we will assume here that you have your data in Tidy Data format; that is, each row describes a different individual and columns correspond to variables. For a 2-sample t-test, two variables are used, one categorical and one numerical. So we assume that there is a column in the data frame indicating which group an individual belongs to, and another column that contains the measurements for the numerical variable of interest.\nWith this data format, we can most easily give input to the t.test() function using what R calls a “formula”. In a formula, the response variable is given first, followed by a tilde (~), followed by the explanatory variables (e.g. height ~ sex, read “height as a function of sex”“). With a t-test, the explanatory variable is the categorical variable defining the two groups and the response variable is the numerical variable.\nFor example, imagine that we want to test whether individuals which survived the Titanic disaster had the same average age as those passengers who did not survive. For this, the categorical variable is survive and the numerical variable is age. We would write the formula as age ~ survive.\nTo do a 2-sample t-test, t.test() also needs two other pieces of input. You need to specify which data frame contains the data, and you need to specify that you want to assume that the variances of the two groups are equal. (If you specify this assumption, you are telling R to do a classic 2-sample t-test rather than a Welch’s t-test. We’ll see how to do Welch’s next, which allows for unequal variances.)\nTo specify the data frame to use, we give a value for the argument data, such as data = titanicData. To tell R to assume that the variances are equal, we use the option “var.equal = TRUE”.\nHere is the complete command for this example and the output:\n\nt.test(formula = age ~ survive, \n       data = titanicData, \n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  age by survive\nt = 2.0173, df = 631, p-value = 0.04409\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n 0.06300333 4.68528476\nsample estimates:\n mean in group no mean in group yes \n         32.24811          29.87396 \n\n\n\nNotice that in the formula here (age ~ survive), we do not need to use the name of the data frame (as in titanicData$age). This is because we are using a different argument to specify which data frame to use with the data argument (data = titanicData).\nThe output from t.test() gives a lot of information, which is probably mainly self-explanatory. The title it tells us that it did a Two Sample t-test as we requested. The output gives us the test statistic t, the degrees of freedom for the test (df), and the P-value for the test of equal population means (which in this case is P = 0.044).\nUnder “95 percent confidence interval,” this output gives the 95% confidence interval for the difference between the population means of the two groups. Finally, it gives the sample means for each group in the last line."
  },
  {
    "objectID": "lab06-2-groups.html#welchs-t-test",
    "href": "lab06-2-groups.html#welchs-t-test",
    "title": "Lab 06 Two-group tests",
    "section": "5 Welch’s t-test",
    "text": "5 Welch’s t-test\nThe 2-sample t-test assumes that both populations have the same variance for the numerical variable. As we will see in the Activity section later, the 2-sample t-test can have very high Type I error rates when the populations in fact have unequal variances. Fortunately, Welch’s t-test does not make this assumption of equal variance. (Remember, both methods assume that the two samples are random samples from their respective populations and that the numerical variable has a normal distribution in both populations.)\nCalculating Welch’s t-test in R is straightforward using the function t.test(). The command is exactly like that for the 2-sample t-test that we did above, but with the option var.equal set to FALSE.\n\nt.test(formula = age ~ survive, \n       data = titanicData, \n       var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  age by survive\nt = 1.9947, df = 570.96, p-value = 0.04655\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n 0.03644633 4.71184176\nsample estimates:\n mean in group no mean in group yes \n         32.24811          29.87396 \n\n\n\nNotice that the output format (and results) is very similar to the standard 2-sample t-test above, except that the first line of the output tells us that R did a Welch’s t-test.\nWelch’s t-test (with var.equal = FALSE) is actually the default for t.test(). If this argument is omitted, R will perform a Welch’s test.\nWelch’s t-test is nearly as powerful as a 2-sample t-test, but it has a much more accurate Type I error rate for cases when the variances are truly unequal. Its only real disadvantage is that it is more difficult to calculate by hand, but with R it is a much better test to use in most circumstances."
  },
  {
    "objectID": "lab06-2-groups.html#paired-t-test",
    "href": "lab06-2-groups.html#paired-t-test",
    "title": "Lab 06 Two-group tests",
    "section": "6 Paired t-test",
    "text": "6 Paired t-test\nThe function t.test() can also perform paired t-tests. A paired t-test is used when each data point in one group is paired meaningfully with a data point in the other group.\nWe will use a dataset looking at a theory in the study of sexual selection that has a 2 sample paired design. The theory is that males that are attractive to females tend to have high testosterone levels (associated with brighter colours in birds, etc.). However testosterone is costly to males and may lower immunocompetence. Thus, only the evolutionarily “fittest” males can pay the cost. Females preference for these males would be associated with good genes that enable this costly trait (or possibly “sexy sons”). This study manipulated testosterone in 13 male red-winged blackbrids and measured immunocompetence before and after the treatment (Hasselquist et al. 1999).\n\n\n\n\nRed winged blackbird (strong nostalgia from growing up in Florida seeing these)\n\n\n\n\nLoad the data with read.csv().\n\nblackbird <- read.csv(\"data/blackbirdTestosterone.csv\")\n\n\nThe dataset contains the log antibody production of male blackbirds before (logBeforeImplant) and after (logAfterImplant) the birds received a testosterone implant. There is a before and after measure for each bird, so the data are paired with the individual bird.\nWe can perform the paired t-test on these data with t.test() by specifying the option paired = TRUE. We also need to give the names of the two columns that have the data from the two conditions.\n\nt.test(blackbird$logAfterImplant, \n       blackbird$logBeforeImplant, \n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  blackbird$logAfterImplant and blackbird$logBeforeImplant\nt = 1.2714, df = 12, p-value = 0.2277\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.04007695  0.15238464\nsample estimates:\nmean of the differences \n             0.05615385 \n\n\n\nAgain we get a lot of useful output, and most of it is self-explanatory. R reminds us that it has done a paired t-test, and then gives the test statistic, the degrees of freedom, and the P-value. It gives the 95% confidence interval for the mean of the difference between groups. (It will calculate the difference by subtracting the variable you listed second from the variable you listed first: here that is logAfterImplant – logBeforeImplant.) In this case, the P-value is 0.22, which is not small enough to cause us to reject the null hypothesis."
  },
  {
    "objectID": "lab06-2-groups.html#levenes-test",
    "href": "lab06-2-groups.html#levenes-test",
    "title": "Lab 06 Two-group tests",
    "section": "7 Levene’s test",
    "text": "7 Levene’s test\nThere exist several tests to compare the variances of two or more groups. One of the best methods is Levene’s test. It is difficult to calculate by hand, but it is very easy in R. Levene’s test is available the package {car} with the function leveneTest().\nAs with other packages you would need to install the package {car} first if you have not done so already, and then use the library() function to load it into R.\n\n# fancy way to check if a package is installed...\n# install it if it is not...\n# and load it if it is\nif(!require(\"car\")) install.packages(\"car\")\n\n# The \"hard\" way\ninstall.packages(\"car\") # Run if needed\n\nWarning: package 'car' is in use and will not be installed\n\nlibrary(car)\n\n\nTo use leveneTest(), you need three arguments for the input.\n\ndata: the data frame that contains the data\nformula: an R formula that specifies the response variable and the explanatory variables\ncenter: how to calculate the center of each group. For the standard Levene’s test, use center = mean\n\n\nLet’s try this with the Titanic data, with age as the numerical variable and survive as the group variable (a factor).\n\nleveneTest(data = titanicData, \n        age ~ factor(survive), \n        center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n       Df F value  Pr(>F)  \ngroup   1   3.906 0.04855 *\n      631                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nFor these data, the P-value of Levene’s test is P = 0.049. We would reject the null hypothesis that the group that survived and the group that died had the same population variances for the variable age."
  },
  {
    "objectID": "lab06-2-groups.html#r-commands-summary",
    "href": "lab06-2-groups.html#r-commands-summary",
    "title": "Lab 06 Two-group tests",
    "section": "8 R commands summary",
    "text": "8 R commands summary\n\n\n\n\n\nSurvivors"
  },
  {
    "objectID": "lab06-2-groups.html#activity",
    "href": "lab06-2-groups.html#activity",
    "title": "Lab 06 Two-group tests",
    "section": "9 Activity",
    "text": "9 Activity\nInvestigating robustness\nRemember that the two-sample t-test assumes:\n-the variable has a Gaussian distribution within the populations\n-the variance of these distributions is the same in the two populations\n-the two samples are random samples\nThe t-test is fairly robust to violations of its Gaussian assumption, but it can be very sensitive to its assumption about equal variances. “Robust” means that even when the assumption is not correct, the t-test often performs quite well (in other words, its actual Type I error rate is close to the stated significance level, α).\nOpen a browser and load the applet at http://shiney.zoology.ubc.ca/whitlock/RobustnessOfT/\nThis applet will simulate t-tests from random samples from two populations, and it lets you specify the true parameters of those populations.\nEqual means, equal standard deviation.\nLet’s begin by looking at a case when the null hypothesis is true, and when the assumptions of the test are met. The app begins with equal means (both populations set with the sliders at a mean equal to 10), but the default values of the variances are unequal. Use the slider for population 1 to give it a standard deviation equal to 2 (so that it matches population 2—confirm that population 2 has that SD value as well). When the mean and standard deviation parameters are equal between the populations, the two populations will have identical normal distributions and the top graph on the app will seem to only show one curve.\nWhen you change the setting for a parameter on the left, the app will automatically simulate 1000 random samples from each population. For each sample, it will calculate a 2-sample t-test for the null hypothesis that the populations have equal means. With the null hypothesis true and the assumptions met, the Type I error rate ought to be 5%. Is this what you observe? (By chance, you may see some subtle departure from this 5%, because we have only simulated 1000 tests rather than an infinite number.) In this case, both distributions are normal and the variances are equal, just as assumed by the t-test. Therefore we expect it to work well in this case.\nEqual means, unequal variance, balanced sample size.\nLet’s now look at the Type I error rate of the 2-sample t-test when the variance is different between the two populations. Leaving the sample sizes for each group to be the same (at n = 25), set the standard deviation to be 2 to 3 times higher in population 2 than in population 1. (For example, perhaps leave the standard deviation at 1.5 in population 1 and set the standard deviation to 4 in population 2.) What does this do to the Type I error rate? (You should find that the Type I error rate is slightly higher than 5%, but not by much.)\nEqual means, unequal variance, unbalanced sample size.\nThe real problems start when the variances are unequal AND the sample size is different between the two populations. The 2-sample t-test performs particularly badly when the sample size is smaller in the population with the higher variance. Let’s try the following parameters to see the problem. For population 1, set the standard deviation to 1.5 and the sample size to 25. For population 2, set the standard deviation to 4.5 and the sample size to 5. What happens to the Type I error rate in this case? Think about what a high Type I error rate means—we wrongly reject true null hypotheses far more often than the stated significance level α = 5%.\nEqual means, unequal variance, Welch’s t-test.\nFinally, let’s look at how well Welch’s t-test performs in this case. Leave the standard deviations and sample size as in the last paragraph, but click the button at the top left by “Welch’s”. This will tell the simulation to use Welch’s t-test instead. Does Welch’s t-test result in a Type I error rate that is much closer to 5%?\nDifferent means, power.\nThe applet will also allow you to investigate the power of the 2-sample and Welch’s t-tests when the null hypothesis is false. Power is measured by the rate of rejection of the null hypothesis. Compare the power of the tests when the means of population 1 and population 2 are close to one another in value (but still different) with scenarios in which the difference between population means is greater. Explore the effects on power of having a larger sample size instead of a small sample size. Compare power when the population standard deviations are small versus large. If you compare the results of the 2-sample t-test and Welch’s t-test, you should find that the power of Welch’s test is almost as great as that of the 2-sample test, even though the Type I error rate is much better for Welch’s."
  },
  {
    "objectID": "lab06-2-groups.html#exercises",
    "href": "lab06-2-groups.html#exercises",
    "title": "Lab 06 Two-group tests",
    "section": "10 Exercises",
    "text": "10 Exercises\n\n\n10.1\nIn 1898, Hermon Bumpus collected data on one of the first examples of natural selection directly observed in nature (We mentioned this dataset in a previous lab). Immediately following a bad winter storm, 136 English house sparrows were collected and brought indoors. Of these, 72 subsequently recovered, but 64 died. Bumpus made several measurements on all of the birds, and he was able to demonstrate strong natural selection on some of the traits as a result of this storm. Natural selection has occurred if traits are different between the survivors and non-survivors.\nBumpus published all of his data, and they are given in the file “bumpus.csv.” Let’s examine whether there was natural selection in body weight from this storm event, by comparing the weights of the birds that survived to those that died.\n\nPlot a graph with multiple histograms for body weight (called “weight_g” in the bumpus.csv data set), comparing the surviving and nonsurviving groups (given in the variable “survival”). Do the distributions look approximately normal? Do the variances look approximately equal?\nUse t.test() to do a Welch’s t-test to look for a difference in mean body weight between the surviving and dying birds.\nWhat is the 95% confidence interval for the difference in mean weight of surviving and dying birds?\nUse a Levene’s test to ask whether the surviving and dying birds had the same variance in weight.\n\n\n\n\n10.2\nLet’s return to the data about ecological footprint in the data set “countries.csv”. As you remember, the ecological footprint of a country is a measure of the average amount of resources consumed by a person in that country (as measured by the amount of land required to produce those resources and to handle the average person’s waste). For many countries in the data set, information is available on the ecological footprint in the year 2000 as well as in 2012. Use a paired t-test to ask whether the ecological footprint has changed over that time period.\n\nPlot a histogram showing the difference in ecological footprint between 2012 and 2000. (Note, you will probably need to create a new vector containing the differences, and use hist() to make the graph.)\nUse t.test() to do a paired t-test to determine whether the ecological footprint changed significantly between 2000 and 2012.\nInterpret your result. Is there evidence that the ecological footprint changed over that time? If so did it increase or decrease?\n\n\n\n\n10.3\nA common belief is that shaving causes hair to grow back faster or coarser than it was before. Is this true? Lynfield and McWilliams (1970) did a small experiment to test for an effect of shaving. Five men shaved one leg weekly for several months while leaving the other leg unshaved. (The data from the shaved leg has the word “test” in the variable name; the data from the unshaved leg is labeled with “control.”)\nAt the end of the experiment, the researchers measured the difference in leg hair width and the hair growth rate on each of the legs. These data are given in “leg shaving.csv”.\n\nPerform a suitable hypothesis test to determine whether shaving affects hair thickness.\nHow big is the difference? Find the 95% confidence interval for the difference between shaved and unshaved legs for hair thickness.\n\n\n\n\n10.4\nThe ratio of the lengths of your second and fourth fingers varies in humans (second digit and 4th digit, we will refer to the lengths as 2D and 4D). The 2D:4D ratio is influenced by the amount of testosterone that a human is exposed to as a fetus during the period of gestation improtant for finger growth and development. As such, we might predict there to be a difference between males and females in the 2D:4D ratio. Use these data to calculate the 2D:4D ratio (index finger length divided by ring finger length) on the right hands. The data come from Lolli et al. 2017 and are available in the data file “humanDigits.xlsx”. The dataset is in tidy format, including a data dictionary.\nNB the file is Excel format and can be loaded using the function read.xlsx() available in the package {openxlsx}.\n\nif(!require(\"openxlsx\")) install.packages(\"openxlsx\")\n\n# Example code to load the data\n# NB you will probably have to supply your own file path\ndigits <- read.xlsx(\"data/humanDigits.xlsx\")\n\n\nTo do this you may need to “slice out” specific rows of the data object, and it may be helpful to review methods for doing that here\n\nMake an appropriate graph to show the relationship between sex and 2D:4D ratio of the right hand.\nTest for a difference in the mean 2D:4D ratio between men and women.\nWhat is the magnitude of the difference? Compute the 95% confidence interval for the difference in means."
  },
  {
    "objectID": "lab06-2-groups.html#harper-adams-data-science",
    "href": "lab06-2-groups.html#harper-adams-data-science",
    "title": "Lab 06 Two-group tests",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab07-ANOVA.html#objectives",
    "href": "lab07-ANOVA.html#objectives",
    "title": "Lab 07 ANOVA",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nUse R to perform analysis of variance (ANOVA) to compare the means of multiple groups\nPerform Tukey-Kramer tests to look at unplanned contrasts between all pairs of groups\nUse Kruskal-Wallis tests to test for difference between groups without assumptions of Gaussianity\nCompare the variances of two groups using Levene’s test\n\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab07-ANOVA.html#start-a-script",
    "href": "lab07-ANOVA.html#start-a-script",
    "title": "Lab 07 ANOVA",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab07-ANOVA.html#setup",
    "href": "lab07-ANOVA.html#setup",
    "title": "Lab 07 ANOVA",
    "section": "3 Setup",
    "text": "3 Setup\nFor the examples in this lab, we will be using the Titanic data set. We’ll group passengers by the passenger class they travelled under (a categorical variable) and ask whether different passenger classes differed in their mean age (a numerical variable).\nFirst, load the data.\n\n# Load data\n# NB your file path may be different than mine\ntitanicData <- read.csv(\"data/titanic.csv\" )\n\n\nLet’s first look at the data to get a sense of how well it fits the assumptions of ANOVA. Multiple stacked histograms are useful for this purpose. We can use ggplot() and facets to make this plot (as we saw in a previous lab):\n\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nggplot(titanicData, aes(x = age)) +   \n  geom_histogram(bins = 25) + \n  facet_wrap(~ passenger_class, ncol = 1) \n\n\n\n\n\nThese data look sufficiently Gaussian and with similar spreads that ANOVA would be appropriate.\nTo confirm these visual impressions, it would be useful to construct a table of the means and standard deviations of each group. There are numerous ways to do this in R, but one of the neatest is to use functions from the package dplyr. If you have not done so yet, install the dplyr package from the “Packages” tab in RStudio. Then load the dplyr package with library()."
  },
  {
    "objectID": "lab07-ANOVA.html#dplyr",
    "href": "lab07-ANOVA.html#dplyr",
    "title": "Lab 07 ANOVA",
    "section": "4 dplyr()",
    "text": "4 dplyr()\n\nif(!require(\"dplyr\")) install.packages(\"dplyr\")\n\n\n\n4.1 group_by()\nThe package {dplyr} has several useful features for manipulating data sets. For our current purposes, we will find two functions particularly useful: group_by() and summarise(). These two functions are well named and work together well, first to organize the data by groups, and second to summarize the results for each group. These functions are alternatives to the base R function aggregate().\nFirst, use group_by() to organize your data frame by the appropriate grouping variable. For example, here we want to organize the titanicData by passenger_class:\n\ntitanic_by_passenger_class <- group_by(titanicData,passenger_class)\n\n\n\n\n4.2 summarise()\nAfter applying group_by() to a data frame, we can summarize the data using summarise(). With summarise(), we can apply any type of function that summarizes data (e.g. mean(), median(), var(), etc.), and receive that summary group by group. For example, to calculate the mean age of each passenger_class, we can use:\n\nsummarise(titanic_by_passenger_class, \n          group_mean = mean(age, na.rm = TRUE))\n\n# A tibble: 3 x 2\n  passenger_class group_mean\n  <chr>                <dbl>\n1 1st                   39.7\n2 2nd                   28.3\n3 3rd                   24.5\n\n\n\nAs input, we give the name of the grouped table created by group_by() and the function we want to apply to each group. In this case we used mean(age, na.rm=TRUE). “group_mean” is a name we give to that summary variable (it could have been any name we wanted). The output looks like a table and includes the names of the groups being summarized. (A “tibble” is not how New Zealanders spell “table”, but is a type of table like a data frame.) “3 x 2” here refers to the number of rows x columns in the “tibble” output.\nWe can give summarise() many summary functions at once, and it will create columns in the output table for each one. For example, if we want to output both the mean and the standard deviation, we can add group_sd = sd(age, na.rm=TRUE) to the function above.\n\nsummarise(titanic_by_passenger_class, \n          group_mean = mean(age, na.rm = TRUE), \n          group_sd = sd(age, na.rm = TRUE))\n\n# A tibble: 3 x 3\n  passenger_class group_mean group_sd\n  <chr>                <dbl>    <dbl>\n1 1st                   39.7     14.9\n2 2nd                   28.3     13.0\n3 3rd                   24.5     11.3\n\n\n\nNote that the standard deviations are very similar, which means that these data fit the equal variance assumption of ANOVA."
  },
  {
    "objectID": "lab07-ANOVA.html#anova",
    "href": "lab07-ANOVA.html#anova",
    "title": "Lab 07 ANOVA",
    "section": "5 ANOVA",
    "text": "5 ANOVA\nAnalysis of variance (or ANOVA) is not quite as simple in R as one might hope. Doing ANOVA takes at least two steps. First, we fit the ANOVA model to the data using the function lm(). This step carries out a bunch of intermediate calculations. Second, we use the results of first step to do the ANOVA calculations and place them in an ANOVA table using the function anova(). The function name lm() stands for “linear model”; this is actually a very powerful function that allows a variety of calculations. One-way ANOVA is a type of linear model.\n\n\n5.1 lm()\nThe function lm() needs an R formula and a data frame object as arguments. The formula is a statement specifying the “model” that we are asking R to fit to the data. A model formula always takes the form of a response variable, followed by a tilde (~), and then at least one explanatory variable. In the case of a one-way ANOVA, this model statement will take the form\n\nnumerical_variable ~ categorical_variable\n\nFor example, to compare differences in mean age among passenger classes on the Titanic, this formula is\n\nage ~ passenger_class\n\nThis formula tells R to “fit” a model in which the ages of passengers are grouped by the variable passenger_class.\n\nThe name of the data frame containing the variables stated in the formula is the second argument of lm().\ndata = your_actual_data_obeJct_name_may_vary  (TM)\n\nFinally, to complete the lm() command, it is useful to save the intermediate results by assigning them to a new object, which anova() can then use to make the ANOVA table. For example, here we assign the results of lm() to a new object named “titanicANOVA”:\n\ntitanicANOVA <- lm(age ~ passenger_class, data = titanicData)\n\n\n\n\n5.2 anova()\nThe function anova() takes the results of lm() as input and returns an ANOVA table as output:\n\nanova(titanicANOVA)\n\nAnalysis of Variance Table\n\nResponse: age\n                 Df Sum Sq Mean Sq F value    Pr(>F)    \npassenger_class   2  26690 13344.8  75.903 < 2.2e-16 ***\nResiduals       630 110764   175.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThis table shows the results of a test of the null hypothesis that the mean ages are the same among the three groups. The P-value is very small, and so we reject the null hypothesis of no differences in mean age among the passenger classes.\n\n\n\n5.3 Tukey’s HSD (Honestly Significant Difference)\nA single-factor ANOVA can tell us that at least one group has a different mean from another group, but it does not inform us which group means are different from which other group means. A Tukey-Kramer test lets us test the null hypothesis of no difference between the population means for all pairs of groups. The Tukey-Kramer test (also known as a Tukey Honestlt Significance Test, or Tukey’s HSD), is implemented in R in the function TukeyHSD().\nWe will use the results of an ANOVA done with lm() as above, that we stored in the variable titanicANOVA. To do a Tukey-Kramer test on these data, we need to first apply the function aov() to titanicANOVA, and then we need to apply the function TukeyHSD() to the result. We can do this in a single command:\n\nTukeyHSD(aov(titanicANOVA))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = titanicANOVA)\n\n$passenger_class\n              diff        lwr        upr     p adj\n2nd-1st -11.367459 -14.345803  -8.389115 0.0000000\n3rd-1st -15.148115 -18.192710 -12.103521 0.0000000\n3rd-2nd  -3.780656  -6.871463  -0.689849 0.0116695\n\n\n\nThe key part of this output is the table. It shows:\n\nThe difference between the means of groups (for example, the 2nd passenger class compared to the 1st passenger class)\nThe 95% confidence interval for the difference between the corresponding population means. (“lwr” and “upr” correspond to the lower and upper bounds of that confidence interval for the difference in means.)\nThe P-value from a test of the null hypothesis of no difference between the means (the column headed with “p adj”). In the case of the Titanic data, P is less than 0.05 in all pairs, and we therefore reject every null hypothesis. We conclude that the population mean ages of all passenger classes are significantly different from each other."
  },
  {
    "objectID": "lab07-ANOVA.html#kruskal-wallis-test",
    "href": "lab07-ANOVA.html#kruskal-wallis-test",
    "title": "Lab 07 ANOVA",
    "section": "6 Kruskal-Wallis test",
    "text": "6 Kruskal-Wallis test\nA Kruskal-Wallis test is a non-parametric analog of a one-way ANOVA. It does not assume that the numeric variable has a Gaussian distribution for each group.\nTo run a Kruskal-Wallis test, use the R function kruskal.test(). The input for this function is the same as we used for lm() above. It includes a model formula statement and the name of the data frame to be used.\n\nkruskal.test(formula = age ~ passenger_class, \n          data = titanicData)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  age by passenger_class\nKruskal-Wallis chi-squared = 116.08, df = 2, p-value < 2.2e-16\n\n\nYou can see for the output that a Kruskal-Wallis test also strongly rejects the null hypothesis of equality of age for all passenger class groups with the Titanic data."
  },
  {
    "objectID": "lab07-ANOVA.html#r-commands-summary",
    "href": "lab07-ANOVA.html#r-commands-summary",
    "title": "Lab 07 ANOVA",
    "section": "7 R commands summary",
    "text": "7 R commands summary\n\n\n\n\n\nSurvivors"
  },
  {
    "objectID": "lab07-ANOVA.html#exercises",
    "href": "lab07-ANOVA.html#exercises",
    "title": "Lab 07 ANOVA",
    "section": "8 Exercises",
    "text": "8 Exercises\n\n8.1\nThe European cuckoo does not look after its own eggs, but instead lays them in the nests of birds of other species. Previous studies showed that cuckoos sometimes have evolved to lay eggs that are colored similarly to the host bird species. Is the same true of egg size? Do cuckoos lay eggs similar in size to the size of the eggs of their hosts? The data file cuckooeggs.csv contains data on the lengths of cuckoo eggs laid in the nests of a variety of host species. Here we compare the mean size of cuckoo eggs found in the nests of different host species.\n\nPlot a multiple histogram showing cuckoo egg lengths by host species.\nCalculate a table that shows the mean and standard deviation of length of cuckoo eggs for each host species.\nLook at the graph and the table. For these data, would ANOVA be a valid method to test for differences between host species in the lengths of cuckoo eggs in their nests?\nUse ANOVA to test for a difference between host species in the mean size of the cuckoo eggs in their nests. What is your conclusion?\nAssuming that ANOVA rejected the null hypotheses of no mean differences, use a Tukey-Kramer test to decide which pairs of host species are significantly different from each other in cuckoo egg mean length. What is your conclusion?\n\n\n\n\n8.2\nThe pollen of the maize (corn) plant is a source of food to larval mosquitoes of the species Anopheles arabiensis, the main vector of malaria in Ethiopia. The production of maize has increased substantially in certain areas of Ethiopia recently, and over the same time period, malaria has entered in to new areas where it was previously rare. This raises the question, is the increase of maize cultivation partly responsible for the increase in malaria?\nOne line of evidence is to look for an association between maize production and malaria incidence at different geographically dispersed sites (Kebede et al. 2005). The data set “malaria vs maize.csv” contains information on several high-altitude sites in Ethiopia, with information about the level of cultivation of maize (low, medium or high in the variable maize_yield) and the rate of malaria per 10,000 people (incidence_rate_per_ten_thousand).\n\nPlot a multiple histogram to show the relationship between level of maize production and the incidence of malaria.\nANOVA is a logical choice of method to test differences in the mean rate of malaria between sites differing in level of maize production. Calculate the standard deviation of the incidence rate for each level of maize yield. Do these data seem to conform to the assumptions of ANOVA? Describe any violations of assumptions you identify.\nCompute the log of the incidence rate and redraw the multiple histograms for different levels of maize yield. Calculate the standard deviation of the log incidence rate for each level of maize yield. Does the log-transformed data better meet the assumptions of ANOVA than did the untransformed data?\nTest for an association between maize yield and malaria incidence.\n\n\n\n\n8.3\nAnimals that are infected with a pathogen often have disturbed circadian rhythms. (A circadian rhythm is an endogenous daily cycle in a behavior or physiological trait that persists in the absence of time cues.) Shirasu-Hiza et al. (2007) wanted to know whether it was possible that the circadian timing mechanism itself could have an effect on disease. To test this idea they sampled from three groups of fruit flies: one “normal”, one with a mutation in the timing gene tim01, and one group that had the tim01 mutant in a heterozygous state. They exposed these flies to a dangerous bacteria, Streptococcus pneumoniae, and measured how long the flies lived afterwards, in days. The date file “circadian mutant health.csv” shows some of their data.\n\nPlot a histogram of each of the three groups. Do these data match the assumptions of an ANOVA?\nUse a Kruskal-Wallis test to ask whether lifespan differs between the three groups of flies."
  },
  {
    "objectID": "lab07-ANOVA.html#harper-adams-data-science",
    "href": "lab07-ANOVA.html#harper-adams-data-science",
    "title": "Lab 07 ANOVA",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab08-cor-reg.html#objectives",
    "href": "lab08-cor-reg.html#objectives",
    "title": "Lab 08 Correlation and regression",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nCalculate a correlation coefficient and the coefficient of determination\nTest hypotheses about correlation\nUse the non-parametric Spearman’s correlation\nEstimate slopes of regressions\nTest regression models\nPlot regression lines\nExamine residual plots for deviations from the assumptions of linear regression \n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab08-cor-reg.html#start-a-script",
    "href": "lab08-cor-reg.html#start-a-script",
    "title": "Lab 08 Correlation and regression",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab08-cor-reg.html#association",
    "href": "lab08-cor-reg.html#association",
    "title": "Lab 08 Correlation and regression",
    "section": "3 Association",
    "text": "3 Association\n\nThis lab will demonstrate methods to evaluate the relationship between two numerical variables, using correlation and regression.\nWe will use a data set collected to investigate the relationship between how ornamented a father guppy is (fatherOrnamentation) and how attractive to females are his sons (sonAttractiveness). Load the data from the guppy-attractiveness.csv file:\n\n# Example code to load the data\n# NB you will probably have to supply your own file path\nguppyData <- read.csv(\"data/guppy-attractiveness.csv\")\n\nBefore you go further, it would probably be wise to plot a scatterplot to view the relationship of the two variables.\n\n\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\nggplot(guppyData, aes(x = fatherOrnamentation,                       y = sonAttractiveness)) +\n   geom_point() +\n   theme_minimal() +\n   xlab(\"Father's ornamentation\") +\n   ylab(\"Son's attractiveness\")\n\n\n\n\n\nNote that the data seem to have a moderately strong, positive relationship."
  },
  {
    "objectID": "lab08-cor-reg.html#correlation",
    "href": "lab08-cor-reg.html#correlation",
    "title": "Lab 08 Correlation and regression",
    "section": "4 Correlation",
    "text": "4 Correlation\n\n\n4.1 cor() function\nCalculating a correlation coefficient in R is straightforward. The function cor() calculates the correlation between the two variables given as input:\n\ncor(guppyData$fatherOrnamentation, guppyData$sonAttractiveness)\n\n[1] 0.6141043\n\n\nAs we predicted from the graph, the correlation coefficient of these data is positive and fairly strong.\nR has no explicit function for calculating the coefficient of determination. However, the coefficient of determination is simply the square of the correlation coefficient, so we can calculate it by simply squaring the output of cor(). Remember from Lab 1 that the caret sign (^) is used to denote exponents, as in the following example:\n\ncor(guppyData$fatherOrnamentation, guppyData$sonAttractiveness)^2\n\n[1] 0.377124\n\n\n\n\n\n4.2 cor.test()\nTo test a hypothesis about the correlation coefficient or to calculate its confidence interval, use cor.test(). It takes as input the names of vectors containing the variables of interest.\n\ncor.test(x = guppyData$fatherOrnamentation, \n         y = guppyData$sonAttractiveness)\n\n\n    Pearson's product-moment correlation\n\ndata:  guppyData$fatherOrnamentation and guppyData$sonAttractiveness\nt = 4.5371, df = 34, p-value = 6.784e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3577455 0.7843860\nsample estimates:\n      cor \n0.6141043 \n\n\n\nThe output gives many bits of information we might want. The title here, “Pearson’s product–moment correlation” is the technical name for the classic correlation coefficient. After, re-stating the names of the variables being used, the output gives us the test statistic t, degrees of freedom, and P-value of a test of the null hypothesis that the population correlation coefficient is zero. In this case the P-value is quite small, P = 6.8 x 10-5. After that we have the 95% confidence interval for the correlation coefficient and finally the estimate of the correlation coefficient itself.\n\n\n\n4.3 Spearman’s correlation\nThe function cor.test() can also calculate a Spearman’s rank correlation, if we add the option method = “spearman” to the command.\n\ncor.test(x = guppyData$fatherOrnamentation, \n        y = guppyData$sonAttractiveness, \n        method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  guppyData$fatherOrnamentation and guppyData$sonAttractiveness\nS = 3269.4, p-value = 0.0002144\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.5792287 \n\n\n\nThe output here is similar to what we described above with a Pearson’s correlation. Spearman’s ρ (rho) in this case is about 0.579."
  },
  {
    "objectID": "lab08-cor-reg.html#simple-linear-regression",
    "href": "lab08-cor-reg.html#simple-linear-regression",
    "title": "Lab 08 Correlation and regression",
    "section": "5 Simple linear regression",
    "text": "5 Simple linear regression\nRegression in R is a two-step process similar to the steps used in ANOVA last week. In fact, we again start by using lm() to fit a linear model to the data. (Both ANOVA and regression are special cases of linear models, which also can be used to generate much more complicated analyses than these.) We then give the output of lm() to the function summary() to see many useful results of the analysis.\nUsing the lm() function to calculate regression is similar to the steps used for ANOVA. The first argument is:\n-a formula, in the form response_variable ~ explanatory_variable. In this case we want to predict son’s attractiveness from father’s ornamentation, so our formula will be sonAttractiveness ~ fatherOrnamentation.\n-The second input argument is the name of the data frame with the data. We will want to assign the results to a new object with a name (we chose “guppyRegression”), so that we can use the results in later calculations with summary().\n\nguppyRegression <- lm(sonAttractiveness ~ fatherOrnamentation, \n                      data = guppyData)\n\n\nLet’s look at the output of lm() in this case.\n\nguppyRegression \n\n\nCall:\nlm(formula = sonAttractiveness ~ fatherOrnamentation, data = guppyData)\n\nCoefficients:\n        (Intercept)  fatherOrnamentation  \n           0.005084             0.982285  \n\n\n\nThis tells us that the estimate of the slope of the regression line is 0.982285, and the y-intercept is estimated to be 0.005084. Therefore the line that is estimated to be the best fit to these data is:\n\\(sonAttractiveness = 0.005084 + (0.982285\\ \\times\\ fatherOrnamentation)\\)\n\nWe can find other useful information by looking at the summary() of the lm() result:\n\nsummary(guppyRegression)\n\n\nCall:\nlm(formula = sonAttractiveness ~ fatherOrnamentation, data = guppyData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.66888 -0.14647 -0.02119  0.27727  0.51324 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.005084   0.118988   0.043    0.966    \nfatherOrnamentation 0.982285   0.216499   4.537 6.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3212 on 34 degrees of freedom\nMultiple R-squared:  0.3771,    Adjusted R-squared:  0.3588 \nF-statistic: 20.59 on 1 and 34 DF,  p-value: 6.784e-05\n\n\n\nWe see the estimates of the slope and intercept repeated here, in the “Coefficients” table under “Estimate”. Now, we also are given the standard error and P-value for each of these numbers in that same table. For these data, the P-value for the null hypothesis that the true slope is zero is 6.78 x 10–5.\nPlotting this line on the scatterplot is fairly straightforward in ggplot(). We can use the same plot function as above, with a new layer added with + geom_smooth(method=lm) in the last line below:\n\nggplot(guppyData, aes(x = fatherOrnamentation, \n   y= sonAttractiveness)) +\n   geom_point() +\n   theme_minimal() +\n   xlab(\"Father's ornamentation\") +\n   ylab(\"Son's attractiveness\") +    \n   geom_smooth(method = lm)\n\n\n\n\n\nThis layer adds both the best-fitting regression line and also the 95% confidence interval for the line shown in grey shading. The outer edges of the shaded area represent the confidence bands, indicating the 95% confidence intervals for the mean of the Y-variable (son’s attractiveness) at each value of the X-variable (father’s ornamentation). If you want a plot without this confidence interval, add the argument se = FALSE to the geom_smooth() function, as in geom_smooth(method=lm, se = FALSE).\n\n\n5.1 Residual plots\nTo check that the assumptions of regression apply for your data set, it is can be really helpful to look at a residual plot. A residual is the difference between the actual value of the y variable and the predicted value based on the regression line.\n\n\n\n5.2 residuals()\nR can calculate the residuals from a model with the residuals() function. Simply give this function the results from the lm() function, such as the guppyRegression that we calculated above. A vector of all the residuals for this regression line would be calculated by\n\nresiduals(guppyRegression)\nWith a residual plot, we plot the residuals of each data point as a function of the explanatory variable.\n\nplot(residuals(guppyRegression) ~ fatherOrnamentation, \n      data = guppyData)\n\nabline(h=0)\n\n\n\n\n\nIn this case we used the built-in function plot() because it is fast and easy, and this residual plot may only be for ourselves. The second command abline(h=0) adds the horizontal line at 0 to the plot so that it is easier to see the baseline.\nThis residual plot shows no major deviation from the assumptions of linear regression. There is no strong tendency for the variance of the residuals (indicated by the amount of scatter in the vertical dimension) to increase or decrease with increasing x. The residuals show no outliers or other evidence of not being normally distributed for each value of x."
  },
  {
    "objectID": "lab08-cor-reg.html#r-commands-summary",
    "href": "lab08-cor-reg.html#r-commands-summary",
    "title": "Lab 08 Correlation and regression",
    "section": "6 R commands summary",
    "text": "6 R commands summary\n\n\n\n\n\nSurvivors"
  },
  {
    "objectID": "lab08-cor-reg.html#activities",
    "href": "lab08-cor-reg.html#activities",
    "title": "Lab 08 Correlation and regression",
    "section": "7 Activities",
    "text": "7 Activities\n\n1. Developing an intuition for correlation coefficients.\nIn a web browser, open the app at https://shiney.zoology.ubc.ca/whitlock/Guessing_correlation/\nThis app is simple—it will plot some data in a scatterplot, and you guess the correct correlation coefficient for those data. Select one of the three choices and click the little circle next to your choice. Most people find this pretty challenging at first, but that is the point—to let you develop a better intuition about what a given value of a correlation coefficient means for how strong a relationship is between two numerical variables.\nKeep trying new data sets (by clicking the “Simulate new data” button) until you feel like you can get it right most of the time.\n\n2. Visualizing residuals.\nOpen another app at http://shiney.zoology.ubc.ca/whitlock/Residuals/ This app will let you visualize the meaning of the term “residual” and help to understand what a residual plot is.\nStart with the first tab (“1. Residuals”), and work through the tabs in order."
  },
  {
    "objectID": "lab08-cor-reg.html#exercises",
    "href": "lab08-cor-reg.html#exercises",
    "title": "Lab 08 Correlation and regression",
    "section": "8 Exercises",
    "text": "8 Exercises\n\n\n8.1\nThe ends of chromosomes are called telomeres. These telomeres are shortened a bit during each cell cycle as DNA is replicated. One of their purposes is to protect more valuable DNA in the chromosome from degradation during replication. As people get older and their cells have replicated more often, their telomeres shorten. There is evidence that these shortened telomeres may play a role in aging. Telomeres can be lengthened in germ cells and stem cells by an enzyme called telomerase, but this enzyme is not active in most healthy somatic cells. (Cancer cells, on the other hand, usually express telomerase.)\n\nGiven that the length of telomeres is biologically important, it becomes interesting to know whether telomere length varies between individuals and whether this variation is inherited. A set of data was collected by Nordfjäll et al. (2005) on the telomere length of fathers and their children; these data are in the file telomere inheritance.csv.\n\n\nCreate a scatter plot showing the relationship between father and offspring telomere length.\nDo the data require any transformation before analysis using linear regression?\nEstimate an equation that predicts the offspring telomere length from its father’s. Is there evidence that the father’s telomere length predicts his offspring’s value?\n\n\n\n\n8.2\nOpfer and Segler (2007) asked second- and fourth-grade school children to mark on a number line where a given number would fall. Each child was given a drawing of a number line with two ends marked at 0 and 1000, and was then asked to make an X on that line where a number, for example 150, should be placed. They asked each child to place several different numbers on the number lines, each on a fresh new piece of paper.\n\n\n\n\nSurvivors\n\n\n\n\nThe researchers then measured the placement of each mark on a linear scale. The results, averaged over all 93 kids for each group, are given in the file numberline.csv.\n\n\nPlot the fourth graders’ guesses against the true value. Is this relationship linear? If not, find a transformation of X or Y that converts the relationship into an approximately linear one.\nPlot the second-graders’ guesses against the true value. Is this relationship linear? If not, find a transformation of X or Y that converts the relationship into an approximately linear one. Fit a linear regression to both the transformed and untransformed data. Examine the residual plots for both the transformed and untransformed data.\nAssume that the difference between the shapes of these curves is real. What would you conclude about the difference in the way 2nd graders and 4th graders perceive numbers?\n\n\n\n\n8.3\nLarger animals tend to have larger brains. But is the increase in brain size proportional to the increase in body size? A set of data on body and brain size of 62 mammal species was collated by Allison and Cicchetti (1976), and these data are in the data set mammals.csv. The file contains columns giving the species name, the average body mass (in kg) and average brain size (in g) for each species. The study of how relatively large body parts (like the brain) in relation to body size (like body weight) is sometimes called allometry. It is traditional to model the body part as the dependent variable on the y axis, and use body size as the predictor variable on the x axis.\n\n\nPlot brain size against body size. Is the relationship linear?\nFind a transformation (for either or both variables) that makes the relationship between these two variables linear.\nIs there statistical evidence that brain size is correlated with body size? Assume that the species data are independent.\nWhat line best predicts (transformed) brain size from (transformed) body size?\nBased on your answer in (d), what is the predicted change in log-brain size accompanying an increase of 3 units of log- body size?\nMake a residual plot using the regression fitted to the transformed variables. Do the data look like they match the assumptions of linear regression?\nWhich species has the highest brain size relative to that predicted by its body size? Which species has the smallest brain relative to that predicted by its body size? [hint: You could slice out large residuals using base R, or try using filter() from the dplyr package…]"
  },
  {
    "objectID": "lab08-cor-reg.html#harper-adams-data-science",
    "href": "lab08-cor-reg.html#harper-adams-data-science",
    "title": "Lab 08 Correlation and regression",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab09-graph2.html#objectives",
    "href": "lab09-graph2.html#objectives",
    "title": "Lab 09 Graphing II",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nEvaluate data using graphical exploration\nGraph and evaluate frequency distributions\nCompare frequency distributions\nPractice good practice in graph design and data exploration\nPractice {ggplot2} tools and syntax\n\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab09-graph2.html#start-a-script",
    "href": "lab09-graph2.html#start-a-script",
    "title": "Lab 09 Graphing II",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab09-graph2.html#displaying-data-using-graphs-and-tables",
    "href": "lab09-graph2.html#displaying-data-using-graphs-and-tables",
    "title": "Lab 09 Graphing II",
    "section": "3 Displaying data using graphs and tables",
    "text": "3 Displaying data using graphs and tables\nThe purpose of this lab is to tour the table and graphics capabilities of R, and to explore the best methods for displaying patterns in data. We will do this by exploring some real datasets."
  },
  {
    "objectID": "lab09-graph2.html#data-mammal-body-mass",
    "href": "lab09-graph2.html#data-mammal-body-mass",
    "title": "Lab 09 Graphing II",
    "section": "4 DATA: Mammal body mass**",
    "text": "4 DATA: Mammal body mass**\nThese data were published as a data paper in Ecology and deposited in the Ecological Archives (F. A. Smith, S. K. Lyons, S. K. M. Ernest, K. E. Jones, D. M. Kaufman, T. Dayan, P. A. Marquet, J. H. Brown, and J. P. Haskell. 2003. Body mass of late Quaternary mammals. Ecology 84: 3403.)\nMost of the variables are categorical, with multiple named categories. “Continent” includes mammals on islands (“Insular” category) whereas “Oceanic” refers to marine mammals. Body mass (in grams) is the sole numeric variable. The “status” variable indicates whether species is currently present in the wild (extant), extinct as of late Pleistocene (extinct), extinct within the last 300 years (historical), or an introduced species (introduction).\n\n\n4.1 Read and examine the data\nThe data file we will work with is biogeogMammals.csv.\n\nRead the contents of the file to a data frame.\nUse the head() function to view the first few lines of the data frame on the screen. You’ll see that every row represents the data for a different mammal species.\n\n\n\n\n4.2 Frequency tables\n\nWhich continent has the greatest number of mammal species? Which has the least? Make a table (e.g., using table()) of the frequency of cases on each continent.\n\n2.You’ll notice in the frequency table for the variable “continent” that there’s a typo in the data. One case is shown as having the continent Af rather than AF. Fix this using the command line in R and recalculate the frequency table.\n3.How many extinct mammals are recorded in the data file? Use a frequency table to find out. Create a two-way frequency table (contingency table) showing the status of mammal species on each continent.\n4.Judging by eye, which continent has the greatest number of extinctions relative to the number of extant species?\n\n\n\n4.3 Suggested solutions\nAll lines below beginning with double hashes are R output\n\n\n# Load the packages you might need\n\n# Load data\n# NB your file path may be different than mine\nmammals <- read.csv(\"data/biogeogMammals.csv\")\nhead(mammals)\n\n  continent status        order  family      genus       species mass.grams\n1        AF extant Artiodactyla Bovidae      Addax nasomaculatus    70000.3\n2        AF extant Artiodactyla Bovidae  Aepyceros      melampus    52500.1\n3        AF extant Artiodactyla Bovidae Alcelaphus    buselaphus   171001.5\n4        AF extant Artiodactyla Bovidae Ammodorcas       clarkei    28049.8\n5        AF extant Artiodactyla Bovidae Ammotragus        lervia    48000.0\n6        AF extant Artiodactyla Bovidae Antidorcas   marsupialis    39049.9\n\n# Number of mammal species on each continent\ntable(mammals$continent)\n\n\n     Af      AF     AUS      EA Insular     NAm Oceanic      SA \n      1    1033     346    1033    1484     779      78     977 \n\n# Fix \"Af\"\nwhich(mammals$continent==\"Af\")\n\n[1] 322\n\nmammals$continent[322]<-\"AF\" # replace the wrong value\n\ntable(mammals$continent) # now the table is fixed\n\n\n     AF     AUS      EA Insular     NAm Oceanic      SA \n   1034     346    1033    1484     779      78     977 \n\n# How many extinct mammals?\n# The table shows that 242 species of mammal are listed as extinct\nz <- table(mammals$status)\nz\n\n\n      extant      extinct   historical introduction \n        5388          242           84           17 \n\n# Extinction status by continent (contingency table)\n# The table shows that Australia (AUS) has the greatest number of extinct species\n# relative to the total number.\ntable(mammals$continent, mammals$status)\n\n         \n          extant extinct historical introduction\n  AF        1017      13          4            0\n  AUS        261      45         23           17\n  EA        1027       0          6            0\n  Insular   1405      29         50            0\n  NAm        700      78          1            0\n  Oceanic     78       0          0            0\n  SA         900      77          0            0"
  },
  {
    "objectID": "lab09-graph2.html#example-graphing-frequency-distributions",
    "href": "lab09-graph2.html#example-graphing-frequency-distributions",
    "title": "Lab 09 Graphing II",
    "section": "5 EXAMPLE: Graphing frequency distributions",
    "text": "5 EXAMPLE: Graphing frequency distributions\n\n\nPlot the number of mammal species on each continent using a simple bar graph. Include a label for the y axis.\nThe plot categories are listed in alphabetical order by default, which is arbitrary and makes the visual display less efficient than other possibilities. Redo the bar graph with the continents appearing in order of decreasing numbers of species.\nGenerate a histogram of the body masses of mammal species. How informative is that?!\nCreate a new variable in the mammal data frame: the log (base 10) of body mass. (See “Transform and add a variable” on the R tips “Data” page if you need help with this.)\nGenerate a histogram of log body mass. Is this more informative? Morphological data commonly require a log-transformation to analyze.\nRedo the previous histogram but use a bin width of 2 units. How much detail is lost?\nRedo the histogram but try a bin width of of 1; then try 0.5; and then 0.1. Which bin width is superior?\nRedo the histogram, but display probability density instead of frequency.\nHow does the frequency distribution of log body mass depart from a normal distribution? Answer by visual examination of the histogram you just created. Now answer by examining a normal quantile plot instead. Which display is more informative?\nOptional: redraw the histogram of log body mass and superimpose a normal density curve to assess help detect deviations from normality.\n\n\n\n5.1 Suggested solutions\n\n# Load the packages you might need\nlibrary(ggplot2)\n\n# Bar plot of mammal species by continent\nbarplot(\n  table(mammals$continent),\n  col = \"goldenrod\",\n  cex.names = 0.8,\n  ylim = c(0, 1600),\n  las = 1\n)\n\n\n\n\n\n\n# Barplot sorted by frequency\nbarplot(\n  sort(table(mammals$continent),\n       decreasing = TRUE),\n  col = \"goldenrod\",\n  cex.names = 0.8,\n  las = 1,\n  ylim = c(0, 1600),\n  ylab = \"Frequency\"\n)\n\n\n\n\n\n\n# ggplot methods\nggplot(mammals, aes(x = continent)) +\n  geom_bar(stat = \"count\", fill = \"goldenrod\") +\n  labs(x = \"Continent\", y = \"Frequency\") +\n  theme_classic()\n\n\n\n\n\n\n\n# To order by category in ggplot, first make a new factor variable\n\nmammals$continent_ordered <- factor(mammals$continent,\n                                    levels = names(sort(table(mammals$continent),\n                                                        decreasing = TRUE)))\n\nggplot(mammals, aes(x = continent_ordered)) +\n  geom_bar(stat = \"count\", fill = \"goldenrod\") +\n  labs(x = \"Continent\", y = \"Frequency\") +\n  theme_classic()\n\n\n\n\n\n\n# Histogram of body masses\nhist(mammals$mass.grams, col=\"goldenrod\", \n      right = FALSE, \n      las = 1, \n      xlab = \"Body mass (g)\", \n      main = \"\")\n\n\n\n\n\n\n# Add a new variable, log10 of body mass\nmammals$logmass <- log10(mammals$mass.grams)\n\nhist(mammals$logmass, \n      col=\"goldenrod\", \n      right = FALSE, \n      las = 1,\n      xlab = \"Log10 body mass\", \n      main = \"\", \n      breaks = seq(0, 8.5, by = 0.5))\n\n\n\n\n\nSame but using ggplot. You’ll see a Warning: Removed rows containing non-finite values. These are rows with missing data on mass. Use the argument na.rm = TRUE in geom_histogram to get rid of the warning.\n\nggplot(mammals, aes(x = logmass)) + \n    geom_histogram(fill = \"goldenrod\", \n                  col = \"black\", \n                  binwidth = 0.5, \n                  boundary = 0) + \n    labs(x = \"log10 body mass\", y = \"Frequency\") + \n    theme_classic()\n\n\n\n\n\n\n\n# Plot density instead\nhist(mammals$logmass, \n      col=\"goldenrod\", \n      right = FALSE, \n      las = 1, \n      prob = TRUE,\n      xlab = \"Log10 body mass\", \n      main = \"\", \n      breaks = seq(0, 8.5, by = 0.5))\n\n\n\n\n\n\n# with ggplot\nggplot(mammals, aes(x = logmass)) + \n    geom_histogram(fill = \"goldenrod\", \n                  col = \"black\", \n                  binwidth = 0.5, \n                  boundary = 0, \n                  aes(y = ..density..)) + \n    labs(x = \"log10 body mass\", y = \"Density\") + \n  theme_classic()\n\n\n\n## Warning: Removed 1372 rows containing non-finite values (stat_bin).\n\n\n\n# Normal quantile plot\nqqnorm(mammals$logmass)\n# adds the straight line for comparison through 1st and 3rd quartiles\nqqline(mammals$logmass, col = 'red') \n\n\n\n\n\n\n# Histogram with best-fit normal curve superimposed.\n# The curve function is fussy about the name of the variable: must be \"x\"\nx <- mammals$logmass\n\nhist(\n  x,\n  col = \"goldenrod\",\n  right = FALSE,\n  las = 1,\n  prob = TRUE,\n  xlab = \"Log10 body mass\",\n  main = \"\",\n  breaks = seq(0, 8.5, by = 0.5)\n)\n\nm <- mean(x, na.rm = TRUE)\ns <- sd(x, na.rm = TRUE)\n\ncurve(\n  dnorm(x, mean = m, sd = s),\n  col = \"blue\",\n  lwd = 3,\n  lty = 3,\n  add = TRUE\n)"
  },
  {
    "objectID": "lab09-graph2.html#example-comparing-frequency-distributions",
    "href": "lab09-graph2.html#example-comparing-frequency-distributions",
    "title": "Lab 09 Graphing II",
    "section": "6 EXAMPLE Comparing frequency distributions",
    "text": "6 EXAMPLE Comparing frequency distributions\n\n\nUse a box plot to compare the distribution of body sizes (log scale most revealing) of mammals having different extinction status. Are extinct mammals similar to, larger than, or smaller than, extant mammals?\nExamine the previous box plot. How do the shapes of the body size distributions compare between extinct and extant mammals?\nRedo the previous box plot but make box width proportional to the square root of sample size. Add a title to the plot.\nDraw a violin plot to compare the frequency distribution of log body sizes of mammals having different extinction status. Which do you find is more revealing about the shapes of the body size distributions: box plot or violin plot?\nUse multiple histograms to compare the frequency distribution of log body sizes of mammals having different extinction status. Stack the panels one above the other. In this plot, how easy is it to visualize differences among treatments in the distributions compared to your previous plots?\nMake a table of the median log body mass of each extinction-status group of mammals. Are the values consistent with the plotted distributions?\n\n\n\nSuggested solutions\nAll lines below beginning with double hashes are R output\n\n# Box plot to compare the distribution of body sizes\n# Extinct mammals tend to have large mass compared to extant mammals.\n# The frequency distributions for these two groups also have opposite skew.\n\nboxplot(logmass ~ status, data = mammals, ylab = \"log10 body mass\", \n        col = \"goldenrod1\", las = 1)\n\n\n\n\n\n\n# or ggplot method\nggplot(mammals, aes(x = status, y = logmass)) + \n    geom_boxplot(fill = \"goldenrod1\", notch = FALSE) + \n    labs(x = \"Status\", y = \"Log10 body mass\") + \n    theme_classic()\n\n\n\n\n\n\n# Violin plot\nggplot(mammals, aes(x = status, y = logmass)) + \n    geom_violin(fill = \"goldenrod1\") + \n    labs(x = \"Status\", y = \"Log10 body mass\") + \n    stat_summary(fun.y = mean,  geom = \"point\", color = \"black\") +\n  theme_classic()\n\n\n\n\n\n\n# Multiple histograms\nggplot(mammals, aes(x = logmass)) + \n    geom_histogram(fill = \"goldenrod1\", col = \"black\", \n             binwidth = 0.2, boundary = 0) +\n    labs(x = \"log10 body mass\", y = \"Frequency\") + \n    facet_wrap(~status, ncol = 1, scales = \"free_y\", strip.position = \"right\") +\n  theme_classic()"
  },
  {
    "objectID": "lab09-graph2.html#data-fly-sex-and-longevity",
    "href": "lab09-graph2.html#data-fly-sex-and-longevity",
    "title": "Lab 09 Graphing II",
    "section": "7 DATA: Fly sex and longevity",
    "text": "7 DATA: Fly sex and longevity\n\nThe data are from L. Partridge and M. Farquhar (1981), Sexual activity and the lifespan of male fruit flies, Nature 294: 580-581. The experiment placed male fruit flies with varying numbers of previously-mated or virgin females to investigate how mating activity affects male lifespan. The data are in the file fruitflies.csv.\n\n\n7.1 Inspect the data\nOpen the data file in a spreadsheet program to have a look at it. View the first few lines of the data frame on the screen, and familiarize yourself with the variable names.\nOur goal here is to find a plot type that clearly and efficiently visualizes the patterns in the data, especially the differences among groups.\n\n\n\n7.2 Analysis\n\n\nRead the data file into a new data frame.\nUse a strip chart to examine the distribution of longevities in the treatment groups. Try the jitter method to reduce overlap between points. If needed, adjust the size or rotation of the treatment labels so that they all fit on the graph. What pattern of differences between treatments in longevity is revealed?\nCompare the strip chart to a box plot of the same data. Is the pattern in the data as clear in both types of plot?\nThe variable thorax stands for thorax length, which was used as a measure of body size. The measurement was included in case body size also affected longevity. Produce a scatter plot of thorax length and longevity. Make longevity the response variable (i.e., plot it on the vertical axis). Is there a relationship?\nRedraw the scatter plot but this time use different symbols or colors for the different treatment groups. Add a legend to identify the symbols. Describe the pattern of differences between treatments.\nAdd scatterplot smoothers or linear regressions to the previous figure, one for each group. Do the differences in longevity among the treatments stand out when variation in body size is incorporated?\n\n\n\n\n7.3 Suggested solutions\n\n\n# Read and inspect data\n# NB your file path may be different than mine\nx <- read.csv(\"data/fruitflies.csv\")\n\nhead(x)\n\n  Npartners          treatment longevity.days thorax.mm\n1         8 8 pregnant females             35      0.64\n2         8 8 pregnant females             37      0.68\n3         8 8 pregnant females             49      0.68\n4         8 8 pregnant females             46      0.72\n5         8 8 pregnant females             63      0.72\n6         8 8 pregnant females             39      0.76\n\n# Strip chart\nstripchart(longevity.days ~ treatment, \n          data = x, \n          vertical = TRUE, \n          method = \"jitter\",\n          pch = 16, \n          col = \"blue\", \n          cex.axis=0.7, \n          ylab=\"Longevity (days)\")\n\n\n\n\n\n\n# Strip chart using ggplot\nggplot(x, aes(x=treatment, y=longevity.days)) +\n  geom_jitter(color = \"blue\", size = 3, width = 0.15) +\n  labs(x = \"Treatment\", y = \"Longevity (days)\") + \n  theme_classic()\n\n\n\n\n\n\n# Box plot\nboxplot(\n  longevity.days ~ treatment,\n  data = x,\n  cex.axis = .7,\n  ylab = \"Longevity (days)\",\n  boxwex = 0.5,\n  col = \"goldenrod1\"\n)\n\n\n\n\n\n\n# Box plot using ggplot\nggplot(x, aes(x=treatment, y=longevity.days)) +\n  geom_boxplot(fill = \"goldenrod1\", width = 0.5) +\n  labs(x = \"Treatment\", y = \"Longevity (days)\") + \n  theme_classic()\n\n\n\n\n\n\n# Scatter plot\nplot(\n  longevity.days ~ thorax.mm,\n  data = x,\n  pch = 16,\n  col = \"blue\",\n  las = 1,\n  xlab = \"Thorax length (mm)\",\n  ylab = \"Longevity (days)\"\n)\n\n\n\n\n\n\n# Scatter plot with ggplot\nggplot(x, aes(x = thorax.mm, y = longevity.days)) + \n    geom_point(size = 3, col = \"blue\") + \n    labs(x = \"Thorax length (mm)\", y = \"Longevity (days)\") + \n    theme_classic()\n\n\n\n\n\n\n# Not run - just here to show code\n\n# Scatter plot with separate colors for each group \nplot(\n  longevity.days ~ thorax.mm,\n  data = x,\n  pch = as.numeric(factor(treatment)),\n  col = as.numeric(factor(treatment)),\n  las = 1,\n  xlab = \"Thorax length (mm)\",\n  ylab = \"Longevity (days)\"\n)\n\n# NB the locator(1) function allows you to click ON YOUR PLOT\n# to place the legend... neat eh?\n# legend(\n#   locator(1),\n#   legend = as.character(levels(factor(x$treatment))),\n#   pch = 1:length(levels(factor(x$treatment))),\n#   col = 1:length(levels(factor(x$treatment)))\n# )\n\nlegend(\n  y = 90, x = .65,\n  legend = as.character(levels(factor(x$treatment))),\n  pch = 1:length(levels(factor(x$treatment))),\n  col = 1:length(levels(factor(x$treatment)))\n)\n\n\n\n\n\n# Scatter plot with separate colors for each group using ggplot\nggplot(x, aes(x = thorax.mm, y = longevity.days, colour = treatment, \n            shape = treatment)) + \n    geom_point(size = 2) + \n    labs(x = \"Thorax length (mm)\", y = \"Longevity (days)\") + \n    theme_classic()\n\n\n\n\n\n\n# Add lines; shown for ggplot method only\nggplot(x, aes(x=thorax.mm, y=longevity.days, colour = treatment, \n            shape = treatment)) + \n    geom_point(size = 2) +\n    geom_smooth(method = lm, size = 1, se = FALSE) +\n    labs(x = \"Thorax length (mm)\", y = \"Longevity (days)\") + \n    theme_classic()\n\n\n\n## `geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "lab09-graph2.html#harper-adams-data-science",
    "href": "lab09-graph2.html#harper-adams-data-science",
    "title": "Lab 09 Graphing II",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab10-sample-size.html#objectives",
    "href": "lab10-sample-size.html#objectives",
    "title": "Lab 10 Sample size",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nEvaluate principles to increase sampling precision in experimental design\nEvaluate power of sampling\nDemonstrate power tools in R, like {pwr}\n\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab10-sample-size.html#start-a-script",
    "href": "lab10-sample-size.html#start-a-script",
    "title": "Lab 10 Sample size",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab10-sample-size.html#experiment-planning-tools",
    "href": "lab10-sample-size.html#experiment-planning-tools",
    "title": "Lab 10 Sample size",
    "section": "3 Experiment planning tools",
    "text": "3 Experiment planning tools\n\nBefore carrying out a time- and fund-consuming experiment, it is useful to get an idea of what to expect from the results. How big an effect are you expecting? What are the chances that you would detect it? What sample size would you need to have a reasonable chance of succeeding? How narrow a confidence interval around the estimated effect would you be happy with? In this lab we will show how R can be used to address some of these questions."
  },
  {
    "objectID": "lab10-sample-size.html#random-sampling-warm-up",
    "href": "lab10-sample-size.html#random-sampling-warm-up",
    "title": "Lab 10 Sample size",
    "section": "4 Random sampling warm-up",
    "text": "4 Random sampling warm-up\n\nTo begin, let’s get some practice sampling (randomly) categorical and Gaussian-distributed data from a population. The intention is to use sample\n\n\nRandomly sample 20 observations from a population having two groups of individuals, “infected” and “uninfected”, in equal proportions. Summarize the results in a frequency table.\nRepeat the previous step five times to convince yourself that the outcome varies from sample to sample.\nSample 18 individuals from a population having two groups of individuals, “mated” and “unmated”, where the proportion mated in the population is 0.7. Summarize the results in a frequency table.\nRepeat the previous step five times to convince yourself that the outcome varies from sample to sample.\nSample 30 observations from a Gaussian-distributed population having mean 0 and standard deviation 2. Plot the results in a histogram.\nRepeat the following 5 times and calculate the mean each time: sample 30 observations from a Gaussian-distributed population having mean 0 and standard deviation 2. Convince yourself that the sample mean is different each time.\n\n\n\n4.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\n# 1.\n\n# run this several times in your own script \n# and see how the output changes\nsample(c(\"infected\",\"uninfected\"), \n       prob = c(.5,.5),\n       size = 20, \n       replace = TRUE)\n\n [1] \"infected\"   \"uninfected\" \"infected\"   \"uninfected\" \"uninfected\"\n [6] \"uninfected\" \"uninfected\" \"infected\"   \"infected\"   \"uninfected\"\n[11] \"infected\"   \"infected\"   \"uninfected\" \"infected\"   \"infected\"  \n[16] \"infected\"   \"uninfected\" \"uninfected\" \"uninfected\" \"uninfected\"\n\n# 3. \nz <- sample(c(\"mated\",\"unmated\"), size = 18, replace = TRUE, prob = c(.7,.3))\n\ntable(z)\n\nz\n  mated unmated \n     14       4 \n\n# 5. \n\nz <- rnorm(30, mean = 0, sd = 2)\nhist(z, right = FALSE, col = \"goldenrod\", las = 1)\n\n\n\n\n\n\n# 6. \nz <- rnorm(1000, mean = 0, sd = 2)\n\nhist(z, right = FALSE, col = \"goldenrod\", las = 1)"
  },
  {
    "objectID": "lab10-sample-size.html#plan-for-precision",
    "href": "lab10-sample-size.html#plan-for-precision",
    "title": "Lab 10 Sample size",
    "section": "5 Plan for precision",
    "text": "5 Plan for precision\nConsider an experiment to estimate mate preference of females of a species of jumping spider. Each independent trial involves presenting a female spider with two tethered males. One of the males is from her own species, and the other is from its sister species. To avoid pseudoreplication, females are tested only once and males are replaced between tests. You want to estimate p, the proportion of female spiders that choose males of their own species. Before carrying out the experiment, it is useful to generate data under different scenarios to get a sense of the sample size you would need to estimate preference with sufficient precision.\n\n\n5.1 Estimate weak or no preference\nWe’ll start with the case of weak or no preference: Imagine that females choose males essentially randomly (p = 0.5), with half choosing the male from her own species and the other half picking the male of the other species. How much data would you need to demonstrate this (and convince your skeptical supervisory committee)? One idea is to collect data and use it to test the null hypothesis of no preference. If the null hypothesis is true, you should fail to reject it. However, this won’t be very convincing to your committee. Failing to reject a null hypothesis is inconclusive by itself. Maybe your test won’t have much power.\nA second idea is to plan your sample size so as to obtain a narrow confidence interval (i.e. having high precision) for the strength of preference. If, at the end of your experiment, you end up with an estimate of p close to 0.5 AND your 95% confidence interval for p is relatively narrow, you’ll be in a strong position to say that the true preference really is weak, even if you can’t say it is exactly 0.5. What sample size is necessary to achieve a reasonably narrow confidence interval in this case? Investigate this question by simulating data."
  },
  {
    "objectID": "lab10-sample-size.html#questions",
    "href": "lab10-sample-size.html#questions",
    "title": "Lab 10 Sample size",
    "section": "6 Questions",
    "text": "6 Questions\n\nRandomly sample n = 10 females from a population having equal numbers of “successes” (females who choose males of her own species) and “failures” (females who choose males of the other species). What was the proportion of successes in your sample?\nUsing the data from step 1, calculate an approximate 95% confidence interval for the population proportion of successes. Use the Agresti-Coull method in the binom package in R, which you will need to install if you haven’t already done so.\n\n\n\n# if(!require(\"binom\")) install.packages(\"binom\")\nlibrary(binom)\n\n\nTo obtain the 95% confidence interval, use the binom.confint function explained below. The argument x is the number of “successes” in your generated sample (number of females who chose males of her own species) and n is the sample size (number of females tested).\n\n\n# gets the confidence interval\nn <- 50 # number of trials\nx <- 27 # number of successes\nmyCI <- binom.confint(x, n, method = \"ac\")  \n\nprint(myCI)   # shows the results\n\n         method  x  n mean     lower     upper\n1 agresti-coull 27 50 0.54 0.4039603 0.6703319\n\nmyCI$lower    # lower limit of confidence interval\n\n[1] 0.4039603\n\nmyCI$upper    # upper limit\n\n[1] 0.6703319\n\n\n\nObtain the 95% confidence interval for the proportion using your data from step 1. What was the span of your confidence interval (upper limit minus lower limit)? Can you be confident that the true population proportion is 0.5 or very close to it?\n\n\nRepeat steps 1 and 2 five times and keep a record of the confidence intervals you obtained. What was the lowest value for the span of the confidence interval from the 5 samples?\n\n\n\nYou can speed up the effort if you create a for loop in R that automatically repeats steps 1 and 2 as many times as you decide. A loop that repeats ten times would look something like the following. The “i” in this loop is a counter, starting at 1 and increasing by 1 each time the commands in the loop are executed. Don’t forget to include a command inside the loop to print each result.\n\n\n\nfor(i in 1:10) {\n#  [paste in your R commands for steps 1 and 2 here]\n}\n\n\n\nIncrease the sample size to n = 20 and run the loop from step 4 again. How much narrower are the confidence interval spans? Are the spans adequate?\n\n\n\nBy modifying the sample size and re-running the loop a bunch of times, find a sample size (ballpark, no need to be exact at this point) that usually produces a confidence interval having a span no greater than 0.2. This would be the span of a confidence interval that had, e.g., a lower limit of 0.4 and an upper limit of 0.6. Surely this would be convincing evidence that the mate preference really was weak.\n\nBy this point you might wish to speed things up by saving the results of each iteration to a vector or data frame rather than print the results to the screen. This will make it possible to increase the number of iterations (say, to 100 times instead of just 10) for a given value of n.\n\n\nGiven the results of step 6, you would now have some design options before you. Is the sample size n that your simulation indicated was needed to generate a confidence interval of span 0.2 realistic? In other words, would an experiment with so many female spiders (and so many males) be feasible? If the answer is yes, great, get started on your experiment! If the answer is no, the sample size required is unrealistically large, then you have some decisions to make:\n\n\nForget all about doing the experiment. (Consider a thesis based on theory instead.)\nRevise your concept of what represents a “narrow” confidence interval. Maybe a confidence interval for p spanning, say, 0.3 to 0.7 (a span of 0.4) would be good enough to allow you to conclude that the preference was “not strong”. This would not require as big a sample size as a narrower interval.\n\n\n\nRepeat the above procedures to find a sample size that usually gives a confidence interval having a span of 0.4 or less.\n\n\n\n6.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\nlibrary(binom)\n\n# 1. 10 females from a population having equal numbers \n# of successes (1) and failures (0)\n\nx <- sample(c(1, 0), size = 10, c(0.5, 0.5), replace = TRUE)\nx\n\n [1] 0 1 1 1 0 0 0 0 1 0\n\n##  [1] 1 1 0 0 0 1 1 1 1 0\n\nsum(x)/10\n\n[1] 0.4\n\n## [1] 0.6\n\n# 2. \n\nmyCI <- binom.confint(sum(x), length(x), method = \"ac\")\nmyCI\n\n         method x  n mean     lower     upper\n1 agresti-coull 4 10  0.4 0.1671106 0.6883959\n\n##          method x  n mean     lower     upper\n## 1 agresti-coull 6 10  0.6 0.3116041 0.8328894\n\nmySpan <- myCI$upper - myCI$lower\nmySpan\n\n[1] 0.5212853\n\n## [1] 0.5212853\n\n# 3 & 4. \n\n# initialize empty vector \"span\"\nspan <- vector()\n\nfor(i in 1:5){\n    x <- sample(c(1,0), size = 10, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), 10, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nmin(span)\n\n[1] 0.4299092\n\n## [1] 0.4747451\n\n# 5.\n\nn <- 20       # sample size each time\nnruns <- 100  # number of runs\n\nspan <- vector()\n\nfor(i in 1:nruns){\n    x <- sample(c(1,0), size = n, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), n, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nhist(span, right = FALSE, col = \"goldenrod\", las = 1)\n\n\n\n\n\n\n# 6.\n\nn <- 93       # sample size each time\nnruns <- 100  # number of runs\n\nspan <- vector()\n\nfor(i in 1:nruns){\n    x <- sample(c(1,0), size = n, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), n, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nhist(span, right = FALSE, col = \"goldenrod\", las = 1)\n\n\n\n\n\n\n# 8.\n\nn <- 21       # sample size each time\nnruns <- 100  # number of runs\n\nspan <- vector()\n\nfor(i in 1:nruns){\n    x <- sample(c(1,0), size = n, c(0.5,0.5), replace = TRUE)\n    myCI <- binom.confint(sum(x), n, method = \"ac\")\n    span[i] <- myCI$upper - myCI$lower\n    }\n\nhist(span, right = FALSE, col = \"goldenrod\", las = 1)"
  },
  {
    "objectID": "lab10-sample-size.html#plan-for-power",
    "href": "lab10-sample-size.html#plan-for-power",
    "title": "Lab 10 Sample size",
    "section": "7 Plan for power",
    "text": "7 Plan for power\n\nAssume that the preference p really is different from 0.5, and use null hypothesis significance testing to detect it. What strength of preference would we like to be able to detect in our experiment? To pick an extreme case, if the true proportion of females in the population choosing a male from her own species is 0.51 rather than 0.50, you would need an enormous sample size to detect it. But we don’t really care about such a small effect. Let’s start instead with the more realistic proportion p = 0.7. What sample size would be needed to detect it with reasonably high probability?\n\n\nSample 20 females from a population in which the true fraction of “successes” is 0.7\n\n\n\nApply the binomial test to your sample, to test the null hypothesis that the population proportion is 0.5. The binomial test calculates the exact 2-tailed probability of a result as extreme or more extreme as that observed if the null hypothesis is true. The method is implemented in R in the following command,\n\n\n# z <- binom.test(x, n, p = 0.5)\n\n\nwhere x is the observed number of successes in your sample from step 1, and n is the sample size. z here is an object that stores the result. To see the results of the test enter print(z) or just z in the command line. If you just want to see the resulting P-value of the test, enter this instead:\n\n# z$p.value\n\n\nDid you reject the null hypothesis?\n\n\nCreate a loop to repeat steps 1 and 2 ten times. In what fraction of iterations was the null hypothesis rejected?\n\n\n\nBy modifying the sample size and re-running the loop multiple times, find a sample size (ballpark, no need to be exact at this point) that usually results in the null hypothesis being rejected. Compare your results to those from the confidence interval simulation above.\n\n\n\nIs the sample size you determined feasible in an experiment? If the answer is yes, great! If the answer is no, because the sample size required is too large, then you have some decisions to make. You could decide not to run the experiment after all. Or, you could revise your aims. Perhaps your committee would be happy if you if you could detect a preference of 0.8 instead of 0.7.\n\n\n\n7.1 Suggested solutions\n\n\n# 1. Sample 20 females from a population in which the true fraction of \"successes\" is 0.7\n\nx <- sample(c(\"success\",\"failure\"), size = 20, c(0.7,0.3), replace = TRUE)\nnsuccess <- length(which(x == \"success\"))\n\n\n# 2. Apply the binomial test\n\nz <- binom.test(nsuccess, 20, 0.5)\nz$p.value\n\n[1] 0.1153183\n\n# 3. Repeat 10 times\n\nresult <- vector()\n\nfor(i in 1:10){\n    x <- sample(c(\"success\",\"failure\"), size = 20, c(0.7,0.3), replace = TRUE)\n    nsuccess <- length(which(x == \"success\"))\n    z <- binom.test(nsuccess, 20, 0.5)\n    result[i] <- z$p.value\n    }\n\nwhich(result <= 0.05)\n\n[1] 2 5 6 8 9\n\n# 4. Repeating 100 times shows that a sample size of n = 50 females \n#   seems to reject Ho roughly 80% of the time\n\nn <- 50 \n\nresult <- vector()\n\nfor(i in 1:100){\n    x <- sample(c(\"success\",\"failure\"), size = n, c(0.7,0.3), replace = TRUE)\n    nsuccess <- length(which(x == \"success\"))\n    z <- binom.test(nsuccess, n, 0.5)\n    result[i] <- z$p.value\n    }\n\nlength(which(result <= 0.05))/100\n\n[1] 0.75"
  },
  {
    "objectID": "lab10-sample-size.html#power-tools-in-r",
    "href": "lab10-sample-size.html#power-tools-in-r",
    "title": "Lab 10 Sample size",
    "section": "8 Power tools in R",
    "text": "8 Power tools in R\nSimulating random samples on the computer, as we did above, is a great way to investigate power and sample size requirements. It works in any situation and can mimic, even complicated study designs. However, a number of quantitative tools have been developed for mainly simple designs that do the work for you.\n\n8.1 Try the {pwr} package\n\nLoad the pwr library and use it to do some of the calculations for you. See the [Sample size and power page here for tips](https://dsgarage.netlify.app/misc-r/04-sample-size-power/.\n\n\nUse the pwr package to calculate the approximate minimum sample size needed to detect a preference of 0.6 with a power of 0.80 (i.e., the null hypothesis would be rejected in 80% of experiments). The null hypothesis is that the population proportion p of females who would choose the male from her own population is 0.5. The goal is to design an experiment that has a high probability of rejecting the null hypothesis when p is 0.6.\n\n\n\nRepeat the above procedure for a preference of 0.7, 0.8, and 0.9.\n\n\n\n\n8.2 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\n# You might need to install {pwr}\nlibrary(pwr)\n\n# 1.\n\nh <- ES.h(0.5, 0.6)\n\nz <- pwr.p.test(h, power = 0.8)\nz$n\n\n[1] 193.5839\n\n# 2.\n\n# Repeat for range values of pref\n\npref <- c(0.6, 0.7, 0.8, 0.9)\n\nfor(i in 1:length(pref)){\n  h <- ES.h(0.5, pref[i])\n  z <- pwr.p.test(h, power = 0.8)\n  print(z$n)\n}\n\n[1] 193.5839\n[1] 46.34804\n[1] 18.95431\n[1] 9.127904"
  },
  {
    "objectID": "lab10-sample-size.html#plan-a-2-x-2-experiment",
    "href": "lab10-sample-size.html#plan-a-2-x-2-experiment",
    "title": "Lab 10 Sample size",
    "section": "9 Plan a 2 x 2 experiment",
    "text": "9 Plan a 2 x 2 experiment\n\nIn an experiment on the great tit, two eggs were removed from 30 nests, which caused the attending females to lay one more egg. 35 un-manipulated nests served as controls. The response variable was incidence of malaria in female great tits at the end of the experiment. The results of the experiment are tabulated below.\n\n\n\n\n\nSurvivors\n\n\n\n\nImagine that you are considering repeating this experiment on a different species of songbird. What are the chances of detecting an effect? What sample sizes should you plan?\n\n\nRandomly sample 30 females from a control population in which the fraction of infected birds is 0.2 (the fraction in the tit data). Sample also 30 females from an experimental population in which the fraction of infected birds is 0.5 (the fraction in the tit data). Combined the samples into a data frame. Include a variable indicating treatment.\n\n\n\nDisplay the 2 x 2 frequency table from your random sample. Is there an association?\n\n\n\nRepeat steps 1-3 three times to convince yourself that the answer is different each time.\n\n\n\nUsing the tools in pwr calculate the sample size needed to achieve 80% power in this design.\n\n\n\n9.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\nlibrary(pwr)\n\n\n# 1. \n\nx <-\n  sample(\n    c(\"malaria\", \"no\"),\n    size = 30,\n    replace = TRUE,\n    prob = c(0.2, 0.8)\n  )\n\ny <-\n  sample(\n    c(\"malaria\", \"no\"),\n    size = 30,\n    replace = TRUE,\n    prob = c(0.5, 0.5)\n  )\n\nz <- rep(c(\"control\", \"treat\"), c(30, 30))\n\nmydata <- cbind.data.frame(\n  response = c(x, y),\n  treatment = z,\n  stringsAsFactors = FALSE\n)\n\n# 2.\n\ntable(mydata)\n\n         treatment\nresponse  control treat\n  malaria       4    14\n  no           26    16\n\n# 3. Repeat above\n\n# 4. \n\ncontrol <- c(0.2,0.8)\ntreatment <- c(0.5,0.5)\n\nprobs <- cbind.data.frame(treatment = treatment,\n                          control = control,\n                          stringsAsFactors = FALSE)\n\n# Cohen's effect size \"w\"\nw <- ES.w2(probs/sum(probs))           \n\nz <- pwr.chisq.test(w, df = 1, power = 0.80)\nz$N\n\n[1] 79.36072"
  },
  {
    "objectID": "lab10-sample-size.html#plan-a-2-treatment-experiment",
    "href": "lab10-sample-size.html#plan-a-2-treatment-experiment",
    "title": "Lab 10 Sample size",
    "section": "10 Plan a 2-treatment experiment",
    "text": "10 Plan a 2-treatment experiment\n\nImagine that you are considering a two-treatment experiment for a numeric response variable. The treatments consist of two grazing regimes and the response variable is the number of plant species in plots at the end of the experiment. How many replicate plots should you set up? As usual, we will investigate only the case of equal sample sizes in the two groups.\n\nWe’ll assume that the number of plant species in plots has a Gaussian distribution in both treatment and control. We’ll round the numbers so that they are integers.\n\n\nRandomly sample 20 measurements from a Gaussian distribution having a mean of 10 and a variance of 10 (so the standard deviation is the square root of 10). Call this the “control” group. Let’s round the numbers so that they are integers.\n\n\n# << YOUR CODE HERE FOR RANDOM SAMPLE\n\n# control <- round(control, 0)\n\n\n\nRepeat step 1 for a second sample, this time from a Gaussian distribution having a mean of 15 but the same sample variance, 10. (This is a bit unrealistic, as we would expect the variance in numbers of species to be higher as the mean increases, but never mind for now). Call this the “treatment” group. In other words, we will investigate the power of this experiment to detect a 1.5-fold change in the mean number of species from control to treatment.\n\n\n\nAssemble the samples into a data frame in “long” format, with a second variable indicating which measurements are from the control group and which are from the treatment group. Create a histogram for each of the two samples and compare the distributions by eye.\n\n\n\nUsing the power.t.test command in the basic R stats package, determine the power of the above design – probability that the experiment will detect a significant difference between the treatment and control means based on random samples.\n\n\n\nUsing the same command, determine the sample size that would be necessary to achieve a power of 0.80.\n\n\n\n10.1 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\nlibrary(pwr)\nlibrary(ggplot2)\n\n# 1.\n\nx1 <- rnorm(20, mean = 10, sd = sqrt(10))\nx1 <- round(x1,0)\n\n\n# 2.\n\nx2 <- rnorm(20, mean = 15, sd = sqrt(10))\nx2 <- round(x2,0)\n\n\n# 3.\n\nnspecies <- c(x1, x2)\n\ntreatment <- rep(c(\"control\", \"treatment\"), c(20,20))\n\nmydata <- cbind.data.frame(nspecies, treatment, stringsAsFactors = FALSE)\n\nggplot(mydata, aes(x = nspecies)) + \n        geom_histogram(fill = \"goldenrod\", col = \"black\", \n        boundary = 0, closed = \"left\", binwidth = 2) +\n    labs(x = \"Number of species\", y = \"Frequency\") + \n    theme(aspect.ratio = 0.5) + \n    facet_wrap( ~ treatment, ncol = 1, scales = \"free_y\")+\n    theme_classic()\n\n\n\n\n\n\n# 4.\n\nz <- power.t.test(n = 20, delta = 5, sd = 10)\nz$power\n\n[1] 0.3377084\n\n# 5.\n\nz <- power.t.test(delta = 5, sd = 10, power = 0.80)\nz$n\n\n[1] 63.76576"
  },
  {
    "objectID": "lab10-sample-size.html#harper-adams-data-science",
    "href": "lab10-sample-size.html#harper-adams-data-science",
    "title": "Lab 10 Sample size",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab11-linear-mods.html#objectives",
    "href": "lab11-linear-mods.html#objectives",
    "title": "Lab 11 Linear models",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nLoad and examine linear regression data\nFit a linear model to data\nEvaluate statistical results\n\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab11-linear-mods.html#start-a-script",
    "href": "lab11-linear-mods.html#start-a-script",
    "title": "Lab 11 Linear models",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab11-linear-mods.html#linear-regression",
    "href": "lab11-linear-mods.html#linear-regression",
    "title": "Lab 11 Linear models",
    "section": "3 Linear regression",
    "text": "3 Linear regression\nIn this lab we will use R to fit linear models to data to review basic principles of linear model fitting and implementation. Linear models for fixed effects are implemented in the R command lm(). This method is not suitable for models that contain random effects."
  },
  {
    "objectID": "lab11-linear-mods.html#prediction-with-linear-regression",
    "href": "lab11-linear-mods.html#prediction-with-linear-regression",
    "title": "Lab 11 Linear models",
    "section": "4 Prediction with linear regression",
    "text": "4 Prediction with linear regression\nWe’ll start with linear regression because this is the most familiar type of linear model. The data are from Whitman et al (2004 Nature 428: 175-178), who noticed that the amount of black pigmentation on the noses of male lions increases as they get older. They used data on the proportion of black on the noses of 32 male lions of known age (years) in Tanzania. We will use fit a linear model to these data to predict a lion’s age from the proportion of black in his nose. The data are in the file lions.csv.\n\n\n4.1 Read and examine the data\n\n\nRead the data from the file.\n\n\n\nView the first few lines of data to make sure it was read correctly.\n\n\n\nCreate a scatter plot of the data. Choose the response and explanatory variables with care: we want to predict age from the proportion black in the nose.\n\n\n\n\n4.2 Fit a linear model\n\n\nFit a linear model to the lion data. Store the output in an lm() object. Choose the response and explanatory variables with care: we want to predict age from the proportion black in the nose.\n\n\n\nAdd the best-fit line to the scatter plot. Does the relationship appear linear? From the scatter plot, visually check for any serious problems such as outliers or changes in the variance of residuals.*\n\n\n\nUsing the same fitted model object, obtain the estimates for the coefficients, slope and intercept, and standard errors. What is the R2 value for the model fit?**\n\n\n\nObtain 95% confidence intervals for the slope and intercept.\n\n\n\nTest the fit of the model to the data with an ANOVA table.\n\n\n\nApply the plot() command to the lm() object created in (1) to diagnose violations of assumptions (keep hitting  in the command window to see all the plots). Recall the assumptions of linear models. Do the data conform to the assumptions? Are there any potential concerns? What options would be available to you to overcome any potential problems with meeting the assumptions?*** Most of the plots will be self-explanatory, except perhaps the last one. “Leverage” calculates the influence that each data point has on the estimated parameters. For example if the slope changes a great deal when a point is removed, that point is said to have high leverage. “Cook’s distance” measures the effect of each data point on the predicted values for all the other data points. A value greater than 1 is said to be worrisome. Points with high leverage don’t necessarily have high Cook’s distance, and vice versa.\n\n\n\nOne of the data points (the oldest lion) has rather high leverage. To see the effect this has on the results, refit the data leaving this point out. Did this change the regression line substantially?\n\n\nThe variance of the residuals for black in the nose tends to rise with increasing age but the trend is not severe. The residuals might not be quite normal, but they are not badly skewed so we are probably OK.\n\n** 0.61\n*** It is even easier to see with these plots how the variance of the residuals tends to increase at higher fitted values. A transformation of one or both of the variables is usually the first course of action. R also has a toolkit for robust regression, which as the name suggests is more robust to violations of standard assumptions.\n\n\n\n4.3 Prediction\n\n\nDisplay the data once again in a scatter plot. Add the regression line.\n\n\n\nAdd confidence bands to the scatter plot. These are confidence limits for the prediction of mean of lion age at each value of the explanatory variable. You can think of these as putting bounds on the most plausible values for the “true” or population regression line. Note the spread between the upper and lower limits and how this changes across the range of values for age.\n\n\n\nAdd prediction intervals to the scatter plot. These are confidence limits for the prediction of new individual lion ages at each value of the explanatory variable. Whereas confidence bands address questions like “what is the mean age of lions whose proportion black in the nose is 0.5 ?”, prediction intervals address questions like “what is the age of that new lion over there, which has a proportion black in the nose of 0.5 ?”.\n\n\n\nExamine the confidence bands and prediction intervals. Is the prediction of mean lion age from black in the nose relatively precise? Is prediction of individual lion age relatively precise? Could this relationship be used in the field to age lions?\n\n\n\n\n4.4 Suggested solutions\n\nAll lines below beginning with double hashes are R output\n\n# You might need to install these libraries\nlibrary(visreg, warn.conflicts=FALSE)\nlibrary(ggplot2, warn.conflicts=FALSE)\n\n# Read and examine the data\nx <- read.csv(\"data/lions.csv\", \n        stringsAsFactors = FALSE)\nhead(x)\n\n  age black\n1 1.1  0.21\n2 1.5  0.14\n3 1.9  0.11\n4 2.2  0.13\n5 2.6  0.12\n6 3.2  0.13\n\n# Scatter plot\nplot(age ~ black, \n      data = x, \n      pch = 16, las = 1, col = \"blue\", cex = 1.5,  # Mere vanity\n      xlab = \"Proportion nose black\", \n      ylab = \"Age (years)\")\n\n\n\n\n\nFit a linear model\n\n\n# 1. fit model\n\nz <- lm(age ~ black, data=x)\n\n# 2. Add the best-fit line to the scatter plot\n\nplot(age ~ black, data = x, \n    pch = 16, las = 1, col = \"blue\", cex = 1.5, # Mere vanity\n    xlab = \"Proportion nose black\", \n    ylab = \"Age (years)\")\n\nabline(z, col = 'red')\n\n\n\n\n\n\n# or use ggplot method\n\nggplot(x, aes(y = age, x = black)) +\n    geom_point(size = 3, col = \"blue\") +\n    geom_smooth(method = lm, col = \"red\", size = 0.5, se = FALSE) +\n    labs(x = \"Proportion nose black\", y = \"Age (years)\") + \n    theme_classic()\n\n\n\n## `geom_smooth()` using formula 'y ~ x'\n\n\n\n# 3. Estimate coefficients\n\n# \"(Intercept)\" refers to intercept, \"black\" to slope\nsummary(z)\n\n\nCall:\nlm(formula = age ~ black, data = x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.8790     0.5688   1.545    0.133    \nblack        10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,    Adjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08\n\nvisreg(z, points.par = list(pch = 16, cex = 1.2, col = \"blue\"))\n\n\n\n\n\n\n# 5. ANOVA table\nanova(z)\n\nAnalysis of Variance Table\n\nResponse: age\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nblack      1 138.544 138.544  49.751 7.677e-08 ***\nResiduals 30  83.543   2.785                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n# 6. Diagnostics\n\n# diagnostic plots\npar(mfrow=c(2,2))\nplot(z)\n\n\n\npar(mfrow=c(1,1))\n\n\n\n# 7. Redo after removing oldest lion\n\nz1 <- lm(age ~ black, data=x[x$age < 12, ])\n\nsummary(z1)\n\n\nCall:\nlm(formula = age ~ black, data = x[x$age < 12, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0522 -0.9810 -0.4072  0.6353  3.4973 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.2938     0.5089   2.542   0.0166 *  \nblack         8.8498     1.4175   6.243 8.19e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.447 on 29 degrees of freedom\nMultiple R-squared:  0.5734,    Adjusted R-squared:  0.5587 \nF-statistic: 38.98 on 1 and 29 DF,  p-value: 8.191e-07\n\n\n\nPrediction\n\nz <- lm(age ~ black, data=x)\n\nx2 <- predict(z, interval = \"prediction\")\n\n\n\nx2 <- cbind.data.frame(x, x2, stringsAsFactors = FALSE)\n\nggplot(x2, aes(y = age, x = black)) +\n      geom_point(size = 3, col = \"blue\") +\n    geom_smooth(method = \"lm\", se = TRUE) +\n    geom_line(aes(y = lwr), color = \"black\", linetype = \"dashed\") +\n    geom_line(aes(y = upr), color = \"black\", linetype = \"dashed\") +\n    labs(x = \"Proportion nose black\", y = \"Age (years)\") + \n    theme_classic()"
  },
  {
    "objectID": "lab11-linear-mods.html#light-and-circadian-rhythms",
    "href": "lab11-linear-mods.html#light-and-circadian-rhythms",
    "title": "Lab 11 Linear models",
    "section": "5 Light and circadian rhythms",
    "text": "5 Light and circadian rhythms\n\nOur second example fits a linear model with a categorical explanatory variable. The data are from an experiment by Wright and Czeisler (2002. Science 297: 571) that re-examined a previous claim that light behind the knees could reset the circadian rhythm of an individual the same way as light to the eyes. One of three light treatments was randomly assigned to 22 subjects (a three-hour episode of bright lights to the eyes, to the knees, or to neither). Effects were measured two days later as the magnitude of phase shift in each subject’s daily cycle of melatonin production, measured in hours. A negative measurement indicates a delay in melatonin production, which is the predicted effect of light treatment. The data are in the file “knees.csv”.\n\n\n5.1Read and examine the data\n\n\nRead the data from the file.\n\n\n\nView the first few lines of data to make sure it was read correctly.\n\n\n\nPlot the phase shift data, showing the individual data points in each treatment group.\n\n\n\nDetermine whether the categorical variable “treatment” is a factor. If not a factor, convert treatment to a factor using the factor() command. This will be convenient when we fit the linear model.\n\n\n\nUse the levels() command on the factor variable “treatment” to see how R has ordered the different treatment groups. The order will be alphabetical, by default. Conveniently, you will find that the control group is listed first in the alphabetical sequence. (As you are about to analyze these data with a linear model in R, can you think of why having the control group first in the order is convenient?)\n\n\n\nTo get practice, change the order of the levels so that the “knee” treatment group is second in the order, after “control”, and the “eyes” group is listed third.\n\n\n\nPlot the phase shift data again to see the result.\n\n\n\n\n5.2 Fit a linear model\n\n\nFit a linear model to the light treatment data. Store the output in an lm() object.\n\n\n\nCreate a graphic that illustrates the fit of the model to the data. In other words, include the predicted (fitted) values to your plot.\n\n\n\nUse plot() to check whether the assumptions of linear models are met in this case. Examine the plots. Are there any potential concerns? There are several options available to you if the assumptions are not met (transformations, robust regression methods, etc.) but we don’t seem to need them in this case.\n\n\n\nRemember from lecture that R represents the different levels of the categorical variable using dummy variables. To peek at this behind-the-scenes representation, use the model.matrix() command on the model object from your linear model fit in step (1). The output should have a column of 1’s for the intercept and two additional columns representing two of the three levels of the explanatory variable. Why is one level left out? Which level is the one not represented by a dummy variable?*\n\n\n\nUsing the lm() model object, obtain the parameter estimates (coefficients) along with standard errors. Examine the parameter estimates. If you’ve done the analysis correctly, you should see the three coefficients. Rounded, they are -0.309, -0.027, and -1.24. What do each of these coefficients represent – what is being estimated by each value?** Note that the output will also include an R2 value. This is loosely interpretable as the “percent of the variance in phase shift that is explained by treatment.”\n\n\n\nThe P-values associated with the three coefficients are generally invalid. Why? Under what circumstance might one of the P-values be valid?***\n\n\n\nObtain 95% confidence intervals for the three parameters.\n\n\n\nTest the effect of light treatment on phase shift with an ANOVA table.\n\n\n\nProduce a table of the treatment means using the fitted model object, along with standard errors and confidence intervals. Why are these values not the same as those you would get if you calculated means and SE’s separately on the data from each treatment group?\n\n\n\nOne of the columns must be dropped because the information in the 4 columns is redundant in a particular way (a combination of three of the columns exactly equals the fourth). By default, R drops the column corresponding to the first level of the categorical variables.\n\n** The mean of the first group and the differences between the second and third groups from the first.\n*** The tests shown are t-tests of differences between means. However, a posteriori pairwise comparisons (unplanned comparisons) between groups requires a Tukey test or other test that accounts for the number of pairs of means. The only time a t-test is valid is for a planned comparison.\n\n\n\n5.3 Suggested solutions\nRead and examine the data\n\nlibrary(visreg, warn.conflicts=FALSE)\nlibrary(ggplot2, warn.conflicts=FALSE)\nlibrary(emmeans, warn.conflicts=FALSE)\n\n# 1.\n\nx <- read.csv(\"data/knees.csv\", \n      stringsAsFactors = FALSE)\n\n# 2.\n\nhead(x)\n\n  treatment shift\n1   control  0.20\n2   control  0.53\n3   control -0.68\n4   control -0.37\n5   control -0.64\n6   control  0.36\n\n# 3.\n\nis.factor(x$treatment)\n\n[1] FALSE\n\nx$treatment <- factor(x$treatment)\n\n# 4. \n\nstripchart(shift ~ treatment, data=x, vertical = TRUE, method = \"jitter\", \n    jitter = 0.2, pch = 16, las = 1, cex = 1.5, col = \"blue\",\n    ylab = \"Phase shift (h)\")\n\n\n\n\n\n\n# 5. \n\nlevels(x$treatment)\n\n[1] \"control\" \"eyes\"    \"knee\"   \n\n# 6. \n\nx$treatment <- factor(x$treatment, levels = c(\"control\", \"knee\", \"eyes\"))\nlevels(x$treatment)\n\n[1] \"control\" \"knee\"    \"eyes\"   \n\n# 7.\n\nstripchart(shift ~ treatment, data=x, vertical = TRUE, method = \"jitter\", \n    jitter = 0.2, pch = 16, las = 1, cex = 1.5, col = \"blue\",\n    ylab = \"Phase shift (h)\")\n\n\n\n\n\nFit a linear model\n\n\n# 1.\nz <- lm(shift ~ treatment, data=x)\n\n# 2.\nvisreg(z, \n      whitespace = 0.4, \n      points.par = list(cex = 1.2, col = \"blue\"))\n\n\n\n\n\n\n# 3.\npar(mfrow = c(2,2))\nplot(z)\n\n\n\npar(mfrow = c(1,1))\n\n\n\n# 4. \nmodel.matrix(z)\n\n   (Intercept) treatmentknee treatmenteyes\n1            1             0             0\n2            1             0             0\n3            1             0             0\n4            1             0             0\n5            1             0             0\n6            1             0             0\n7            1             0             0\n8            1             0             0\n9            1             1             0\n10           1             1             0\n11           1             1             0\n12           1             1             0\n13           1             1             0\n14           1             1             0\n15           1             1             0\n16           1             0             1\n17           1             0             1\n18           1             0             1\n19           1             0             1\n20           1             0             1\n21           1             0             1\n22           1             0             1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$treatment\n[1] \"contr.treatment\"\n\n\n\n# 5.\nsummary(z)\n\n\nCall:\nlm(formula = shift ~ treatment, data = x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.27857 -0.36125  0.03857  0.61147  1.06571 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   -0.30875    0.24888  -1.241  0.22988   \ntreatmentknee -0.02696    0.36433  -0.074  0.94178   \ntreatmenteyes -1.24268    0.36433  -3.411  0.00293 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7039 on 19 degrees of freedom\nMultiple R-squared:  0.4342,    Adjusted R-squared:  0.3746 \nF-statistic: 7.289 on 2 and 19 DF,  p-value: 0.004472\n\n\n\n# 7.\nconfint(z)\n\n                   2.5 %     97.5 %\n(Intercept)   -0.8296694  0.2121694\ntreatmentknee -0.7895122  0.7355836\ntreatmenteyes -2.0052265 -0.4801306\n\n\n\n# 8.\nanova(z)\n\nAnalysis of Variance Table\n\nResponse: shift\n          Df Sum Sq Mean Sq F value   Pr(>F)   \ntreatment  2 7.2245  3.6122  7.2894 0.004472 **\nResiduals 19 9.4153  0.4955                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# 9.\n# The ANOVA method assumes that the variance of the residuals is the same\n# in every group. The SE's and confidence intervals for means make use of\n# the mean squared error from the model fit, not just the values in the group.\n\nemmeans(z, \"treatment\")\n\n treatment emmean    SE df lower.CL upper.CL\n control   -0.309 0.249 19   -0.830    0.212\n knee      -0.336 0.266 19   -0.893    0.221\n eyes      -1.551 0.266 19   -2.108   -0.995\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "lab11-linear-mods.html#fly-sex-and-longevity",
    "href": "lab11-linear-mods.html#fly-sex-and-longevity",
    "title": "Lab 11 Linear models",
    "section": "6 Fly sex and longevity",
    "text": "6 Fly sex and longevity\n\nWe analyzed the data in fruitflies.csv previously in the graphics lab. Here we will analyze them further by fitting a linear model to the data.\nThe data are from L. Partridge and M. Farquhar (1981), Sexual activity and the lifespan of male fruit flies, Nature 294: 580-581. The experiment placed male fruit flies with varying numbers of previously-mated or virgin females to investigate whether mating activity affects male lifespan.\nThe linear model will have longevity as the response variable, and two explanatory variables: treatment (categorical) and thorax length (numerical; representing body size). The goal will be to compare differences in fly longevity among treatment groups, correcting for differences in thorax length. Correcting for thorax length will possibly improve the estimates of treatment effect. The method is also known as analysis of covariance, or ANCOVA.\n\nRead and examine data\n\n\nRead the data from the file.\n\n\n\nView the first few lines of data to make sure it was read correctly.\n\n\n\nDetermine whether the categorical variable “treatment” is a factor. If not a factor, convert treatment to a factor. This will be convenient when we fit the linear model.\n\n\n\nUse the levels() command on the factor variable “treatment” to see how R has ordered the different treatment groups (should be alphabetically).\n\n\n\nChange the order of the categories so that a sensible control group is first in the order of categories. Arrange the order of the remaining categories as you see fit.\n\n\n\nThis repeats an exercise from the graphics workshop. Create a scatter plot, with longevity as the response variable and body size (thorax length) as the explanatory variable. Use a single plot with different symbols (and colors too, if you like) for different treatment groups. Or make a multipanel plot using the lattice or ggplot2 package\n\n\nFit a linear model\n\n\nFit a linear model to the fly data, including both body size (thorax length) and treatment as explanatory variables. Place thorax length before treatment in the model formula. Leave out the interaction term for now – we’ll assume for now that there is no interaction between the explanatory variables thorax and treatment.\n\n\n\nUse plot() to check whether the assumptions of linear models are met in this case. Are there any potential concerns? If you have done the analysis correctly, you will see that the variance of the residuals is not constant, but increases with increasing fitted values. This violates the linear model assumption of equal variance of residuals.\n\n\n\nAttempt to fix the problem identified in step (3) using a log-transformation of the response variable. Refit the model and reapply the graphical diagnostic tools to check assumptions. Any improvements? (To my eye the situation is improved but the issue has not gone away entirely.) Let’s continue anyway with the log-transformed analysis.\n\n\n\nVisualize the fit of the model to the data using the visreg package. Try two different possibilities. In the first, plot the fit of the response variable to thorax length separately for each treatment group. In the second, plot the fit of the data to treatment, conditioning on the value of the covariate (thorax length).\n\n\n\nObtain the parameter estimates and standard errors for the fitted model. Interpret the parameter estimates. What do they represent*? Which treatment group differs most from the control group?\n\n\n\nObtain 95% confidence intervals for the treatment and slope parameters.\n\n\n\nTest overall treatment effects with an ANOVA table. Interpret each significance test – what exactly is being tested?\n\n\n\nRefit the model to the data but this time reverse the order in which you entered the two explanatory variables in the model. Test the treatment effects with an ANOVA table. Why isn’t the table identical to the one from your analysis in (7)**?\n\n\n\nOur analysis so far has assumed that the regression slopes for different treatment groups are the same. Is this a valid assumption? We have the opportunity to investigate just how different the estimated slopes really are. To do this, fit a new linear model to the data, but this time include an interaction term between the explanatory variables.\n\n\n\nThe parameters will be more complicated to interpret in the model including an interaction term, so lets skip this step. Instead, go right to the ANOVA table to test the interaction term using the new model fit. Interpret the result. Does it mean that the interaction term really is zero?\n\n\n\nAnother way to help assess whether the assumption of no interaction is a sensible one for these data is to determine whether the fit of the model is “better” when an interaction term is present or not, and by how much. We will learn new methods later in the course to determine this, but in the meantime a simple measure of model fit can be obtained using the adjusted \\(R^2\\) value. The ordinary \\(R^2\\) measures the fraction of the total variation in the response variable that is “explained” by the explanatory variables.\n\nThis, however, cannot be compared between models that differ in the number of parameters because fitting more parameters always results in a larger \\(R^2\\), even if the added variables are just made-up random numbers. To compare the fit of models having different parameters, use the adjusted \\(R^2\\) value instead, which takes account of the number of parameters being fitted. Use the summary() command on each of two fitted models, one with and the other without an interaction term, and compare their adjusted \\(R^2\\) values. Are they much different? If not, then maybe it is OK to assume that any interaction term is likely small and can be left out.\n\n*In a linear model with a factor and a continuous covariate, and no interaction term, the coefficient for the covariate is the common regression slope. The “intercept” coefficient represents the y-intercept of the first category (first level in the order of levels) of the treatment variable. Remaining coefficients represent the difference in intercept between that of each treatment category and the first category.\n**R fits model terms sequentially when testing. Change the order of the terms in the formula for the linear model and the results might change.\n\n\n6.1 Suggested solutions\n\nRead and examine the data.\n\n# 1.\nfly <- read.csv(\"data/fruitflies.csv\", \n                stringsAsFactors = FALSE)\n\n# 2.\nhead(fly)\n\n  Npartners          treatment longevity.days thorax.mm\n1         8 8 pregnant females             35      0.64\n2         8 8 pregnant females             37      0.68\n3         8 8 pregnant females             49      0.68\n4         8 8 pregnant females             46      0.72\n5         8 8 pregnant females             63      0.72\n6         8 8 pregnant females             39      0.76\n\n# 3.\nis.factor(fly$treatment)\n\n[1] FALSE\n\nfly$treatment <- factor(fly$treatment)\n\n# 4.\nlevels(fly$treatment)\n\n[1] \"1 pregnant female\"  \"1 virgin female\"    \"8 pregnant females\"\n[4] \"8 virgin females\"   \"no females added\"  \n\n# 5.\n# Put the controls first\nfly$treatment <- factor(fly$treatment, levels=c(\"no females added\", \n        \"1 virgin female\", \"1 pregnant female\", \"8 virgin females\",\n        \"8 pregnant females\"))\n\n# 6.\n# See earlier lab for overlaid graph.\n# Here's a multipanel plot\nggplot(fly, aes(thorax.mm, longevity.days)) +\n  geom_point(col = \"blue\", size = 2) +\n  geom_smooth(\n    method = lm,\n    size = I(1),\n    se = FALSE,\n    col = \"firebrick\"\n  ) +\n  xlab(\"Thorax (mm)\") +\n  ylab(\"Longevity (days)\") +\n  facet_wrap( ~ treatment, ncol = 2) +\n  theme_classic()\n\n\n\n\n\nFit a linear model\n\n# 1.\nz <- lm(longevity.days ~ thorax.mm + treatment, data = fly)\n\n# 2.\npar(mfrow = c(2,2))\nplot(z)\n\n\n\npar(mfrow = c(1,1))\n\n\n\n# 3.\n\n# log model\nfly$log.longevity<-log(fly$longevity)\n\nz <- lm(log.longevity ~ thorax.mm + treatment, data = fly)\n\npar(mfrow = c(2,2))\nplot(z)\n\n\n\npar(mfrow = c(1,1))\n\n\n\n# 4.\n# Conditional plot\nvisreg(z, xvar = \"thorax.mm\", type = \"conditional\", \n       points.par = list(cex = 1.1, col = \"blue\"))\n\n\n\n\n\n\n# Separate plots in separate panels\nvisreg(z, xvar = \"thorax.mm\", by = \"treatment\", whitespace = 0.4, \n    points.par = list(cex = 1.1, col = \"blue\"))\n\n\n\n\n\n\n# Separate plots overlaid\nvisreg(z, xvar = \"thorax.mm\", by = \"treatment\", overlay = TRUE, \n    band = FALSE, points.par = list(cex = 1.1), legend = F)\n\n\n\n\n\n\n# 5. \nsummary(z)\n\n\nCall:\nlm(formula = log.longevity ~ thorax.mm + treatment, data = fly)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52208 -0.13457 -0.00799  0.13807  0.39234 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  1.82123    0.19442   9.368 5.89e-16 ***\nthorax.mm                    2.74895    0.22795  12.060  < 2e-16 ***\ntreatment1 virgin female    -0.12391    0.05448  -2.275   0.0247 *  \ntreatment1 pregnant female   0.05203    0.05453   0.954   0.3419    \ntreatment8 virgin females   -0.41826    0.05509  -7.592 7.79e-12 ***\ntreatment8 pregnant females  0.08401    0.05491   1.530   0.1287    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1926 on 119 degrees of freedom\nMultiple R-squared:  0.7055,    Adjusted R-squared:  0.6932 \nF-statistic: 57.02 on 5 and 119 DF,  p-value: < 2.2e-16\n\n\n\n# 6.\nconfint(z)\n\n                                  2.5 %      97.5 %\n(Intercept)                  1.43626358  2.20619388\nthorax.mm                    2.29759134  3.20030370\ntreatment1 virgin female    -0.23177841 -0.01604413\ntreatment1 pregnant female  -0.05593661  0.15999702\ntreatment8 virgin females   -0.52734420 -0.30918075\ntreatment8 pregnant females -0.02472886  0.19273902\n\n\n\n# 7.\nanova(z)\n\nAnalysis of Variance Table\n\nResponse: log.longevity\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nthorax.mm   1 6.4256  6.4256  173.23 < 2.2e-16 ***\ntreatment   4 4.1499  1.0375   27.97 2.231e-16 ***\nResiduals 119 4.4141  0.0371                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# 8.\nz1 <- lm(log.longevity ~ treatment + thorax.mm, data = fly)\nanova(z1)\n\nAnalysis of Variance Table\n\nResponse: log.longevity\n           Df Sum Sq Mean Sq F value    Pr(>F)    \ntreatment   4 5.1809  1.2952  34.918 < 2.2e-16 ***\nthorax.mm   1 5.3946  5.3946 145.435 < 2.2e-16 ***\nResiduals 119 4.4141  0.0371                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# 9.\nz <- lm(log.longevity ~ thorax.mm * treatment, data = fly)\n\n# 10.\nanova(z)\n\nAnalysis of Variance Table\n\nResponse: log.longevity\n                     Df Sum Sq Mean Sq  F value Pr(>F)    \nthorax.mm             1 6.4256  6.4256 176.4955 <2e-16 ***\ntreatment             4 4.1499  1.0375  28.4970 <2e-16 ***\nthorax.mm:treatment   4 0.2273  0.0568   1.5611 0.1894    \nResiduals           115 4.1868  0.0364                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lab11-linear-mods.html#harper-adams-data-science",
    "href": "lab11-linear-mods.html#harper-adams-data-science",
    "title": "Lab 11 Linear models",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab12-mixed-effects.html#objectives",
    "href": "lab12-mixed-effects.html#objectives",
    "title": "Lab 12 Mixed effects",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nLoad and examine data for several mixed effects experimental designs\nFit a mixed effects models to data\nExaluate the outcome of using random effects in linear models\nPractice graphing paired and mixed effects data\nEvaluate statistical results for linear mixed effects models\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab12-mixed-effects.html#start-a-script",
    "href": "lab12-mixed-effects.html#start-a-script",
    "title": "Lab 12 Mixed effects",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab12-mixed-effects.html#linear-mixed-effects-models",
    "href": "lab12-mixed-effects.html#linear-mixed-effects-models",
    "title": "Lab 12 Mixed effects",
    "section": "3 Linear mixed-effects models",
    "text": "3 Linear mixed-effects models\nIn this lab we will fit linear mixed-effects models to data in R. Linear mixed-effects models are used when you have random effects, which occurs when multiple measurements are made on randomly sampled units. The measurements from these sampled units are not independent random samples on their own. Instead, the units or groups are assumed to be randomly sampled from a larger “population” of groups. Example situations include:\n\n\nWhen you divide up plots and apply separate treatments to the parts (plot is the random group)\nWhen your sampling design is nested, such as quadrats within transects; transects within woodlots; woodlots within districts (transects, woodlots, and districts are all random groups)\nWhen you take measurements on related individuals (family is the random group)\nWhen you measure subjects repeatedly over time (subject is the random group)\n\n\nLinear models for mixed effects are implemented in the R command lme4 and lmerTest package (lmerTest includes lme4 plus additional functions). An alternative option is to use the lme method in the nmle package. The methods used to calculate approximate degrees of freedom in lme4 are a bit more accurate than those used in the nmle package, especially when sample size is not large.\n\nTo begin using methods for fitting linear mixed-effects models, install (if you have not already done so) and load the {lmerTest} package."
  },
  {
    "objectID": "lab12-mixed-effects.html#repeatability-of-a-sexual-signal-trait",
    "href": "lab12-mixed-effects.html#repeatability-of-a-sexual-signal-trait",
    "title": "Lab 12 Mixed effects",
    "section": "4 Repeatability of a sexual signal trait",
    "text": "4 Repeatability of a sexual signal trait\nThis data set was extracted from a paper by Griffith and Sheldon (2001, Animal Behaviour 61: 987–993), who measured the white forehead patch of 30 male collared flycatchers in two years on the Swedish island of Gotland. The patch is important in mate attraction, but varies in size from year to year. Our goal here will be to estimate the repeatability of patch length (mm). The data are in the file \"flycatcher.csv\".\n\n\n4.1 Read and examine the data\n\n\nRead the data from the file.\nView the first few lines of data to make sure it was read correctly.\nCreate a plot showing the pair of measurements for each individual flycatcher in the two years of study. You can try to make the kind of dot plot I showed in lecture. Is there evidence of measurement variability between years?\n\n\n\n\n4.2 Fit a linear mixed-effects model\n\n\nFit a linear mixed-effects model to the data, treating the individual birds as the random groups. Note: The two measurements on each bird were taken in successive years of the study. For simplicity here, do not include year in the model. (Okay, if you really want to try including year in the model, go ahead. Just make sure to convert it to a character or factor in R so it is not treated as a numeric variable. Recalculate repeatability with this model as described in steps (2) and (3) below. How is the interpretation of repeatability changed?)\nExtract parameter estimates (coefficients) from the saved lmer() object (the command is the same one we used with lm() to get the coefficients table). Inspect the output for the random effects. What are the two sources of random variation? What does the fixed effect refer to?\nIn the output, examine the standard deviations for the random effects. There should be two standard deviations: one for “(Intercept)” and one for “Residual”. This is because the mixed effects model has two sources of random variation: variation among repeat measurements within birds, and true variation among birds in their patch lengths. Which of these two sources corresponds to “(Intercept)” and which to “Residual”?\nAlso examine the output for the fixed effect results. The only fixed effect in the model formula is the grand mean of all the patch length measurements. It is called “(Intercept)”, but don’t confuse with the intercept for the random effects. The fixed effect output gives you the estimate of the grand mean and a standard error for that estimate. Notice how the fixed effect output provides estimates of means, whereas the random effects output provides estimates of variances (or standard deviations).\nExtract the variance components from the fitted model and estimate the repeatability of patch length from year to year*.\nInterpret the measure of repeatability obtained in the previous step. If the repeatability you obtained is less than 1.0, what is the source of the variation among measurements within individuals. Is it measurement error alone?\nProduce a plot of residuals against fitted values. Notice anything odd? There sees to be a slightly positive trend. This isn’t a mistake, but results from “shrinkage” of the best linear unbiased predictors (BLUPs). Consult the lecture material for information on what is happening.\n\n\n* 0.776.\n\n\n\n4.2 Suggested solutions\nRead and examine the data.\n\n# Load libraries\nlibrary(visreg, warn.conflicts=FALSE)\nlibrary(ggplot2, warn.conflicts=FALSE)\nlibrary(emmeans, warn.conflicts=FALSE)\nlibrary(lmerTest, warn.conflicts=FALSE)\n\n\n# Load your data\nflycat <- read.csv(\"data/flycatcher.csv\", \n              stringsAsFactors = FALSE)\n              \nhead(flycat)\n\n  bird patch year\n1    1  10.5 1998\n2    2  10.6 1998\n3    3   8.7 1998\n4    4   8.6 1998\n5    5   9.0 1998\n6    6   9.3 1998\n\n\n\n# These commands produce a type of \"dot plot\"\n# This is useful to compare variation AMONGST individuals\n# and WITHIN individuals for Patch length\n\nstripchart(patch ~ bird, data = flycat, vertical = TRUE, pch = 16, \n    cex = 1.5, las = 2, col = \"blue\",\n    xlab = \"Individual bird\", ylab = \"Patch length (mm)\")\n\n\n\n\n\n\n# A different way to display paired data is with an interaction plot for pairs\n# A base R version\n\ninteraction.plot(response = flycat$patch, \n                  x.factor = flycat$year, \n                  trace.factor = flycat$bird, \n                  legend = FALSE, lty = 1, col = \"blue\", \n                  xlab = \"Year\", \n                  ylab = \"Patch length (mm)\", \n                  type = \"b\", pch = 16, las = 1, cex = 1.0)\n\n\n\n\n\n\n# A ggplot version\nggplot(flycat, aes(y = patch, x = factor(year))) +  \n    geom_point(size = 4, col = \"blue\", alpha = 0.5) + \n    geom_line(aes(group = bird)) +\n    labs(x = \"Year\", y = \"Patch length (mm)\") + \n    theme_classic()\n\n\n\n\n\nFit a linear mixed effects model. The output from summary() used on a linear model object will show two sources of random variation: variation among individual birds (bird Intercepts), and variation among the repeat measurements made on the same birds (Residuals). An estimated variance and standard deviation are given for each source. The fixed effect is just the mean of all birds — another “Intercept”.\n\n\n# 1. Mixed effects model\n# (1|bird) identifies bird as the random effects variable\nz <- lmer(patch ~ 1 + (1|bird), data = flycat)\n\n# 2. Parameter estimates\nsummary(z)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: patch ~ 1 + (1 | bird)\n   Data: flycat\n\nREML criterion at convergence: 171\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.62368 -0.58351  0.03328  0.51009  1.67263 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n bird     (Intercept) 1.243    1.1150  \n Residual             0.358    0.5983  \nNumber of obs: 60, groups:  bird, 30\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)   7.5100     0.2177 29.0000   34.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# 5. Variance components and repeatability\nVarCorr(z)\n\n Groups   Name        Std.Dev.\n bird     (Intercept) 1.11504 \n Residual             0.59833 \n\n\n\n# Repeatability\n1.11504^2/(1.11504^2 + 0.59833^2)\n\n[1] 0.7764342\n\n\n\n# 7. Plot of residuals against fitted values\nplot(z, col=\"blue\")"
  },
  {
    "objectID": "lab12-mixed-effects.html#goldies-vision",
    "href": "lab12-mixed-effects.html#goldies-vision",
    "title": "Lab 12 Mixed effects",
    "section": "5 Goldie’s vision",
    "text": "5 Goldie’s vision\nCronly-Dillon and Muntz (1965; J. Exp. Biol 42: 481-493) used the optomotor response to measure color vision in the goldfish. Here we will fit a model to the data and include the full set of wavelengths tested. Each of 5 fish was tested at all wavelengths in random order. A large value of sensitivity indicates that the fish can detect a low light intensity.\nAn important feature of the optomotor response is that fish don’t habituate, and it is unlikely that a measurement of visual sensitivity under one wavelength would carry over and have an effect on later measurements at another wavelength. The data are in the file goldfish.csv.\n\n\n5.1 Read and examine the data\n\n\nRead the data from the file, and view the first few lines to make sure it was read correctly.\nUse an interaction plot to compare the responses of individual fish across the different light wavelength treatments.\nWhat type of experimental design was used?* This will determine the linear mixed model to use when fitting the data.\n\n\n\n\n5.2 Fit a linear mixed-effects model\n\n\nFit a linear mixed-effects model to the data. This will work using lmer(), but R will give you the message: “boundary (singular) fit”. The reason will become clearer below. Meantime, proceed as though all is well.\nPlot the fitted (predicted) values**. The difference between the predicted and observed values for each fish represent the residuals.\nWhat assumptions are you making in (1)? Create a plot of residuals against fitted values to check one of these assumptions.\nExtract parameter estimates from the saved lmer() object. Inspect the results for the fixed effects. The coefficients given have the same interpretation as in the case of a categorical variable analyzed using lm() (arbitrarily, the light treatment “nm426” is set as the “control”).\nInspect the output for the random effects. Once again we have two sources of random error in our mixed effects model. What are they? Which of them corresponds to the (Intercept) and which to the Residual in the output? Notice that the estimated standard deviation for one of the sources of variation is very small in this data set. This is the reason behind the “boundary (singular) fit” message. It is unlikely that the variance among fish really is zero, but this data set is very small and low variance estimates can occur because of sampling error.\nGenerate the model-based estimates of the mean sensitivities for each wavelength.\nAre the differences among wavelengths significant? Generate the ANOVA table for the lmer() object. What effects are tested here, the random effects or the fixed effects?*** Interpret the ANOVA results.\n\n\n*It is a “subjects-by-treatment” repeated measures design, since each fish is measured once under each treatment. It is essentially the same as a randomized complete block design (think of the individual fish as “blocks”).\n**visreg() is preferred, because both the data and the fitted values are plotted. Note how the predicted values are very similar between fish. This indicates that there was very little estimated variance among individual fish in this study.\n***Generally, only the fixed effects are tested in an ANOVA table. It is possible to test the null hypothesis of no variance in a random effect using lmerTest, but I’ve yet to think of a compelling reason why one would ever do this.\n\n\n\n5.3 Suggested solutions\nAll lines below beginning with double hashes are R output.\nRead and examine the data.\n\nx <- read.csv(\"data/goldfish.csv\", \n        stringsAsFactors = FALSE)\nhead(x)\n\n   fish wavelength sensitivity\n1 fish1      nm426        0.94\n2 fish2      nm426        0.94\n3 fish3      nm426        0.94\n4 fish4      nm426        1.14\n5 fish5      nm426        0.94\n6 fish1      nm462        1.09\n\n\n\nFit a linear mixed effects model\nThe model assumes normally distributed residuals with equal variance for all fitted values. The method also assumes that the random intercepts among individual fish are normally distributed. The method also assumes a random sample of groups (fish) and no carry-over between measurements made on the same fish.\n\n\n# 1. Fit mixed effects model.\nz <- lmer(sensitivity ~ wavelength + (1|fish), data = x)\n\n\n# 2. This plots the fitted values separately for each fish.\n# Is the response similar for each fish across wavelengths?\nvisreg(z, xvar = \"wavelength\", by = \"fish\",\n      scales = list(rot = 90))\n\n\n\n\n\n\n# 3. Test assumptions\nplot(z)\n\n\n\n\n\n\n# 4. Extract parameter estimates\n# There will be lots of results for this summary\nsummary(z)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: sensitivity ~ wavelength + (1 | fish)\n   Data: x\n\nREML criterion at convergence: -32.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5811 -0.3162  0.0000  0.3162  1.8974 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n fish     (Intercept) 0.000    0.0000  \n Residual             0.016    0.1265  \nNumber of obs: 45, groups:  fish, 5\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)      0.98000    0.05657 36.00000  17.324  < 2e-16 ***\nwavelengthnm462  0.15000    0.08000 36.00000   1.875 0.068923 .  \nwavelengthnm494 -0.11000    0.08000 36.00000  -1.375 0.177629    \nwavelengthnm528  0.06000    0.08000 36.00000   0.750 0.458128    \nwavelengthnm585  0.29000    0.08000 36.00000   3.625 0.000886 ***\nwavelengthnm615  0.71000    0.08000 36.00000   8.875 1.36e-10 ***\nwavelengthnm634  0.70000    0.08000 36.00000   8.750 1.94e-10 ***\nwavelengthnm670 -0.39000    0.08000 36.00000  -4.875 2.20e-05 ***\nwavelengthnm700 -0.82000    0.08000 36.00000 -10.250 3.20e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) wvl462 wvl494 wvl528 wvl585 wvl615 wvl634 wvl670\nwvlngthn462 -0.707                                                 \nwvlngthn494 -0.707  0.500                                          \nwvlngthn528 -0.707  0.500  0.500                                   \nwvlngthn585 -0.707  0.500  0.500  0.500                            \nwvlngthn615 -0.707  0.500  0.500  0.500  0.500                     \nwvlngthn634 -0.707  0.500  0.500  0.500  0.500  0.500              \nwvlngthn670 -0.707  0.500  0.500  0.500  0.500  0.500  0.500       \nwvlngthn700 -0.707  0.500  0.500  0.500  0.500  0.500  0.500  0.500\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see ?isSingular\n\n\n\n# 6. Model-based estimates of the mean sensitivities \nemmeans(z, \"wavelength\", data = x)\n\n wavelength emmean     SE df lower.CL upper.CL\n nm426        0.98 0.0566 36   0.8653    1.095\n nm462        1.13 0.0566 36   1.0153    1.245\n nm494        0.87 0.0566 36   0.7553    0.985\n nm528        1.04 0.0566 36   0.9253    1.155\n nm585        1.27 0.0566 36   1.1553    1.385\n nm615        1.69 0.0566 36   1.5753    1.805\n nm634        1.68 0.0566 36   1.5653    1.795\n nm670        0.59 0.0566 36   0.4753    0.705\n nm700        0.16 0.0566 36   0.0453    0.275\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n# 7. ANOVA\nanova(z)\n\nType III Analysis of Variance Table with Satterthwaite's method\n           Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nwavelength 9.5111  1.1889     8    36  74.306 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lab12-mixed-effects.html#yukon-yarrow",
    "href": "lab12-mixed-effects.html#yukon-yarrow",
    "title": "Lab 12 Mixed effects",
    "section": "6 Yukon yarrow",
    "text": "6 Yukon yarrow\n\nThe Kluane project experimentally investigated the effects of fertilization and herbivory on vegetation dynamics in the boreal forest ecosystem of Kluane National Park in the Yukon (Krebs, C.J., Boutin, S. & Boonstra, R., eds (2001a) Ecosystem dynamics of the Boreal Forest. The Kluane Project. Oxford University Press, New York). The data here are from a study of the effects of plant resources and herbivory on the defensive chemistry of understory plant species.\n\nEach of sixteen 5x5 m plots was randomly assigned one of four treatments:\n\nsurrounded by a fence exclosure to exclude herbivores;\nfertilized with N-P-K fertilizer;\nfenced and fertilized; and\nuntreated control.\n\n\nEach of the 16 plots was then divided in two. One side of each plot (randomly chosen) received the treatment continually over the 20 years of study. The other half of each plot received the treatment for the first ten years, after which it was left to revert to its untreated state.\nThe data to be analyzed here record the concentration of phenolics (a crude measure of plant defense compounds) in yarrow (Achillea millefolium), a herb common in the plots. The measurement units are mg Tannic Acid Equivalents per g dry weight. The data are in the file “kluane.csv”.\n\n\n6.1 Visualize the data\n\nRead the data from the file.\nInspect the first few lines of data. Plot and treatment are self-explanatory. Treatment is given as a single variable with four levels (let’s stick with this approach rather than model as two variables, enclosure and fertilizer, with a 2x2 factorial design). Duration indicates whether the half-plots received the treatment for the full 20 years or whether the treatment was stopped (“reversed”) after 10 years. The variable “phen.ach” is the concentration of phenolics in yarrow.\nDraw a graph to illustrate the concentrations of phenolics in yarrow in the different treatment and duration categories. There aren’t many data points in each combination of treatment and duration levels, so a strip chart by groups is probably a better choice than a box plot by groups.\nOptional challenge question (moderately difficult): A flaw with the strip chart method is that it doesn’t indicate that data points from the same plot (adjacent half-plots) are paired. Can you figure out how to add line segments to connect paired points?\n\n\n\n\n6.2 Fit a linear mixed-effects model\n\nWhat type of experimental design was used?* This will determine the linear mixed model to fit to the data.\nFit a linear mixed model to the data without an interaction between treatment and duration. Use the log of phenolics as the response variable, as the log-transformation improved the fit of the data to linear model assumptions.\nVisualize the model fit to the data. Use the by = argument with visreg() to separate panels by duration (if xvar is treatment) or treatment (if xvar is duration). visreg() won’t preserve the pairing, but will allow you to inspect residuals.\nNow repeat the model fitting, but this time include the interaction between treatment and duration. Visualize the model fit to the data. What is the most noticeable difference between the two model fits, one with the interaction and the other without? Describe what including the interaction term “allows” that the model without an interaction term does not. Judging by eye, which model appears to fit the data best?\nUse the diagnostic plot to check a key assumption of linear mixed models for the model including the interaction term.\nEstimate the parameters of the linear model (including interaction) using the fitted model object. Notice that there are now many coefficients in the table of fixed effects. In principle, these can be understood by examining how R models terms behind the scenes, but the task is made more challenging with two factors and an interaction. It might be more useful to use emmeans() instead to obtain model fitted means, which are more readily interpretable.\nIn the output from the previous step, you will see two quantities given for “Std.Dev” under the label “Random effects”. Explain what these quantities refer to.\nUse emmeans() to estimate the model fitted means for all combinations of the fixed effects.\nGenerate the ANOVA table for the fixed effects. Which terms were statistically significant?\nBy default, lmerTest will test model terms using Type 3 sums of squares (the “drop one” approach) rather than sequentially (Type 1), which is the default in lm(). Repeat the ANOVA table using Type 1 instead. Are the results any different?**\n\n\nThe experiment used a split-plot design, in which whole plots were randomly assigned different treatments, and then different levels of a second treatment (duration) were assigned to plot halves.\n\n** There should be no difference because the design is completely balanced.\n\n\n\n6.2 Suggested solutions\nRead and examine the data.\nA good strategy is to order the treatment categories to put controls first. This will make the output from the linear model fit more useful.\n\n# 1. read data\nx <- read.csv(\"data/kluane.csv\", \n        stringsAsFactors = FALSE)\n\n# 2. Inspect\nhead(x)\n\n  plot  treatment  duration phen.ach\n1    1    control permanent    43.97\n2    1    control   reverse    36.50\n3    2 fertilizer permanent    14.21\n4    2 fertilizer   reverse    28.64\n5    3    control permanent    42.38\n6    3    control   reverse    44.44\n\n# 3. Grouped strip chart\n# First, reorder treatment categories\nx$treatment <- factor(x$treatment,levels=c(\"control\",\"exclosure\",\"fertilizer\",\"both\"))\n\nggplot(data = x, \n      aes(y = log(phen.ach), \n          x = treatment, \n          fill = duration, \n          color = duration)) + \n    geom_point(size = 3, position = position_dodge(width = 0.7)) +\n    labs(x = \"Treatment\", y = \"log phenolics concentration\") +\n    theme_classic()\n\n\n\n\n\n\n# 4. (Bonus) One solution is to plot paired data separately in multiple panels\nggplot(data = x, \n        aes(y = log(phen.ach), \n        x = duration, \n        fill = duration, \n        color = duration)) + \n    geom_line(aes(group = plot), col = \"black\") +\n    geom_point(size = 3, position = position_dodge(width = 0.7), show.legend = FALSE) +\n    facet_wrap(~ treatment, nrow = 1) +\n    labs(x = \"Treatment\", y = \"log phenolics concentration\") +\n    theme(aspect.ratio = 2)+\n    theme_classic()\n\n\n\n\n\nFit a linear mixed effects model. The fixed effects are “treatment” and “duration”, whereas “plot” is the random effect. When fitting an interaction, the magnitudes of differences between treatment levels can differ between duration levels.\nBecause a random effect is also present (plot), the coefficients table will show estimates of variance for two sources of random variation. One is the variance of the residuals of the fitted model. The second is the variance among the (random) plot intercepts.\n\n\n# 2. Fit mixed effects model - no interaction\nz <- lmer(log(phen.ach) ~ treatment + duration + (1|plot), data=x)\n\n\n# 3. Visualize. Here, a two-panel plot is used.\nvisreg(z, xvar = \"treatment\", by = \"duration\", overlay = TRUE,\n      ylab=\"Log phenolics concentration\", data = x)\n\n\n\n\n\n\n# 4. Include interaction and visualize again\nz.int <- lmer(log(phen.ach) ~ treatment * duration + (1|plot), data=x)\n\nvisreg(z.int, xvar = \"treatment\", by = \"duration\", overlay = TRUE,\n      ylab=\"Log phenolics concentration\", data = x)\n\n\n\n\n\n\n# 5. Plot to test homogeneity of variances \n# Do you think the residuals are Gaussian?\nplot(z.int)\n\n\n\n\n\n\n# 6. Coefficients\nsummary(z.int)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(phen.ach) ~ treatment * duration + (1 | plot)\n   Data: x\n\nREML criterion at convergence: -1.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.55278 -0.58471  0.03425  0.52871  1.61347 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n plot     (Intercept) 0.01292  0.1137  \n Residual             0.02413  0.1553  \nNumber of obs: 32, groups:  plot, 16\n\nFixed effects:\n                                    Estimate Std. Error       df t value\n(Intercept)                          3.74031    0.09624 21.39871  38.865\ntreatmentexclosure                   0.12388    0.13610 21.39871   0.910\ntreatmentfertilizer                 -0.87526    0.13610 21.39871  -6.431\ntreatmentboth                       -0.74732    0.13610 21.39871  -5.491\ndurationreverse                      0.03190    0.10984 12.00000   0.290\ntreatmentexclosure:durationreverse  -0.01122    0.15534 12.00000  -0.072\ntreatmentfertilizer:durationreverse  0.40963    0.15534 12.00000   2.637\ntreatmentboth:durationreverse        0.63438    0.15534 12.00000   4.084\n                                    Pr(>|t|)    \n(Intercept)                          < 2e-16 ***\ntreatmentexclosure                   0.37284    \ntreatmentfertilizer                 2.06e-06 ***\ntreatmentboth                       1.78e-05 ***\ndurationreverse                      0.77648    \ntreatmentexclosure:durationreverse   0.94362    \ntreatmentfertilizer:durationreverse  0.02169 *  \ntreatmentboth:durationreverse        0.00152 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) trtmntx trtmntf trtmntb drtnrv trtmntx: trtmntf:\ntrtmntxclsr -0.707                                                 \ntrtmntfrtlz -0.707  0.500                                          \ntreatmntbth -0.707  0.500   0.500                                  \nduratinrvrs -0.571  0.404   0.404   0.404                          \ntrtmntxcls:  0.404 -0.571  -0.285  -0.285  -0.707                  \ntrtmntfrtl:  0.404 -0.285  -0.571  -0.285  -0.707  0.500           \ntrtmntbth:d  0.404 -0.285  -0.285  -0.571  -0.707  0.500    0.500  \n\n\n\n# 8. Model fitted means\nemmeans(z.int, specs = c(\"treatment\", \"duration\"), data = x)\n\n treatment  duration  emmean     SE   df lower.CL upper.CL\n control    permanent   3.74 0.0962 21.4     3.54     3.94\n exclosure  permanent   3.86 0.0962 21.4     3.66     4.06\n fertilizer permanent   2.87 0.0962 21.4     2.67     3.06\n both       permanent   2.99 0.0962 21.4     2.79     3.19\n control    reverse     3.77 0.0962 21.4     3.57     3.97\n exclosure  reverse     3.88 0.0962 21.4     3.68     4.08\n fertilizer reverse     3.31 0.0962 21.4     3.11     3.51\n both       reverse     3.66 0.0962 21.4     3.46     3.86\n\nDegrees-of-freedom method: kenward-roger \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n\n\n# 9. ANOVA tables\nanova(z.int) # Type 3 sums of squares is default in lmerTest\n\nType III Analysis of Variance Table with Satterthwaite's method\n                    Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \ntreatment          1.57355 0.52452     3    12 21.7365 3.848e-05 ***\nduration           0.67324 0.67324     1    12 27.8998  0.000194 ***\ntreatment:duration 0.60739 0.20246     3    12  8.3903  0.002822 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# 10. Type 1 instead\n# NB same as for type 3 due to balanced design!\nanova(z.int, type = 1)\n\nType I Analysis of Variance Table with Satterthwaite's method\n                    Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \ntreatment          1.57355 0.52452     3    12 21.7365 3.848e-05 ***\nduration           0.67324 0.67324     1    12 27.8998  0.000194 ***\ntreatment:duration 0.60739 0.20246     3    12  8.3903  0.002822 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lab12-mixed-effects.html#harper-adams-data-science",
    "href": "lab12-mixed-effects.html#harper-adams-data-science",
    "title": "Lab 12 Mixed effects",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab13-likelihood.html#objectives",
    "href": "lab13-likelihood.html#objectives",
    "title": "Lab 13 Likelihood",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nExplore the calculation of maximum likelihood\nPerform the log-likelihood ratio test\nEvaluate statistical results for likelihood test\nPractice graphical display of likelihood problems\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab13-likelihood.html#start-a-script",
    "href": "lab13-likelihood.html#start-a-script",
    "title": "Lab 13 Likelihood",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab13-likelihood.html#likelihood",
    "href": "lab13-likelihood.html#likelihood",
    "title": "Lab 13 Likelihood",
    "section": "3 Likelihood",
    "text": "3 Likelihood\nIn this lab we will use likelihood methods to estimate parameters and test hypotheses. Likelihood methods are especially useful when modeling data having a probability distribution other than the normal distribution (e.g., binomial, exponential, etc).\n\n\n3.1 Maximum likelihood estimate\nTo estimate a parameter, we treat the data as given and vary the parameter to find that value for which the probability of obtaining the data is highest. This value is the maximum likelihood estimate of the parameter. The likelihood function is also used to obtain a likelihood-based confidence interval for the parameter. This confidence interval is a large-sample approximation, and may be inaccurate for small sample size, depending on the probability distribution of the data.\n\n\n\n3.2 Log-likelihood ratio test\nThe log-likelihood ratio test can be used to compare the fits of two nested models to the same data. The “full” model fits the data using the maximum likelihood estimates for the parameter(s) of interest (for example, a proportion p). The “reduced” model constrains the parameter values to represent a null hypothesis (for example, that p = 0.5, or that p is equal between two treatments). The G statistic is calculated as twice the difference between the log-likelihoods of the two models (“full” minus “reduced”):\n\nG <- 2 *(loglikefull - loglikereduced)\n\nG is referred to as the deviance. Under the null hypothesis, G has an approximate χ2 distribution with degrees of freedom equal to the difference between the “full” and “reduced” models in the number of parameters estimated from data. We’ll work through an example below.\n\n\n\n3.3 Warmup\nWe’ll start by getting familiar with the commands in R to calculate probabilities.\n\n\nThe probability of heads in a coin toss is 0.5. If you flip a coin 10 times, what is the probability of obtaining exactly 5 heads and 5 tails?\nThe fraction of human babies born who are boys is about 0.512. If 20 newborn babies are randomly sampled, what is the probability that exactly 10 are boys?\nPlot the entire probability distribution for the number of boys in families having six children. Assume the probability that any one child is a boy is 0.512.\nIf mortality is independent of age, then the probability of surviving X years after birth, and then dying in the X + 1st year, will follow a geometric distribution (geom()). X is any integer from 0 to infinity. If the probability of dying in any given year is 0.1, what fraction of individuals are expected to survive 10 years and then die in their 11th year?*\nRefer to the previous question. If the probability of death in any give year is 0.1, what fraction of individuals die before they reach their 6th birthday?**\nIn an environment where prey are randomly distributed, the search time between discovered prey items will follow an exponential distribution. Imagine an environment in which the mean search time between prey items is 0.5 hours. What is the probability density corresponding to a search time of 2 hours?***\nRefer to the previous problem. Create a line plot of the exponential probability curve over most the range of possible values for search time between items (e.g., over the range of values between 0 and 5 hours).\n\n*0.03487\n**0.46856\n***0.03663\n\n\n\n3.4 Suggested solutions\n\n# 1.\ndbinom(5, size=10, p=0.5)\n\n[1] 0.2460938\n\n\n\n# 2.\ndbinom(10, size=20, p=0.512)\n\n[1] 0.1751848\n\n\n\n# 3.\nz <- dbinom(0:6, size=6, p=0.512)\nnames(z) <- as.character(0:6)\n\nbarplot(z, space=0, \n        ylab=\"Probability\", \n        xlab = \"Number of boys\",\n        col = \"goldenrod\", las = 1)\n\n\n\n\n\n\n# 4.\ndgeom(10, 0.1)\n\n[1] 0.03486784\n\n\n\n# 5.\nsum(dgeom(0:5, 0.1))\n\n[1] 0.468559\n\n\n\n# 6.\ndexp(2, rate=1/0.5)\n\n[1] 0.03663128\n\n\n\n# 7.\n# Make the x-axis scale values\nx <- seq(0, 5, by = 0.1)\ny <- dexp(x, rate = 1/0.5)\n\nplot(y ~ x, type = \"l\", \n      col = \"blue\", lwd = 2) # mere vanity"
  },
  {
    "objectID": "lab13-likelihood.html#example-left-handed-flowers",
    "href": "lab13-likelihood.html#example-left-handed-flowers",
    "title": "Lab 13 Likelihood",
    "section": "4 Example: Left-handed flowers",
    "text": "4 Example: Left-handed flowers\nIndividuals of most plant species are hermaphrodites (with both male and female sexual organs) and are therefore prone to inbreeding of the worst sort: having sex with themselves. The mud plantain, Heteranthera multiflora (a species of water hyacinth), has a simple mechanism to avoid such “selfing.” The style deflects to the left in some individuals and to the right in others. The anther is on the opposite side. Bees visiting a left-handed plant are dusted with pollen on their right side, which then is deposited on the styles of only right-handed plants visited later.\nTo investigate the genetics of this variation, Jesson and Barrett (2002, Proc. Roy. Soc. Lond., Ser. B, Biol. Sci. 269: 1835-1839) crossed pure strains of left- and right-handed flowers, yielding only right-handed F1 offspring, which were then crossed with one another. Six of the resulting F2 offspring were left-handed, and 21 were right-handed. The expectation under a simple model of inheritance would be that their F2 offspring should consist of left- and right-handed individuals in a 1:3 ratio (i.e., 1/4 of the plants should be left-handed), assuming simple genetic assortment of 2 alleles for 1 locus. We will explore this hypothesis.\n\n\nGenerate a vector that includes a range of possible values for the population proportion of left-handed flowers, p, from 0.01 to 0.99 in increments of 0.01.\nGiven the results above, calculate the log-likelihood of each value for p in the F2 generation.\nCreate a line plot of the log-likelihood against the range of values for p. What is the resulting curve called? Can you see approximately the value of p corresponding to the highest point of the curve? What is this value called?\nTo get closer to this value, repeat steps (1) to (3) using a narrower range of values for p surrounding the highest point in the curve and an additional decimal point.\nUse your results to determine the maximum likelihood estimate of the proportion of left-handed F2 flowers.\nProvide a likelihood-based 95% confidence interval for the population proportion.*\n(Challenge) Use the bbmle package to find the maximum likelihood estimate and 95% confidence interval for the proportion of left-handed flowers. How do the results compare with your calculations?\nWe can compare the fits of two models to these same data, to test the null hypothesis that the proportion of left-handed flowers in the cross is 1/4 (i.e., the proportion predicted by the simplest genetic model for flower handedness). To begin, obtain the log-likelihood corresponding to the maximum likelihood estimate of the proportion of left-handed flowers. This represents the fit of the “full” model to the data. This model estimated one parameter from the data (p, estimated using maximum likelihood).\nNow obtain the log-likelihood of the value for p specified by the null hypothesis. This represents the fit of the “reduced” model to the data. This reduced model estimated zero parameters from the data (instead, p was specified by the null hypothesis).\nCalculate the G statistic for the log-likelihood ratio test**. To obtain a P-value for the test, calculate the tail probability from the χ2 distribution as follows,\n\n1 - pchisq(G, df)\nwhere df is the degrees of freedom, calculated as the difference between the two models in the number of parameters estimated from the data.\n\n\n(Challenge) How similar is the result from your log-likelihood ratio test to that from an ordinary χ2 goodness of fit test? Analyze the same data using the chisq.test() command in R and comment on the outcome.\n\n* 0.094 < p < 0.400\n** 0.114\n\n\n4.1 Suggested solutions\n\n# 1. Vector\n(p <- seq(0.01, 0.99, by = 0.01))\n\n [1] 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15\n[16] 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30\n[31] 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44 0.45\n[46] 0.46 0.47 0.48 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.60\n[61] 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74 0.75\n[76] 0.76 0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.90\n[91] 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99\n\n# 2. The log-likelihoods\n(loglike <- dbinom(6, size = 27, prob = p, log = TRUE))\n\n [1] -15.243930 -11.298246  -9.080842  -7.572368  -6.453404  -5.581699\n [7]  -4.881396  -4.307237  -3.830049  -3.429933  -3.092711  -2.807934\n[13]  -2.567680  -2.365809  -2.197469  -2.058761  -1.946514  -1.858112\n[19]  -1.791380  -1.744494  -1.715907  -1.704306  -1.708567  -1.727723\n[25]  -1.760941  -1.807500  -1.866777  -1.938231  -2.021394  -2.115862\n[31]  -2.221287  -2.337369  -2.463856  -2.600534  -2.747225  -2.903788\n[37]  -3.070110  -3.246107  -3.431725  -3.626934  -3.831728  -4.046126\n[43]  -4.270169  -4.503923  -4.747475  -5.000933  -5.264431  -5.538122\n[49]  -5.822186  -6.116825  -6.422266  -6.738763  -7.066595  -7.406073\n[55]  -7.757535  -8.121354  -8.497936  -8.887726  -9.291208  -9.708911\n[61] -10.141409 -10.589331 -11.053362 -11.534250 -12.032814 -12.549947\n[67] -13.086632 -13.643946 -14.223076 -14.825330 -15.452155 -16.105155\n[73] -16.786116 -17.497029 -18.240126 -19.017916 -19.833235 -20.689302\n[79] -21.589788 -22.538909 -23.541533 -24.603324 -25.730923 -26.932183\n[85] -28.216485 -29.595159 -31.082061 -32.694386 -34.453828 -36.388302\n[91] -38.534573 -40.942443 -43.681736 -46.854729 -50.619989 -55.243176\n[97] -61.222323 -69.675551 -84.170727\n\n# 3. Log-likelihood curve, showing maximum likelihood estimate\nplot(loglike ~ p, \n      xlab=\"Population proportion, p\", \n      ylab=\"Log-likelihood\", \n      type=\"l\",\n      col = \"blue\", lwd = 2) # Mere vanity\n\n\n\n\n\n\n# 4. Narrower range of values for p\np <- seq(0.05, 0.5, by = 0.001)\nloglike <- dbinom(6, size = 27, prob = p, log = TRUE)\n\nplot(loglike ~ p, \n      xlab=\"Population proportion, p\", \n      ylab=\"Log-likelihood\", \n      type=\"l\",\n      col = \"blue\", lwd = 2) # Mere vanity\n\n\n\n\n\n\n# 5. Maximum likelihood estimate\nphat <- p[loglike == max(loglike)]\nphat\n\n[1] 0.222\n\n\n\n# 6.\n# 1.92-unit support limits. \n# This first method gives an interval slightly narrower than the real values\nrange(p[loglike >= (max(loglike) - 1.92)])\n\n[1] 0.095 0.399\n\n# To be conservative, take outer edge of this interval\nmax(p[loglike < (max(loglike) - 1.92) & p < 0.222])\n\n[1] 0.094\n\nmin(p[loglike < (max(loglike) - 1.92) & p > 0.222])\n\n[1] 0.4\n\n\n\n# 7.\n# load bblme, install if necessary\nif(!require(\"bbmle\")) install.packages(\"bbmle\")\nlibrary(bbmle)\n\npNegLogLike <- function(p){-dbinom(6, size=27, p, log=TRUE)}\n\n# It is safe to ignore any warnings for now\nz <- mle2(pNegLogLike, start=list(p=0.5))\n\nsummary(z)\n\nMaximum likelihood estimation\n\nCall:\nmle2(minuslogl = pNegLogLike, start = list(p = 0.5))\n\nCoefficients:\n  Estimate Std. Error z value    Pr(z)   \np 0.222223   0.080009  2.7775 0.005479 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n-2 log L: 3.407838 \n\n\n\npProfile <- profile(z)\nconfint(pProfile, method=\"uniroot\")\n\n     2.5 %     97.5 % \n0.09494227 0.39989553 \n\n\n\n# 8. Log likelihood full model\n\nLLfull <- max(loglike)\nLLfull\n\n[1] -1.703923\n\n\n\n# 9. Log likelihood reduced model\nLLreduced <- loglike[p == 0.25]\nLLreduced\n\n[1] -1.760941\n\n\n\n# 10. Log likelihood ratio statistic\nG <- 2 * (LLfull - LLreduced)\nG\n\n[1] 0.1140369\n\n1 - pchisq(G, 1)\n\n[1] 0.7355942\n\n\n\n# 11. Using chisq.test\nchisq.test(c(6, 21), p = c(0.25, 0.75))\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(6, 21)\nX-squared = 0.11111, df = 1, p-value = 0.7389"
  },
  {
    "objectID": "lab13-likelihood.html#example-counting-elephants",
    "href": "lab13-likelihood.html#example-counting-elephants",
    "title": "Lab 13 Likelihood",
    "section": "5 Example Counting elephants",
    "text": "5 Example Counting elephants\nMark-recapture is a method used to estimate population size when it is difficult to measure directly, which is based on random sampling and other certain assumptions. Even counting elephants in the wild is more challenging than you might think, at least when they live in dense forest and feed at night.\nEggert et al. (2003. Molecular Ecology 12: 1389-1402) used mark-recapture methods to estimate the total number of forest elephants inhabiting Kakum National Park in Ghana without having to see a single one. They spent about two weeks in the park collecting elephant dung, from which they extracted elephant DNA. Using five genetic markers, they generated a unique DNA fingerprint for every elephant encountered in this way. Over the first seven days of collecting they identified 27 elephant individuals. Refer to these 27 elephants as marked. Over the next eight days they sampled 74 individuals, of which 15 had been previously marked. Refer to these 15 elephants as recaptured. We would like to use these numbers to estimate the total number of elephants in the park.\nWe make the following assumptions,\n\nThere were no births, deaths, immigrants, or emigrants while the study was being carried out, and\nThe dung of every elephant, marked or unmarked, regardless of its output, had an equal chance of being sampled, and selection of each individual was independent of the others.\n\n\nIf these assumptions are true, the number of recaptured (i.e., previously marked) individuals X in the second sample should have a “hypergeometric distribution” with parameters k (the size of the second sample of individuals), m (total number of marked individuals in the population), and n (total number of unmarked individuals in the population).\nUsing the appropriate command in R for the hypergeometric distribution, calculate the maximum likelihood estimate for the total number of elephants in the park. Note that the total number is n + m, where n is the unknown parameter to be estimated. Note also that only integer values for n are allowed, and that n cannot be smaller than k - X, the observed number of unmarked individuals in the second sample.*\nCalculate a likelihood-based 95% confidence interval for the total number of elephants.**\n* 133\n** 104 < N < 193\n\n\n5.1 Answers\n\n# 1. Estimate N\nm <- 27   # total marked individuals in the population\nk <- 74   # size of second sample\nX <- 15   # number of recaptures\n\n# Note that N must be at least 86 = 74 - 15 + 27\n\nN <- 86:200\n\nloglike<-dhyper(15, 27, N - 27, 74, log = TRUE)\n\n# or\n\nloglike<-dhyper(X, m, n = N - 27, k, log = TRUE)\n\nplot(loglike ~ N, type=\"l\",\n      col = \"blue\", lwd = 2)\n\nabline(h = max(loglike),\n        col = \"red\", lty = 2, lwd = 2)\n\nabline(v = N[loglike == max(loglike)],\n        col = \"black\", lty = 3, lwd = 2)\n\ntext(x = 170, y = -12,\n    labels = \"The intersection \\n on the x-axis is our \\n best guess of N\")\n\n\n\n\n\n\n# 2. 95% confidence interval\nz <- N[loglike < max(loglike) - 1.92]\nc( max(z[z < 133]), min(z[z > 133]) )\n\n[1] 104 193"
  },
  {
    "objectID": "lab13-likelihood.html#example-voyaging-voles",
    "href": "lab13-likelihood.html#example-voyaging-voles",
    "title": "Lab 13 Likelihood",
    "section": "6 Example: Voyaging voles",
    "text": "6 Example: Voyaging voles\nThe movement or dispersal distance of organisms is often modeled using the geometric distribution, assuming steps are discrete rather than continuous. For example, M. Sandell, J. Agrell, S. Erlinge, and J. Nelson (1991. Oecologia 86: 153-158) measured the distance separating the locations where individual voles, Microtus agrestis, were first trapped and the locations they were caught in a subsequent trapping period, in units of the number of home ranges.\nThe data for 145 male and female voles are in the file vole.csv. The variable “dispersal” indicates the distance moved (number of home ranges) from the location of first capture. If the geometric model is adequate, then Pr[X = 0 home ranges moved] = p Pr[X = 1 home ranges moved] = (1-p)p Pr[X = 2 home ranges moved] = (1-p)2p and so on. p is the probability of success (capture) at each distance from the location of the first capture.\n\n\nTabulate the number of home ranges moved by voles in this study. Use the data from both sexes combined.\nUsing the appropriate commands in R, calculate the log-likelihood of each of a range of possible values for p, the parameter of the geometric distribution. Plot the log likelihood against p.\nCalculate the maximum-likelihood estimate of p and the likelihood based 95% confidence interval.\nUse the appropriate R command and the maximum likelihood estimate of p to calculate the predicted fraction of voles dispersing 0, 1, 2, 3, 4, and 5 home ranges.\nUse the result in (3) to calculate the expected number* of voles (out of 145) dispersing 0 - 5 home ranges, assuming a geometric distribution. How does this compare with the observed frequencies?\n\n* 124.41 17.67 2.51 0.36 0.05 0.01\n\n\n6.1 Suggested solutions\n\nx <- read.csv(\"data/vole.csv\", \n        stringsAsFactors = FALSE)\n        \nhead(x)\n\n     sex dispersal\n1 female         0\n2 female         0\n3 female         0\n4 female         0\n5 female         0\n6   male         0\n\n# 1. Tabulate number of home ranges moved\ntable(x$dispersal)\n\n\n  0   1   2 \n123  20   2 \n\n# 2. MLE of p\np <- seq(0.05, 0.95, by = 0.001)\n\n# Using for loop\n# make a vector to store values we will calculate\nloglike <- vector()\n\nfor(i in 1:length(p)){\n    loglike[i] <- sum(dgeom(x$dispersal, prob=p[i], log = TRUE) )\n    }\n\nplot(p, loglike, type=\"l\", \n      xlab=\"p\", ylab=\"log likelihood\",\n      col = \"blue\", lwd = 2)\n\n\n\n\n\n\n# 3. MLE of p\nmax(loglike)\n\n[1] -69.0532\n\nphat <- p[loglike == max(loglike)]\nz <- p[loglike < max(loglike) - 1.92]\nc( max(z[z < 0.858]), min(z[z > 0.858]) )\n\n[1] 0.800 0.906\n\n\n\n# 4. Expected numbers \nfrac <- dgeom(0:5, prob=phat)\nround(frac,4)\n\n[1] 0.8580 0.1218 0.0173 0.0025 0.0003 0.0000\n\nexpected <- frac * nrow(x)\n\nround(expected, 2)\n\n[1] 124.41  17.67   2.51   0.36   0.05   0.01\n\n# Use the maximum likelihood estimate of p to calculate the \n# predicted fraction of voles dispersing 0, 1, 2, 3, 4, and 5 home ranges.\nfrac <- dgeom(0:5, prob=phat)\nround(frac,4)\n\n[1] 0.8580 0.1218 0.0173 0.0025 0.0003 0.0000\n\n# Expected distances moved\ndist <- nrow(x) * frac\nround(dist, 2)\n\n[1] 124.41  17.67   2.51   0.36   0.05   0.01\n\n# Compare with observed:\ntable(x$dispersal)\n\n\n  0   1   2 \n123  20   2"
  },
  {
    "objectID": "lab13-likelihood.html#example-life-of-bees",
    "href": "lab13-likelihood.html#example-life-of-bees",
    "title": "Lab 13 Likelihood",
    "section": "7 Example: Life of bees",
    "text": "7 Example: Life of bees\nThe life span of individuals in a population are often approximated by an exponential distribution. To estimate the mortality rate of foraging honey bees, P. K. Visscher and R. Dukas (1997. Insectes Sociaux 44: 1-5) recorded the entire foraging life span of 33 individual worker bees in a local bee population in a natural setting. The 33 life spans (in hours) are in the file bees.csv.\n\n\nPlot the frequency distribution of lifespans of the 33 bees. Choose the option to display probability density instead of raw frequency. Does the distribution of lifespans resemble an exponential distribution (make sure to try different bin widths of the histogram)?\nUse the exponential approximation and maximum likelihood to estimate the hourly mortality rate of bees.*\nUsing the maximum likelihood estimate, calculate the probability density for the exponential distribution across a range of values for lifespan. Draw this relationship between probability and lifespan on top of the frequency distribution you plotted in (1). Comment on the fit between the data and the exponential distribution you fitted. Is the exponential distribution a good fit to these data?\nAssume (for the purposes of this exercise) that an exponential probability model is reasonable. Calculate the likelihood-based 95% confidence interval for the mortality rate.**\nCalculate the maximum likelihood estimate for the mean lifespan, with approximate 95% likelihood based confidence interval.***\n(Optional) Use the bbmle package to find the maximum likelihood estimate and 95% confidence interval for the hourly mortality rate of bees. How do the results compare with your calculations?\n\n\n* 0.036 / hour\n** (0.025, 0.050) / hour\n*** 27.8 hours; 95% CI: (20, 40) hours\n\n\n7.1 Suggested solutions\n\nbees <- read.csv(\"data/bees.csv\", \n          stringsAsFactors = FALSE)\n          \nhead(bees)\n\n  id hours\n1  1   7.1\n2  2   2.3\n3  3   9.6\n4  4  25.8\n5  5  14.6\n6  6  12.8\n\n\n\n# 1. Plot histogram\n\nhist(bees$hours, right = FALSE, col = \"goldenrod\", prob = TRUE, \n      ylim = c(0,0.035), las = 1, breaks = 15)\n\n\n\n\n\n\n# 2. MLE of rate (in per hour units)\nrate <- seq(.001, .1, by=.001)\n\n# Using for loop\nloglike <- vector()\n\nfor(i in 1:length(rate)){\n    loglike[i] <- sum(dexp(bees$hours, rate = rate[i], log = TRUE))\n    }\n\nplot(rate, loglike, type=\"l\",\n    col = \"blue\", lwd = 2)\n\n\n\n\n\n\nmax(loglike)\n\n[1] -142.7838\n\nmlrate <- rate[loglike==max(loglike)] # per hour\nmlrate\n\n[1] 0.036\n\n\n\n# 3. Exponential might not be a great fit\nhist(bees$hours, right = FALSE, col = \"goldenrod\", prob = TRUE, \n      ylim = c(0,0.035), las = 1, breaks = 15)\n\nx <- bees$hours[order(bees$hours)]\ny <- dexp(x, rate = 0.036)\nlines(x, y, lwd=2)\n\n\n\n\n\n\n# 4. 95% confidence interval\nz <- rate[loglike < max(loglike) - 1.92]\nc( max(z[z < mlrate]), min(z[z > mlrate]) )\n\n[1] 0.025 0.050\n\n\n\n# 5. Mean lifespan\n1/mlrate\n\n[1] 27.77778\n\n1/c( max(z[z < mlrate]), min(z[z > mlrate]) )\n\n[1] 40 20\n\n\n\n# 6\n# bbmle package\nif(!require(\"bbmle\")) install.packages(\"bbmle\")\nlibrary(bbmle)\n\npNegLogLike <- function(rate){-sum(dexp(bees$hours, rate=rate, log=TRUE))}\n\nsuppressWarnings(\n  z <- mle2(pNegLogLike, start=list(rate = 1))\n  )\n\nsummary(z)\n\nMaximum likelihood estimation\n\nCall:\nmle2(minuslogl = pNegLogLike, start = list(rate = 1))\n\nCoefficients:\n      Estimate Std. Error z value     Pr(z)    \nrate 0.0359102  0.0062512  5.7446 9.216e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n-2 log L: 285.5674 \n\npProfile <- profile(z)\n\nconfint(pProfile, method=\"uniroot\")\n\n     2.5 %     97.5 % \n0.02500926 0.04959301"
  },
  {
    "objectID": "lab13-likelihood.html#harper-adams-data-science",
    "href": "lab13-likelihood.html#harper-adams-data-science",
    "title": "Lab 13 Likelihood",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab14-GLM.html#objectives",
    "href": "lab14-GLM.html#objectives",
    "title": "Lab 14 GLM",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nFit generalized linear models with R\nEvaluate GLM assumptions\nEvaluate statistical and graphical results for GLM\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab14-GLM.html#start-a-script",
    "href": "lab14-GLM.html#start-a-script",
    "title": "Lab 14 GLM",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab14-GLM.html#generalized-linear-models-important-stuff",
    "href": "lab14-GLM.html#generalized-linear-models-important-stuff",
    "title": "Lab 14 GLM",
    "section": "3 Generalized linear models (important stuff)",
    "text": "3 Generalized linear models (important stuff)\nIn this lab we will fit general linear models to data, implemented in the R command glm(). A generalized linear model is useful when the response variable has a distribution other than the Gaussian distribution (like binary outcomes; like data that are counts of something), and when a transformation of the data is undesirable or impossible. Example situations include binary response data (1 or 0, dead or alive) or data that are counts (number of offspring, leaves, or tattoos). The approach is also useful in the analysis of contingency tables.\n\n\n\n\n\nSong sparrow"
  },
  {
    "objectID": "lab14-GLM.html#example-natural-selection-in-song-sparrows",
    "href": "lab14-GLM.html#example-natural-selection-in-song-sparrows",
    "title": "Lab 14 GLM",
    "section": "4 Example: Natural selection in song sparrows",
    "text": "4 Example: Natural selection in song sparrows\nThe song sparrow population on the island of Mandarte has been studied for many years by Jamie Smith, Peter Arcese, and collaborators. The birds were measured and banded and their fates on the island have recorded over many years. Here we will look for evidence of natural selection using the relationship between phenotypes and survival.\nThe data file songsparrow.csv gives survival of young-of-the-year females over their first winter (1=survived, 0=died). The file includes measurements of beak and body dimensions: body mass (g), wing length, tarsus length, beak length, beak depth, beak width (all in mm), year of birth, and survival. These data were analyzed previously in D. Schluter and J. N. M Smith (1986, Evolution 40: 221-231).\n\n\n4.1 Read and examine the data\n\n\nRead the data from the file and inspect the first few lines to make sure it was read correctly.\nWe’ll be comparing survival probabilities among different years. To this end, make sure that year is a categorical variable in your data frame.\nPlot survival against tarsus length of female sparrows. Use a method to reduce the overlap of points (the response variable is 0 or 1) to see the patterns more clearly.\nExamine the plot. Can you visualize a trend? Use a smoothing method to see if any trend is present (most methods won’t constrain the curve to lie between 0 and 1, but at least you’ll get an idea).\n\n\n\n\n4.2 Fit a generalized linear model\n\nLet’s start by ignoring the fact that the data are from multiple years. We will have the option later to add year to the model to see if it makes a difference.\n\nThe response variable is binary. What probability distribution is appropriate to describe the error distribution around a model fit? What is an appropriate link function?\nFit a generalized linear model to the data on survival and tarsus length.\nUse visreg() to visualize the model fit.\nObtain the estimated regression coefficients for the fitted model. What is the interpretation of these coefficients? On a piece of paper, write down the complete formula for the model shown in the visreg plot.\nUse the coefficients to calculate the predicted survival probability of a song sparrow having tarsus length 20.5 mm*. Does the result agree with your plot of the fitted regression curve?\nThe ratio (-intercept/slope) estimates the point at which probability of survival is changing most rapidly. In toxicology this point is known as the LD50. Calculate this value** and compare it visually with the fitted curve. Does it agree? Finally, the slope of the curve at a given value for the explanatory variable x is b * p(x) * ( 1 - p(x) ), where b is the slope coefficient of the fitted logistic regression model and p(x) is the predicted probability of survival at that x.\nCalculate the likelihood-based 95% confidence interval for the logistic regression coefficients.\nThe summary(z) output for the regression coefficients also includes “z values” and P-values. What caution would you take when interpreting these P-values? Use a more accurate method to test the null hypothesis of zero slope.\n\n* -1.148577; 0.2407491\n** 19.58683\n\n\n\n4.3 Suggested solutions\n\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(visreg))\nsuppressPackageStartupMessages(library(MASS))\n\n# 1.Read the data\nx <- read.csv(\"data/songsparrow.csv\", \n        stringsAsFactors = FALSE)\nhead(x)\n\n  mass wing tarsus blength bdepth bwidth year sex survival\n1 23.7 67.0   17.7     9.1    5.9    6.8 1978   f        1\n2 23.1 65.0   19.5     9.5    5.9    7.0 1978   f        0\n3 21.8 65.2   19.6     8.7    6.0    6.7 1978   f        0\n4 21.7 66.0   18.2     8.4    6.2    6.8 1978   f        1\n5 22.5 64.3   19.5     8.5    5.8    6.6 1978   f        1\n6 22.9 65.8   19.6     8.9    5.8    6.6 1978   f        1\n\n# 2. Year as categorical variable\nx$year <- as.character(x$year)\n\n# 3. Plot survival against tarsus length\nggplot(x, aes(tarsus, survival)) +\n        geom_jitter(color = \"blue\", \n                    size = 3, height = 0.04, \n                    width = 0, alpha = 0.5) +\n        labs(x = \"Tarsus length (mm)\", y = \"Survival\") + \n        theme_classic()\n\n\n\n\n\n\n# 4. Examine the plot. Can you visualize a trend?\n\n# Same plot, add trend line with geom_smooth()\nggplot(x, aes(tarsus, survival)) +\n        geom_jitter(color = \"blue\", size = 3, \n                    height = 0.04, width = 0, alpha = 0.5) +\n        geom_smooth(method = \"loess\", size = 1, \n                    col = \"red\", lty = 2, se = FALSE) +\n        labs(x = \"Tarsus length (mm)\", y = \"Survival\") + \n        theme_classic()\n\n\n\n\n\n\n# 5. Binomial distribution. Logit link function\n\n# 6. Fit generalized linear model\nz <- glm(formula = survival ~ tarsus, \n          family = binomial(link=\"logit\"), \n          data = x)\n\n# 7. Visualize model fit (data points added with points() )\nvisreg(z, xvar = \"tarsus\", scale = 'response',\n        rug = FALSE, ylim = c(-.1, 1.1))\n\npoints(jitter(survival, 0.2) ~ tarsus, data = x, \n        pch = 1, col = \"blue\", cex = 1, lwd = 1.5)\n\n\n\n\n\n\n# 8. Estimated regression coefficients of the linear predictor\nsummary(z)\n\n\nCall:\nglm(formula = survival ~ tarsus, family = binomial(link = \"logit\"), \n    data = x)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7292  -1.0659  -0.6273   1.0794   1.8003  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  24.6361     6.7455   3.652 0.000260 ***\ntarsus       -1.2578     0.3437  -3.659 0.000253 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 200.95  on 144  degrees of freedom\nResidual deviance: 185.04  on 143  degrees of freedom\nAIC: 189.04\n\nNumber of Fisher Scoring iterations: 4\n\n# 9. Predicted survival probability of a song sparrow having tarsus length 20.5 mm\npredict(z, newdata = data.frame(tarsus = 20.5), type = \"response\")\n\n        1 \n0.2407491 \n\n# 10. LD50\ndose.p(z)\n\n             Dose        SE\np = 0.5: 19.58683 0.1398026\n\n# 11. Likelihood-based 95% confidence intervals (logit scale)\nconfint(z)\n\n                2.5 %     97.5 %\n(Intercept) 12.035020 38.6162515\ntarsus      -1.970217 -0.6157285\n\n# 12. Test null hypothesis of zero slope\nanova(z, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: survival\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev Pr(>Chi)    \nNULL                     144     200.95             \ntarsus  1   15.908       143     185.04 6.65e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\nWhat is the likelihood of a perfect sunset?"
  },
  {
    "objectID": "lab14-GLM.html#example-crab-klepto-copulation",
    "href": "lab14-GLM.html#example-crab-klepto-copulation",
    "title": "Lab 14 GLM",
    "section": "5 Example: Crab klepto-copulation",
    "text": "5 Example: Crab klepto-copulation\n\nThe horseshoe crab, Limulus polyphemus, has two alternative male reproductive morphs. Some males attach to females with a special appendage. The females bring these males with them when they crawl onto beaches to dig a nest and lay eggs, which the male then fertilizes. Other males are satellites, which are unattached to females but crowd around nesting pairs and obtain fertilizations. What attributes of a female horseshoe crab determine the number of satellite males she attracts on the beaches?\nThe data file satellites.csv provides measurements of 173 female horseshoe crabs and records the number of satellites they attracted. The data were gathered by Brockman (1996. Satellite male groups in horseshoe crabs, Limulus polyphemus. Ethology 102:1-21) and were published by Agresti (2002, Categorical data analysis, 2nd ed. Wiley). The variables are female color, spine condition, carapace width (cm), mass (kg), and number of satellite males.\n\n\n5.1 Read and examine the data\n\nRead the data from the file. View the first few lines of data to make sure it was read correctly. Use the str command to see the variables and groups.\nPlot the number of satellites against the width of the carapace, a measure of female body size. Fit a smooth curve to examine the trend.\n\n\n\n\n5.2 Fit a generalized linear model\n\nFit a generalized linear model to the relationship between number of satellite males and female carapace width. What type of variable is the number of satellites? What probability distribution might be appropriate to describe the error distribution around a model fit? What is the appropriate link function?\nVisualize the model fit on the transformed scale, including confidence bands. This plot reminds us that on the transformed scale, glm() is fitting a straight line relationship. (Don’t worry about the points – they aren’t the transformed data, but rather are “working values” for the response variable from the last iteration of model fitting, which glm() uses behind the scenes to fit the model on the transformed scale.)\nVisualize the model fit on the original data scale. Note that is it curvilinear.\nExtract the estimated regression coefficients from your model object. What is the interpretation of these coefficients? On a piece of paper, write down the complete formula for your fitted model.\nCalculate the likelihood-based 95% confidence interval for the regression coefficients. The most useful estimate is that for the slope: exp(slope) represents the multiple to the response variable accompanying a 1-unit change in the explanatory variable. In other words, if the slope were found to be 1.2, this would indicate that a 1 cm increase in carapace width of a female is accompanied by a 1.2-fold increase in the number of male satellites.\nTest the null hypothesis of no relationship between number of satellite males and female carapace width. Notice how small the P-value is for the null hypothesis test for the slope. I’m afraid that this is a little optimistic. Why? Read on.\nWhen you extracted the regression coefficients from your model object, you probably saw the following line of output: “(Dispersion parameter for poisson family taken to be 1)”. What are we really assuming* here?\nIf you did not want to rely on this assumption (or you wanted to estimate the dispersion parameter), what option is available to you? Refit a generalized linear model without making the assumption that the dispersion parameter is 1.\nExtract and examine the coefficients of the new glm model object. Examine the estimated dispersion parameter. Is it close to 1? On this basis, which of the two glm fits to the same data would you regard as the more reliable?\nHow do the regression coefficients of this new fit compare with the estimates from the earlier model fit? How do the standard errors compare? Why are they larger** this time?\nVisualize the new model fit and compare with the plot of the earlier fit. What difference do you notice?\nRedo the test of significance for the slope of the relationship between number of satellite mates and female carapace width. Remember to use the F test rather than the likelihood ratio test in the anova command. How do the results compare with those from the previous fit?\n\n\n\n\n5.3 Suggested solutions\n\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(visreg))\nsuppressPackageStartupMessages(library(MASS))\n\n# 1. Read data\nx <- read.csv(\"data/satellites.csv\", \n          stringsAsFactors = FALSE)\n\nhead(x)\n\n         color    spine width.cm nsatellites mass.kg\n1       medium both.bad     28.3           8    3.05\n2  dark-medium both.bad     22.5           0    1.55\n3 light-medium     good     26.0           9    2.30\n4  dark-medium both.bad     24.8           0    2.10\n5  dark-medium both.bad     26.0           4    2.60\n6       medium both.bad     23.8           0    2.10\n\n# 2. Plot\nggplot(x, aes(width.cm, nsatellites)) +\n        geom_jitter(color = \"blue\", size = 3, \n                    height = 0.2, width = 0, alpha = 0.5) +\n        geom_smooth(method = \"loess\", size = 1,\n                    col = \"red\", lty = 2, se = FALSE) +\n        labs(x = \"Carapace width (mm)\", y = \"No. satellites\") + \n        theme_classic()\n\n\n\n\n\n\n# 3. Fit model to count data, Poisson distribution, log link function\nz <- glm(formula = nsatellites ~ width.cm, \n         family = poisson(link = \"log\"), \n         data = x)\n\n# 4. Visualize model fit (transformed scale)\nvisreg(z, xvar = \"width.cm\")\n\n\n\n\n\n\n# 5. Visualize model fit (original scale), points overlaid\nvisreg(z, xvar = \"width.cm\", scale = \"response\", rug = FALSE)\npoints(jitter(nsatellites, 0.2) ~ width.cm, data = x, pch = 1, \n        col = \"blue\", lwd = 1.5)\n\n\n\n\n\n\n# 6. Estimated regression coefficients\nsummary(z)\n\n\nCall:\nglm(formula = nsatellites ~ width.cm, family = poisson(link = \"log\"), \n    data = x)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8526  -1.9884  -0.4933   1.0970   4.9221  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.30476    0.54224  -6.095  1.1e-09 ***\nwidth.cm     0.16405    0.01997   8.216  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 632.79  on 172  degrees of freedom\nResidual deviance: 567.88  on 171  degrees of freedom\nAIC: 927.18\n\nNumber of Fisher Scoring iterations: 6\n\n# 7. Likelihood-based 95% confidence interval\nconfint(z)\n\n                 2.5 %     97.5 %\n(Intercept) -4.3662326 -2.2406858\nwidth.cm     0.1247244  0.2029871\n\n# 8. Optimistic test\nanova(z, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: nsatellites\n\nTerms added sequentially (first to last)\n\n         Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                       172     632.79              \nwidth.cm  1   64.913       171     567.88 7.828e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 10. Refit \nz2 <- glm(nsatellites ~ width.cm, family = quasipoisson(link = \"log\"), data = x)\n\n# 11. Coefficients\nsummary(z2)\n\n\nCall:\nglm(formula = nsatellites ~ width.cm, family = quasipoisson(link = \"log\"), \n    data = x)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8526  -1.9884  -0.4933   1.0970   4.9221  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.30476    0.96729  -3.417 0.000793 ***\nwidth.cm     0.16405    0.03562   4.606 7.99e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 3.182205)\n\n    Null deviance: 632.79  on 172  degrees of freedom\nResidual deviance: 567.88  on 171  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6\n\n# 13. Visualize\nvisreg(z2, xvar = \"width.cm\", scale = \"response\", rug = FALSE)\npoints(jitter(nsatellites, 0.2) ~ width.cm, data = x, pch = 1, \n    col = \"blue\", lwd = 1.5)\n\n\n\n\n\n\n# 14. Redo test\nanova(z2, test = \"F\")\n\nAnalysis of Deviance Table\n\nModel: quasipoisson, link: log\n\nResponse: nsatellites\n\nTerms added sequentially (first to last)\n\n         Df Deviance Resid. Df Resid. Dev      F    Pr(>F)    \nNULL                       172     632.79                     \nwidth.cm  1   64.913       171     567.88 20.399 1.168e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nComment 1: By using the Poisson distribution to model the residuals, we assume that for any given value of the x-variable, the variance of y is equal to the mean of y. Typically, however, in real data the variance of y is greater than the mean of y at any given x (“overdispersion”). One reason is that a variety of factors cause variation in y, and most aren’t included in the model being fitted.\nComment 2: The model is now taking account of the actual amount of variance in y for each x, which is larger than that assumed by the first model you fitted."
  },
  {
    "objectID": "lab14-GLM.html#example-prions---resistance-is-not-futile-trekkie-reference",
    "href": "lab14-GLM.html#example-prions---resistance-is-not-futile-trekkie-reference",
    "title": "Lab 14 GLM",
    "section": "6 Example: Prions -> resistance is not futile (Trekkie reference)",
    "text": "6 Example: Prions -> resistance is not futile (Trekkie reference)\n\nThis last example is to demonstrate the use of glm() to model frequencies of different combinations of two (or more) variables in a contingency table. The presence of an interaction between the variables indicates that the relative frequencies of different categories for one variable differ between categories of the other variable. In other words, the two variables are then not independent.\nKuru is a prion disease (similar to Creutzfeldt–Jakob Disease) of the Fore people of highland New Guinea. It was once transmitted by the consumption of deceased relatives at mortuary feasts, a ritual that was ended by about 1960. Using archived tissue samples, Mead et al. (2009, New England Journal of Medicine 361: 2056-2065) investigated genetic variants that might confer resistance to kuru. The data are genotypes at codon 129 of the prion protein gene of young and elderly individuals all having the disease. Since the elderly individuals have survived long exposure to kuru, unusually common genotypes in this group might indicate resistant genotypes. The data are in the file kurudata.csv.\n\n\n6.1 Read and examine the data\n\nRead the data from the file. View the first few lines of data to make sure it was read correctly.\nCreate a contingency table comparing the frequency of the three genotypes at codon 129 of the prion protein gene of young and elderly individuals (all having the disease). Notice any pattern? By comparing the frequencies between young people and older people, which genotype is likely to be more resistant to the disease?\nCreate a grouped bar graph illustrating the relative frequencies of the three genotypes between afflicted individuals in the two age categories.\n\n\n\n\n6.2 Fit a generalized linear model\n\nTo model the frequencies you will first need to convert the contingency table to a “flat” frequency table using data.frame().\nFit a generalized linear model to the frequency table. To begin, fit the additive model, i.e., use a model formula without an interaction between the two variables genotype and age.\nVisualize the fit of the additive model to the frequency data. Notice how the additive model is constrained from fitting the exact frequencies in each category.\nRepeat the model fitting but include the interaction term as well. Visualize the fit of the model to the data. Notice how this “full” model really is full – it fits the frequencies exactly.\nTest whether the relative frequencies of the three genotypes differs between the two age groups (i.e., whether there is a significant interaction between age and genotype).\n\n\n\n\n6.3 Suggested solutions\n\n# 1. Read the data\nx <- read.csv(\"data/kurudata.csv\", \n          stringsAsFactors = FALSE)\nhead(x)\n\n  Genotype Cohort\n1       MM    old\n2       MM    old\n3       MM    old\n4       MM    old\n5       MM    old\n6       MM    old\n\n# 2. Contingency table\nx$Cohort <- factor(x$Cohort, levels = c(\"young\",\"old\")) \n\nkurutable <- table(x)\nkurutable\n\n        Cohort\nGenotype young old\n      MM    22  13\n      MV    12  77\n      VV    14  14\n\n# 3. (Optional) Grouped bar graph \nggplot(x, aes(x = Cohort, fill = Genotype)) + \n        geom_bar(stat = \"count\", \n                  position = position_dodge2(preserve=\"single\")) +\n        labs(x = \"Cohort\", y = \"Frequency\") +\n        theme_classic()\n\n\n\n\n\n\n# 4. Model the frequencies\nx1 <- data.frame(kurutable)\nx1\n\n  Genotype Cohort Freq\n1       MM  young   22\n2       MV  young   12\n3       VV  young   14\n4       MM    old   13\n5       MV    old   77\n6       VV    old   14\n\n# 5. Fit additive model\nz <- glm(Freq ~ Genotype + Cohort, family = poisson(link = \"log\"), data = x1)\n\n# 6. Visualize model fit\n# (scale = \"response\" is broken, visualize on transformed scale instead)\nvisreg(z, xvar = \"Genotype\", by = \"Cohort\")\n\n\n\n\n\n\n# 7. Fit model with interaction and visualize\n# (scale = \"response\" is broken, visualize on transformed scale instead)\nz <- glm(Freq ~ Genotype * Cohort, family = poisson(link = \"log\"), data = x1)\nvisreg(z, xvar = \"Genotype\", by = \"Cohort\") \n\n\n\n\n\n\n# 8. Test interaction\nanova(z, test = \"Chi\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: Freq\n\nTerms added sequentially (first to last)\n\n                Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                                5     96.501              \nGenotype         2   41.174         3     55.327 1.146e-09 ***\nCohort           1   21.126         2     34.202 4.301e-06 ***\nGenotype:Cohort  2   34.202         0      0.000 3.743e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lab14-GLM.html#harper-adams-data-science",
    "href": "lab14-GLM.html#harper-adams-data-science",
    "title": "Lab 14 GLM",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab15-model-selection.html#objectives",
    "href": "lab15-model-selection.html#objectives",
    "title": "Lab 15 Model selection",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nCompare statistical models using real data\nEvaluate statistical model fit\nUse diagnostic information to rank and choose amongst statistical models\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab15-model-selection.html#start-a-script",
    "href": "lab15-model-selection.html#start-a-script",
    "title": "Lab 15 Model selection",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab15-model-selection.html#model-selection",
    "href": "lab15-model-selection.html#model-selection",
    "title": "Lab 15 Model selection",
    "section": "3 Model selection",
    "text": "3 Model selection\nSelecting among candidate models requires a criterion for evaluating and comparing models, and a strategy for searching the possibilities. In this lab we will explore some of the tools available in R for model selection. You might need to download and install the {MuMIn} package from the CRAN website to carry out all the exercises.\n\n\n\n\n\nzebra"
  },
  {
    "objectID": "lab15-model-selection.html#example-scaling-of-bmr-in-mammals",
    "href": "lab15-model-selection.html#example-scaling-of-bmr-in-mammals",
    "title": "Lab 15 Model selection",
    "section": "4 Example: Scaling of BMR in mammals",
    "text": "4 Example: Scaling of BMR in mammals\nSavage et al. (2004, Functional Ecology 18: 257-282) used data to reevaluate competing claims for the value of the allometric scaling parameter β relating whole-organism Basal Metabolic Rate (BMR) rate to body mass in endotherms:\n\\(BMR=\\alpha M \\beta\\)\n\nIn this formula BMR is basal metabolic rate, M is body mass, and α is a constant. On a log scale this can be written as\n\\(log(BMR)=log(\\alpha)+\\beta log(M)\\)\n\nwhere β is now a slope parameter of an ordinary linear regression – a linear model.\nTheory based on optimization of hydrodynamic flows through the circulation system predicts that the exponent should be β=3/4, whereas we would expect β=2/3 if metabolic rate scales with heat dissipation and therefore body surface area. These alternative scaling relationships represent distinct biophysical hypotheses. We will use them as candidate models and apply model selection procedures to compare their fits to data.\n\nSavage et al. compiled data from 626 species of mammals. To simplify, and reduce possible effects of non-independence of species data points, they took the average of log(BMR) among species in small intervals of log(M).\nThe resulting values of basal metabolic rate and mass can be found in the mbr.csv data file. Body mass is in grams, whereas basal metabolic rate is in watts.\n\n\nPlot the data. Is the relationship between mass and metabolic rate linear on a log scale?\nFit a linear model to the log-transformed data (original data are not on the log scale). What is the estimate of slope?\nProduce a 95% confidence interval for the slope. Does the interval include either of the candidate values for the scaling parameter β?\nAdd the least squares regression line from (2) to your plot.\nNow let’s use model selection to compare the fits of the two candidate models to the data using the following steps.\nTo begin, you need to force regression lines having specified slopes through the (log-transformed) data. Replot the data indicating the relationship between log(M) and log(BMR). Add to this plot the best-fit line having slope 3/4. Repeat this for the slope 2/3. By eye, which line appears to fit the data best?\nCompare the residual sum of squares of the two models you fit in (5). Which has the smaller value? Do these values agree with your visual assessment of your plots in (6)?\nCalculate the log-likelihood of each model fitted in (5). Which has the higher value?\nCalculate AIC for the two models, and the AIC difference*. By this criterion, which model is best? How big is the AIC difference?\nIn general terms, what does AIC score attempt to measure?\nCalculate the Akaike weights of the two models**. Which has the higher weight of evidence in its favor? These weights would be used in Multimodel Inference (such as model averaging), which we won’t go into in this course. The weights should sum to 1. (They are sometimes interpreted as the posterior probability that the given model is the “best” model, assuming that the “best” model is one of the set of models being compared, but this interpretation makes assumptions that we won’t go into right now.)\nSummarize the overall findings. Do both models have some support, according to standard criteria, or does one of the two models have essentially no support?\nWhy is it not possible to compare the two models using a conventional log-likelihood ratio test***?\nOptional: Both theories mentioned earlier predict that the relationship between basal metabolic rate and body mass will conform to a power law — in other words that the relationship between log(BMR) and log(M) will be linear. Is the relationship linear in mammals? Use AIC to compare the fit of a linear model fitted to the relationship between log(BMR) and log(M) with the fit of a quadratic regression of log(BMR) on log(M) (a model in which both log(M) and (log(M))2 are included as terms). Don’t force a slope of 2/3 or 3/4. Plot both the linear and quadratic regression curves with the data. Which model has the most support? Which has the least? On the basis of this analysis, does the relationship between basal metabolic rate and body mass in mammals conform to a power law?\n\n* 23.73591\n** 9.99e-01 7.01e-06\n***The models are not nested.\n\n\n4.1 Suggested solutions\n\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(MuMIn))\nsuppressPackageStartupMessages(library(visreg))\nsuppressPackageStartupMessages(library(MASS))\nsuppressPackageStartupMessages(library(car))\n\n\n# 1. Plot the data\nbmr <- read.csv(\"data/bmr.csv\", \n          stringsAsFactors = FALSE)\n\n# Add a new variable right into the data object - I love this\nbmr$logmass <- log(bmr$mass.g)\nbmr$logbmr <- log(bmr$bmr.w)\n\nhead(bmr)\n\n  mass.g bmr.w   logmass    logbmr\n1    2.4 0.063 0.8754687 -2.764621\n2    3.7 0.027 1.3083328 -3.611918\n3    4.6 0.067 1.5260563 -2.703063\n4    5.6 0.108 1.7227666 -2.225624\n5    7.3 0.103 1.9878743 -2.273026\n6    8.9 0.102 2.1860513 -2.282782\n\nplot(logbmr ~ logmass, data = bmr, \n      xlab = \"Log body mass\",   \n      ylab = \"Log basal metabolic rate\",\n      las = 1, pch = 16, col = \"blue\") # mere vanity\n\n\n\n\n\n\n# 2. Linear model\nz <- lm(logbmr ~ logmass, data = bmr)\n\nsummary(z)\n\n\nCall:\nlm(formula = logbmr ~ logmass, data = bmr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18771 -0.13741  0.01169  0.17836  0.62592 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.00329    0.09858  -40.61   <2e-16 ***\nlogmass      0.73654    0.01261   58.42   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3243 on 50 degrees of freedom\nMultiple R-squared:  0.9856,    Adjusted R-squared:  0.9853 \nF-statistic:  3413 on 1 and 50 DF,  p-value: < 2.2e-16\n\nconfint(z)\n\n                 2.5 %     97.5 %\n(Intercept) -4.2013002 -3.8052825\nlogmass      0.7112222  0.7618661\n\n\n\n\n# 4. Add the best-fit regression line to the plot in (1)\nplot(logbmr ~ logmass, data = bmr, \n      xlab = \"Log body mass\",   \n      ylab = \"Log basal metabolic rate\",\n      las = 1, pch = 16, col = \"blue\") # mere vanity\nabline(z,\n       col = \"red\", lty = 2, lwd = 2) # mere vanity \n\n\n\n\n\n\n# 5. Fit the two candidate models\nz1 <- lm(logbmr ~ 1 + offset( (3/4) * logmass ), data = bmr)\nz2 <- lm(logbmr ~ 1 + offset( (2/3) * logmass ), data = bmr)\n\n# 6. Replot\nplot(logbmr ~ logmass, data = bmr, \n      las = 1, pch = 16, col = \"blue\",\n      xlab = \"Log body mass\", \n      ylab = \"Log basal metabolic rate\")\n\n# line for offset( (3/4) * logmass )\nabline(a = coef(z1), b = 3/4, col = \"darkgreen\", lwd = 3)\n\n# line for offset( (2/3) * logmass )\nabline(a = coef(z2), b = 2/3, col = \"red\", lwd = 3)\n\n\n\n\n\n\n# or\nggplot(data = bmr, aes(x = logmass, y = logbmr)) +\n        geom_point(col = \"blue\", cex = 2) + \n        geom_abline(intercept = coef(z1), slope = 3/4, \n                    col = \"darkgreen\", lwd=1.5) +\n        geom_abline(intercept = coef(z2), slope = 2/3, \n                    col = \"red\", lwd=1.5) +\n        labs(x = \"Log body mass\", y = \"Log basal metabolic rate\") + \n        theme_classic()\n\n\n\n\n\n\n# 7. Compare the residual sum of squares\nanova(z1) # Notice model (Mean Sq) error\n\nAnalysis of Variance Table\n\nResponse: logbmr\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 51 5.3777 0.10545               \n\nanova(z2) # Notice more model (Mean Sq) error than z1\n\nAnalysis of Variance Table\n\nResponse: logbmr\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 51 8.4886 0.16644               \n\n\n\n\n# 8. Log-likelihoods\n# higher (better) log-likelihood for z1\nc( logLik(z1), logLik(z2) )\n\n[1] -14.79135 -26.65930\n\n# 9. AIC and AIC difference\n# lower (better) AIC for z1\nc( AIC(z1), AIC(z2) )\n\n[1] 33.58269 57.31860\n\ndelta <- c(AIC(z1), AIC(z2)) - min(AIC(z1), AIC(z2))\ndelta\n\n[1]  0.00000 23.73591\n\n\n\n\n# 11. Akaike weights of the two models\nL <- exp(-0.5 * delta)\nL/sum(L)\n\n[1] 9.999930e-01 7.011488e-06\n\n\n\n\n# 14. Compare the fit of linear and quadratic models\nzlin <- lm(logbmr ~ logmass, data = bmr)\nzquad <- lm(logbmr ~ logmass + I(logmass^2), data = bmr)\n\n# Get predicted values from each model\nbmr$predictLin <- predict(zlin)\nbmr$predictQuad <- predict(zquad)\n\nplot(logbmr ~ logmass, data = bmr, las = 1, pch = 16, col = \"blue\",\n      xlab = \"Log body mass\", ylab = \"Log basal metabolic rate\")\n\nlines(predictLin ~ logmass, data = bmr,\n      col = \"darkgreen\", lwd = 2, lty = 2) # vanity\n\nlines(predictQuad ~ logmass, data = bmr,\n      col = \"red\", lwd = 2, lty = 2) # vanity\n\n\n\nc(AIC(zlin), AIC(zquad))\n\n[1] 34.41123 21.58976\n\nc(AIC(zlin), AIC(zquad)) - min(c(AIC(zlin), AIC(zquad)))\n\n[1] 12.82146  0.00000"
  },
  {
    "objectID": "lab15-model-selection.html#example-bird-abundance-in-forest-fragments",
    "href": "lab15-model-selection.html#example-bird-abundance-in-forest-fragments",
    "title": "Lab 15 Model selection",
    "section": "5 Example: Bird abundance in forest fragments",
    "text": "5 Example: Bird abundance in forest fragments\n\nIn this example we are going “data dredging” (looking at many competing models without a specific hypothesis), with all its attendant risks, unlike the previous example (where we had very specific, biologically motivated hypotheses). Here, we have no candidate models. Let’s just try all possibilities and see what turns up. The data include a set of possible explanatory variables and we want to known which model, of all possible models, is the “best”. Sensibly, we also wish to identify those models that are near-best and should be kept under consideration (e.g., for use in planning, or subsequent multimodel inference).\nThe response variable is the abundance of forest birds in 56 forest fragment in southeastern Australia by Loyn (1987, cited in Quinn and Keough [2002] and analyzed in their Box 6.2). Abundance is measured as the number of birds encountered in a timed survey (units aren’t explained). Six predictor variables were measured in each fragment:\n\narea: fragment area (ha)\ndist: distance to the nearest other fragment (km)\nldist: distance to the nearest larger fragment (km)\ngraze: grazing pressure (1 to 5, indicating light to heavy)\nalt: altitude (m)\nyr.isol: number of years since fragmentation.\n\nThe data are in the file birdabund.csv.\n\n\nUsing histograms, scatter plots, or the pairs command, explore the frequency distributions of the variables. Several of the variables are highly skewed, which will lead to outliers having excessive leverage. Transform the highly skewed variables to solve this problem. (I log-transformed area, dist and ldist. The results are not perfect.)\nUse the cor command to estimate the correlation between pairs of explanatory variables. The results will be easier to read if you round to just a couple of decimals. Which are the most highly correlated variables?\nUsing the model selection tool dredge() in the MuMIn package, determine which linear model best predicts bird abundance (use AIC as the criterion). dredge() carries out an automated model search using subsets of the ‘global’ model provided. Ignore interactions for this exercise. (You will need to install the MuMIn package if you haven’t yet done so.)\nHow many variables are included in the best model*?\nCount the number of models in total having an AIC difference less than or equal to 7. This is one way to decide which models have some support and remain under consideration.\nAnother way to determine the set of models that have support is to use AIC weights. Calculate the Akaike weights of all the models from your dredge() analysis. How much weight is given to the best model**? Are there common features shared among the models having the highest weights?\nHow many models are in the “confidence set” whose cumulative weights reach 0.95***?\nUse a linear model to fit the “best” model to the data. Produce a summary of the results. Use visreg() to visualize the conditional relationship between bird abundance and each of the three variables in the “best” model one at a time. Visually, which variable seems to have the strongest relationship with bird abundance in the model?\nGenerate an ANOVA table for the best model. Use Type 2 or Type 3 sums of squares so that the order of entry of the main effects in the formula don’t affect the tests (there are no interactions). Why should we view the resulting P-values with a great deal of skepticism****?\nNotice that in your ANOVA table, not all terms in the best model are stastically significant at P<0.05 and so would not be retained in a stepwise multiple regression process. Are you OK with this? Good.\n\n* 3 plus intercept (plus variance of residuals makes “df” = 5 parameters estimated)\n** 0.127\n*** 20\n**** Because we arrived at this model by data dredging.\n\nLet’s try analyzing the data using stepAIC() from the {MASS} package. Despite its name the method is not carrying out stepwise multiple regression. Rather, it is using a stepwise search strategy (hopefully) to find the “best” model (the model minimizing the AIC score) given certain restrictions. Restrictions include higher order terms (e.g., interaction between two variables) not being fitted without including corresponding lower order terms (e.g., main effects of those same variables). Unlike dredge() it does not test all (restricted) subsets of the global model and so does not provide a list of all other models that fit the data nearly equally well as the “best” model. But it can be much faster if there are many variables.\n\n\nReturn to the data set you just analyzed using dredge() and run model selection using stepAIC() instead. Did you arrive at the same best model?\nRun stepAIC() again, but this time use a model that includes all two-way interaction terms. This is already pushing the data to the limit, because there are only 56 data points. View the printed output on the screen to see the sequence of steps that stepAIC takes to find the best model.\nEstimate the coefficients of the best-fitting model.\nCalculate AIC for the best model. How does it compare to the AIC value computed in previously for the best additive model (the best model without interaction terms)?** Does the additive model have “essentially no support”, as defined in lecture**?\n\n* 360.7 vs 371.1\n** Yes, because the AIC difference is large, exceeding 10.\n\n\n5.1 Suggested solutions\n\n# 1. Read, plot and transform \nbirds <- read.csv(\"data/birdabund.csv\", \n            stringsAsFactors = FALSE)\nhead(birds)\n\n  abund area yr.isol dist ldist graze alt\n1   5.3  0.1    1968   39    39     2 160\n2   2.0  0.5    1920  234   234     5  60\n3   1.5  0.5    1900  104   311     5 140\n4  17.1  1.0    1966   66    66     3 160\n5  13.8  1.0    1918  246   246     5 140\n6  14.1  1.0    1965  234   285     3 130\n\npairs(birds, gap = 0)\n\n\n\n\n\n\n# Notice variables: area, dist, and ldist have wide scale\n\n# Try log-transforming area, dist, and ldist . \n# The results are better but not perfect)\nbirds2 <- birds[, c(1,3,6,7)]\nbirds2$logarea <- log(birds$area)\nbirds2$logdist <- log(birds$dist)\nbirds2$logldist <- log(birds$ldist)\n\npairs(birds2, gap = 0)\n\n\n\n\n\n\n# 2. Correlation between explanatory variables\nz <- cor(birds2)\nround(z, 2)\n\n         abund yr.isol graze   alt logarea logdist logldist\nabund     1.00    0.50 -0.68  0.39    0.74    0.13     0.12\nyr.isol   0.50    1.00 -0.64  0.23    0.28   -0.02    -0.16\ngraze    -0.68   -0.64  1.00 -0.41   -0.56   -0.14    -0.03\nalt       0.39    0.23 -0.41  1.00    0.28   -0.22    -0.27\nlogarea   0.74    0.28 -0.56  0.28    1.00    0.30     0.38\nlogdist   0.13   -0.02 -0.14 -0.22    0.30    1.00     0.60\nlogldist  0.12   -0.16 -0.03 -0.27    0.38    0.60     1.00\n\n\n\n\n# 3. Best linear model\noptions(na.action = \"na.fail\")\nbirds.fullmodel <- lm(formula = abund ~ ., # The \"~ .\" is shorthand for all other vars\n                      data = birds2)\nbirds.dredge <- dredge(birds.fullmodel, rank = \"AIC\")\n\nhead(birds.dredge, 25)\n\nGlobal model call: lm(formula = abund ~ ., data = birds2)\n---\nModel selection table \n      (Int)     alt    grz   lgr     lgd     lgl  yr.isl df   logLik   AIC\n39 -134.300         -1.902 3.112                 0.07835  5 -180.555 371.1\n40 -141.900 0.02586 -1.601 3.073                 0.07991  6 -179.761 371.5\n55 -113.400         -1.842 3.366         -0.7149 0.06941  6 -180.036 372.1\n47 -120.500         -1.939 3.251 -0.8894         0.07354  6 -180.072 372.1\n7    21.600         -2.854 2.992                          4 -182.257 372.5\n23   25.740         -2.630 3.348         -0.9511          5 -181.347 372.7\n38 -236.700 0.03623        3.540                 0.12480  5 -181.428 372.9\n8    17.280 0.02468 -2.584 2.953                          5 -181.577 373.2\n15   26.630         -2.827 3.169 -1.0750                  5 -181.582 373.2\n48 -131.800 0.02145 -1.676 3.168 -0.5660         0.07658  7 -179.585 373.2\n56 -127.700 0.02081 -1.624 3.234         -0.4331 0.07419  7 -179.597 373.2\n63 -111.800         -1.884 3.368 -0.5458 -0.4805 0.06939  7 -179.908 373.8\n37 -252.200                3.732                 0.13520  4 -182.993 374.0\n24   22.020 0.01615 -2.502 3.245         -0.7451          6 -181.093 374.2\n31   27.280         -2.671 3.350 -0.5475 -0.7159          6 -181.225 374.5\n16   22.110 0.01850 -2.632 3.095 -0.8027                  6 -181.238 374.5\n54 -225.800 0.03206        3.682         -0.3684 0.12050  6 -181.316 374.6\n53 -223.900                4.001         -0.8221 0.12290  5 -182.360 374.7\n46 -233.700 0.03413        3.602 -0.3024         0.12420  6 -181.380 374.8\n64 -125.700 0.01951 -1.668 3.244 -0.3939 -0.2816 0.07387  8 -179.532 375.1\n45 -241.900                3.867 -0.7971         0.13190  5 -182.638 375.3\n32   23.560 0.01474 -2.546 3.256 -0.4328 -0.5771          7 -181.018 376.0\n62 -226.000 0.03180        3.688 -0.1033 -0.3281 0.12070  7 -181.312 376.6\n61 -224.400                4.011 -0.3060 -0.6920 0.12360  6 -182.322 376.6\n6     3.959 0.04862        3.935                          4 -187.342 382.7\n   delta weight\n39  0.00  0.127\n40  0.41  0.103\n55  0.96  0.078\n47  1.04  0.076\n7   1.40  0.063\n23  1.59  0.057\n38  1.75  0.053\n8   2.05  0.046\n15  2.06  0.045\n48  2.06  0.045\n56  2.09  0.045\n63  2.71  0.033\n37  2.88  0.030\n24  3.08  0.027\n31  3.34  0.024\n16  3.37  0.024\n54  3.52  0.022\n53  3.61  0.021\n46  3.65  0.020\n64  3.95  0.018\n45  4.17  0.016\n32  4.93  0.011\n62  5.51  0.008\n61  5.54  0.008\n6  11.57  0.000\nModels ranked by AIC(x) \n\n\n\n\n# 5. Models with delta AIC < 7\nnrow(birds.dredge[birds.dredge$delta < 7, ])\n\n[1] 24\n\n# 6. Akaike weights\nw <- Weights(birds.dredge)\nw\n\nAIC model weights \n   39    40    55    47     7    23    38     8    15    48    56    63    37 \n0.127 0.103 0.078 0.076 0.063 0.057 0.053 0.046 0.045 0.045 0.045 0.033 0.030 \n   24    31    16    54    53    46    64    45    32    62    61     6    22 \n0.027 0.024 0.024 0.022 0.021 0.020 0.018 0.016 0.011 0.008 0.008 0.000 0.000 \n   21    14    30    29     5    13     3    52    20     4    19    35    36 \n0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 \n   51    11    12    60    28    44    27    43    59    50    58    42    34 \n0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 \n   49    33    41    57    18    10    26     2     1     9    17    25 \n0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 \n\n# 7. Models in the 95% \"confidence set\"\nlength(cumsum(w)[cumsum(w)< 0.95]) + 1\n\n[1] 20\n\n# 8. Refit best linear model\nbestmodel <- get.models(birds.dredge, 1)[[1]]\nz <- lm(bestmodel, data = birds2)\nsummary(z)\n\n\nCall:\nlm(formula = bestmodel, data = birds2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.5159  -3.8136   0.2027   3.1271  14.5542 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -134.26065   86.39085  -1.554   0.1262    \ngraze         -1.90216    0.87447  -2.175   0.0342 *  \nlogarea        3.11223    0.55268   5.631 7.32e-07 ***\nyr.isol        0.07835    0.04340   1.805   0.0768 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.311 on 52 degrees of freedom\nMultiple R-squared:  0.6732,    Adjusted R-squared:  0.6544 \nF-statistic: 35.71 on 3 and 52 DF,  p-value: 1.135e-12\n\n\n\n\n# Based on our data dredging, the best model has 3 variables:\n# graze, logarea, and yr.isol\n\n# plot the dependent var against each of these\nvisreg(z, xvar = \"graze\")\n\n\n\n\n\n\nvisreg(z, xvar = \"logarea\")\n\n\n\n\n\n\nvisreg(z, xvar = \"yr.isol\")\n\n\n\n\n\n\n# 9. ANOVA\nAnova(z, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: abund\n             Sum Sq Df F value    Pr(>F)    \n(Intercept)   96.20  1  2.4152   0.12622    \ngraze        188.45  1  4.7315   0.03418 *  \nlogarea     1262.97  1 31.7094 7.316e-07 ***\nyr.isol      129.81  1  3.2590   0.07682 .  \nResiduals   2071.14 52                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nz <- lm(abund ~ ., data = birds2)\n\nz1 <- stepAIC(z, direction = \"both\")\n\nStart:  AIC=214.14\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist\n\n           Df Sum of Sq    RSS    AIC\n- logldist  1      3.80 2000.7 212.25\n- logdist   1      4.68 2001.5 212.27\n- alt       1     27.02 2023.9 212.90\n<none>                  1996.8 214.14\n- yr.isol   1    108.83 2105.7 215.11\n- graze     1    131.07 2127.9 215.70\n- logarea   1   1059.75 3056.6 235.98\n\nStep:  AIC=212.25\nabund ~ yr.isol + graze + alt + logarea + logdist\n\n           Df Sum of Sq    RSS    AIC\n- logdist   1     12.64 2013.3 210.60\n- alt       1     35.12 2035.8 211.22\n<none>                  2000.7 212.25\n- yr.isol   1    121.64 2122.3 213.55\n- graze     1    132.44 2133.1 213.84\n+ logldist  1      3.80 1996.9 214.14\n- logarea   1   1193.04 3193.7 236.44\n\nStep:  AIC=210.6\nabund ~ yr.isol + graze + alt + logarea\n\n           Df Sum of Sq    RSS    AIC\n- alt       1     57.84 2071.1 210.19\n<none>                  2013.3 210.60\n- graze     1    123.48 2136.8 211.94\n- yr.isol   1    134.89 2148.2 212.23\n+ logdist   1     12.64 2000.7 212.25\n+ logldist  1     11.76 2001.5 212.27\n- logarea   1   1227.11 3240.4 235.25\n\nStep:  AIC=210.19\nabund ~ yr.isol + graze + logarea\n\n           Df Sum of Sq    RSS    AIC\n<none>                  2071.1 210.19\n+ alt       1     57.84 2013.3 210.60\n+ logldist  1     38.04 2033.1 211.15\n+ logdist   1     35.36 2035.8 211.22\n- yr.isol   1    129.81 2200.9 211.59\n- graze     1    188.45 2259.6 213.06\n- logarea   1   1262.97 3334.1 234.85\n\n\n\n\n# 11. Model selection using stepAIC()\nz <- lm(abund ~ ., data = birds2)\nz1 <- stepAIC(z, direction = \"both\")\n\nStart:  AIC=214.14\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist\n\n           Df Sum of Sq    RSS    AIC\n- logldist  1      3.80 2000.7 212.25\n- logdist   1      4.68 2001.5 212.27\n- alt       1     27.02 2023.9 212.90\n<none>                  1996.8 214.14\n- yr.isol   1    108.83 2105.7 215.11\n- graze     1    131.07 2127.9 215.70\n- logarea   1   1059.75 3056.6 235.98\n\nStep:  AIC=212.25\nabund ~ yr.isol + graze + alt + logarea + logdist\n\n           Df Sum of Sq    RSS    AIC\n- logdist   1     12.64 2013.3 210.60\n- alt       1     35.12 2035.8 211.22\n<none>                  2000.7 212.25\n- yr.isol   1    121.64 2122.3 213.55\n- graze     1    132.44 2133.1 213.84\n+ logldist  1      3.80 1996.9 214.14\n- logarea   1   1193.04 3193.7 236.44\n\nStep:  AIC=210.6\nabund ~ yr.isol + graze + alt + logarea\n\n           Df Sum of Sq    RSS    AIC\n- alt       1     57.84 2071.1 210.19\n<none>                  2013.3 210.60\n- graze     1    123.48 2136.8 211.94\n- yr.isol   1    134.89 2148.2 212.23\n+ logdist   1     12.64 2000.7 212.25\n+ logldist  1     11.76 2001.5 212.27\n- logarea   1   1227.11 3240.4 235.25\n\nStep:  AIC=210.19\nabund ~ yr.isol + graze + logarea\n\n           Df Sum of Sq    RSS    AIC\n<none>                  2071.1 210.19\n+ alt       1     57.84 2013.3 210.60\n+ logldist  1     38.04 2033.1 211.15\n+ logdist   1     35.36 2035.8 211.22\n- yr.isol   1    129.81 2200.9 211.59\n- graze     1    188.45 2259.6 213.06\n- logarea   1   1262.97 3334.1 234.85\n\nz1\n\n\nCall:\nlm(formula = abund ~ yr.isol + graze + logarea, data = birds2)\n\nCoefficients:\n(Intercept)      yr.isol        graze      logarea  \n -134.26065      0.07835     -1.90216      3.11223  \n\n\n\n\n# 12. Include all two-way interactions\nz <- lm(abund ~ (.)^2, data = birds2)\nz2 <- stepAIC(z, upper = ~., lower = ~1, direction = \"both\")\n\nStart:  AIC=222.63\nabund ~ (yr.isol + graze + alt + logarea + logdist + logldist)^2\n\n                   Df Sum of Sq    RSS    AIC\n- graze:logdist     1     0.003 1360.0 220.63\n- yr.isol:logldist  1     0.026 1360.0 220.63\n- alt:logdist       1     0.058 1360.1 220.64\n- logdist:logldist  1     0.572 1360.6 220.66\n- graze:alt         1     3.100 1363.1 220.76\n- graze:logldist    1     5.262 1365.3 220.85\n- yr.isol:logdist   1    11.826 1371.8 221.12\n- logarea:logdist   1    12.196 1372.2 221.13\n- logarea:logldist  1    20.330 1380.3 221.47\n- alt:logldist      1    20.369 1380.4 221.47\n- alt:logarea       1    24.223 1384.2 221.62\n- graze:logarea     1    26.546 1386.6 221.72\n- yr.isol:alt       1    26.559 1386.6 221.72\n- yr.isol:logarea   1    36.513 1396.5 222.12\n<none>                          1360.0 222.63\n- yr.isol:graze     1   161.328 1521.3 226.91\n\nStep:  AIC=220.63\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    yr.isol:logldist + graze:alt + graze:logarea + graze:logldist + \n    alt:logarea + alt:logdist + alt:logldist + logarea:logdist + \n    logarea:logldist + logdist:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- yr.isol:logldist  1     0.045 1360.1 218.64\n- alt:logdist       1     0.055 1360.1 218.64\n- logdist:logldist  1     0.569 1360.6 218.66\n- graze:alt         1     3.213 1363.2 218.77\n- graze:logldist    1     9.220 1369.2 219.01\n- logarea:logdist   1    13.672 1373.7 219.19\n- alt:logldist      1    20.599 1380.6 219.48\n- logarea:logldist  1    23.052 1383.1 219.58\n- yr.isol:logdist   1    27.360 1387.4 219.75\n- yr.isol:alt       1    27.398 1387.4 219.75\n- graze:logarea     1    27.640 1387.7 219.76\n- alt:logarea       1    29.525 1389.5 219.84\n- yr.isol:logarea   1    37.337 1397.3 220.15\n<none>                          1360.0 220.63\n+ graze:logdist     1     0.003 1360.0 222.63\n- yr.isol:graze     1   194.657 1554.7 226.12\n\nStep:  AIC=218.64\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    graze:alt + graze:logarea + graze:logldist + alt:logarea + \n    alt:logdist + alt:logldist + logarea:logdist + logarea:logldist + \n    logdist:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- alt:logdist       1     0.052 1360.1 216.64\n- logdist:logldist  1     0.527 1360.6 216.66\n- graze:alt         1     3.169 1363.2 216.77\n- graze:logldist    1    14.092 1374.2 217.21\n- logarea:logdist   1    15.428 1375.5 217.27\n- alt:logldist      1    20.608 1380.7 217.48\n- logarea:logldist  1    27.209 1387.3 217.75\n- yr.isol:logdist   1    29.616 1389.7 217.84\n- yr.isol:alt       1    31.735 1391.8 217.93\n- graze:logarea     1    31.869 1391.9 217.93\n- alt:logarea       1    32.099 1392.2 217.94\n- yr.isol:logarea   1    43.797 1403.9 218.41\n<none>                          1360.1 218.64\n+ yr.isol:logldist  1     0.045 1360.0 220.63\n+ graze:logdist     1     0.023 1360.0 220.63\n- yr.isol:graze     1   194.784 1554.8 224.13\n\nStep:  AIC=216.64\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    graze:alt + graze:logarea + graze:logldist + alt:logarea + \n    alt:logldist + logarea:logdist + logarea:logldist + logdist:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- logdist:logldist  1     0.667 1360.8 214.67\n- graze:alt         1     3.471 1363.6 214.78\n- graze:logldist    1    14.281 1374.4 215.22\n- logarea:logdist   1    16.073 1376.2 215.30\n- logarea:logldist  1    29.605 1389.7 215.84\n- alt:logarea       1    32.129 1392.2 215.95\n- yr.isol:alt       1    33.316 1393.4 215.99\n- yr.isol:logdist   1    34.003 1394.1 216.02\n- graze:logarea     1    34.969 1395.1 216.06\n- alt:logldist      1    40.951 1401.1 216.30\n<none>                          1360.1 216.64\n- yr.isol:logarea   1    54.393 1414.5 216.83\n+ alt:logdist       1     0.052 1360.1 218.64\n+ yr.isol:logldist  1     0.042 1360.1 218.64\n+ graze:logdist     1     0.011 1360.1 218.64\n- yr.isol:graze     1   222.578 1582.7 223.13\n\nStep:  AIC=214.67\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    graze:alt + graze:logarea + graze:logldist + alt:logarea + \n    alt:logldist + logarea:logdist + logarea:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- graze:alt         1     2.898 1363.7 212.78\n- graze:logldist    1    18.727 1379.5 213.43\n- logarea:logdist   1    23.605 1384.4 213.63\n- logarea:logldist  1    30.094 1390.9 213.89\n- yr.isol:alt       1    33.061 1393.8 214.01\n- alt:logarea       1    34.491 1395.3 214.07\n- graze:logarea     1    38.287 1399.1 214.22\n- alt:logldist      1    41.609 1402.4 214.35\n- yr.isol:logdist   1    41.909 1402.7 214.37\n<none>                          1360.8 214.67\n- yr.isol:logarea   1    57.207 1418.0 214.97\n+ logdist:logldist  1     0.667 1360.1 216.64\n+ alt:logdist       1     0.192 1360.6 216.66\n+ graze:logdist     1     0.008 1360.8 216.67\n+ yr.isol:logldist  1     0.000 1360.8 216.67\n- yr.isol:graze     1   223.303 1584.1 221.18\n\nStep:  AIC=212.79\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    graze:logarea + graze:logldist + alt:logarea + alt:logldist + \n    logarea:logdist + logarea:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- logarea:logdist   1    25.007 1388.7 211.80\n- graze:logldist    1    28.377 1392.1 211.94\n- logarea:logldist  1    32.029 1395.7 212.09\n- yr.isol:alt       1    33.892 1397.6 212.16\n- alt:logldist      1    38.726 1402.4 212.35\n- alt:logarea       1    45.754 1409.4 212.63\n- yr.isol:logdist   1    46.139 1409.8 212.65\n- graze:logarea     1    49.541 1413.2 212.78\n<none>                          1363.7 212.78\n- yr.isol:logarea   1    74.795 1438.5 213.78\n+ graze:alt         1     2.898 1360.8 214.67\n+ alt:logdist       1     0.419 1363.3 214.77\n+ graze:logdist     1     0.238 1363.4 214.78\n+ logdist:logldist  1     0.095 1363.6 214.78\n+ yr.isol:logldist  1     0.017 1363.7 214.78\n- yr.isol:graze     1   222.021 1585.7 219.23\n\nStep:  AIC=211.8\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    graze:logarea + graze:logldist + alt:logarea + alt:logldist + \n    logarea:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- logarea:logldist  1     7.647 1396.3 210.11\n- graze:logldist    1    13.313 1402.0 210.34\n- yr.isol:logdist   1    23.990 1412.7 210.76\n- yr.isol:alt       1    29.485 1418.2 210.98\n- alt:logldist      1    32.060 1420.8 211.08\n- alt:logarea       1    37.527 1426.2 211.30\n- graze:logarea     1    46.646 1435.3 211.65\n<none>                          1388.7 211.80\n- yr.isol:logarea   1    67.711 1456.4 212.47\n+ logarea:logdist   1    25.007 1363.7 212.78\n+ graze:logdist     1    11.313 1377.4 213.34\n+ yr.isol:logldist  1     6.990 1381.7 213.52\n+ logdist:logldist  1     5.412 1383.3 213.58\n+ graze:alt         1     4.301 1384.4 213.63\n+ alt:logdist       1     4.282 1384.4 213.63\n- yr.isol:graze     1   212.098 1600.8 217.76\n\nStep:  AIC=210.11\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    graze:logarea + graze:logldist + alt:logarea + alt:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- graze:logldist    1     8.526 1404.9 208.45\n- yr.isol:logdist   1    20.997 1417.3 208.95\n- yr.isol:alt       1    28.937 1425.3 209.26\n- alt:logldist      1    30.792 1427.1 209.33\n- alt:logarea       1    32.128 1428.5 209.38\n<none>                          1396.3 210.11\n- graze:logarea     1    57.851 1454.2 210.38\n+ graze:logdist     1    12.848 1383.5 211.59\n+ yr.isol:logldist  1     7.749 1388.6 211.80\n+ logarea:logldist  1     7.647 1388.7 211.80\n+ graze:alt         1     4.856 1391.5 211.91\n+ alt:logdist       1     4.769 1391.6 211.92\n- yr.isol:logarea   1   100.005 1496.3 211.98\n+ logdist:logldist  1     0.651 1395.7 212.08\n+ logarea:logdist   1     0.626 1395.7 212.09\n- yr.isol:graze     1   244.977 1641.3 217.16\n\nStep:  AIC=208.45\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + yr.isol:logdist + \n    graze:logarea + alt:logarea + alt:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- yr.isol:logdist   1    20.776 1425.6 207.27\n- yr.isol:alt       1    25.024 1429.9 207.44\n- alt:logldist      1    39.643 1444.5 208.01\n- graze:logarea     1    49.331 1454.2 208.38\n<none>                          1404.9 208.45\n- alt:logarea       1    53.564 1458.4 208.55\n+ graze:logdist     1    19.974 1384.9 209.65\n+ graze:alt         1     9.823 1395.0 210.06\n+ graze:logldist    1     8.526 1396.3 210.11\n+ logarea:logldist  1     2.860 1402.0 210.34\n+ alt:logdist       1     2.383 1402.5 210.36\n+ logdist:logldist  1     2.145 1402.7 210.37\n+ logarea:logdist   1     0.610 1404.2 210.43\n+ yr.isol:logldist  1     0.046 1404.8 210.45\n- yr.isol:logarea   1   116.372 1521.2 210.91\n- yr.isol:graze     1   260.447 1665.3 215.97\n\nStep:  AIC=207.27\nabund ~ yr.isol + graze + alt + logarea + logdist + logldist + \n    yr.isol:graze + yr.isol:alt + yr.isol:logarea + graze:logarea + \n    alt:logarea + alt:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- logdist           1     8.091 1433.7 205.59\n- alt:logldist      1    38.442 1464.1 206.76\n- graze:logarea     1    44.691 1470.3 207.00\n- alt:logarea       1    50.969 1476.6 207.24\n<none>                          1425.6 207.27\n- yr.isol:alt       1    54.739 1480.4 207.38\n+ yr.isol:logdist   1    20.776 1404.9 208.45\n+ graze:alt         1    13.009 1412.6 208.76\n+ graze:logldist    1     8.305 1417.3 208.95\n- yr.isol:logarea   1    98.416 1524.0 209.01\n+ logdist:logldist  1     3.953 1421.7 209.12\n+ logarea:logldist  1     1.296 1424.3 209.22\n+ alt:logdist       1     0.689 1425.0 209.25\n+ yr.isol:logldist  1     0.535 1425.1 209.25\n+ graze:logdist     1     0.225 1425.4 209.26\n+ logarea:logdist   1     0.187 1425.5 209.27\n- yr.isol:graze     1   253.153 1678.8 214.43\n\nStep:  AIC=205.59\nabund ~ yr.isol + graze + alt + logarea + logldist + yr.isol:graze + \n    yr.isol:alt + yr.isol:logarea + graze:logarea + alt:logarea + \n    alt:logldist\n\n                   Df Sum of Sq    RSS    AIC\n- alt:logldist      1    38.160 1471.9 205.06\n- graze:logarea     1    42.558 1476.3 205.23\n- alt:logarea       1    47.587 1481.3 205.42\n<none>                          1433.7 205.59\n- yr.isol:alt       1    53.683 1487.4 205.65\n+ graze:logldist    1    11.379 1422.3 207.14\n+ graze:alt         1    10.881 1422.8 207.16\n- yr.isol:logarea   1    95.540 1529.3 207.20\n+ logdist           1     8.091 1425.6 207.27\n+ logarea:logldist  1     2.582 1431.2 207.49\n+ yr.isol:logldist  1     0.039 1433.7 207.59\n- yr.isol:graze     1   247.203 1680.9 212.50\n\nStep:  AIC=205.06\nabund ~ yr.isol + graze + alt + logarea + logldist + yr.isol:graze + \n    yr.isol:alt + yr.isol:logarea + graze:logarea + alt:logarea\n\n                   Df Sum of Sq    RSS    AIC\n- logldist          1     4.777 1476.7 203.24\n- alt:logarea       1    21.891 1493.8 203.89\n- yr.isol:alt       1    32.877 1504.8 204.30\n<none>                          1471.9 205.06\n+ alt:logldist      1    38.160 1433.7 205.59\n- graze:logarea     1    73.770 1545.7 205.80\n+ graze:logldist    1    20.795 1451.1 206.26\n+ logdist           1     7.810 1464.1 206.76\n+ graze:alt         1     3.830 1468.1 206.91\n+ yr.isol:logldist  1     2.381 1469.5 206.97\n+ logarea:logldist  1     0.897 1471.0 207.03\n- yr.isol:logarea   1   173.397 1645.3 209.30\n- yr.isol:graze     1   242.803 1714.7 211.61\n\nStep:  AIC=203.24\nabund ~ yr.isol + graze + alt + logarea + yr.isol:graze + yr.isol:alt + \n    yr.isol:logarea + graze:logarea + alt:logarea\n\n                  Df Sum of Sq    RSS    AIC\n- alt:logarea      1    18.904 1495.6 201.96\n- yr.isol:alt      1    28.886 1505.5 202.33\n<none>                         1476.7 203.24\n- graze:logarea    1    79.752 1556.4 204.19\n+ graze:alt        1     6.075 1470.6 205.01\n+ logldist         1     4.777 1471.9 205.06\n+ logdist          1     2.067 1474.6 205.16\n- yr.isol:logarea  1   177.378 1654.0 207.59\n- yr.isol:graze    1   248.142 1724.8 209.94\n\nStep:  AIC=201.95\nabund ~ yr.isol + graze + alt + logarea + yr.isol:graze + yr.isol:alt + \n    yr.isol:logarea + graze:logarea\n\n                  Df Sum of Sq    RSS    AIC\n- yr.isol:alt      1     18.62 1514.2 200.65\n<none>                         1495.6 201.96\n+ alt:logarea      1     18.90 1476.7 203.24\n+ graze:alt        1     18.26 1477.3 203.27\n+ logdist          1      2.03 1493.5 203.88\n+ logldist         1      1.79 1493.8 203.89\n- yr.isol:graze    1    229.47 1725.0 207.95\n- yr.isol:logarea  1    259.20 1754.8 208.91\n- graze:logarea    1    330.71 1826.3 211.14\n\nStep:  AIC=200.65\nabund ~ yr.isol + graze + alt + logarea + yr.isol:graze + yr.isol:logarea + \n    graze:logarea\n\n                  Df Sum of Sq    RSS    AIC\n- alt              1     32.19 1546.4 199.83\n<none>                         1514.2 200.65\n+ yr.isol:alt      1     18.62 1495.6 201.96\n+ alt:logarea      1      8.64 1505.5 202.33\n+ logdist          1      3.31 1510.9 202.53\n+ graze:alt        1      0.36 1513.8 202.63\n+ logldist         1      0.27 1513.9 202.64\n- yr.isol:logarea  1    241.21 1755.4 206.93\n- yr.isol:graze    1    243.50 1757.7 207.00\n- graze:logarea    1    332.95 1847.1 209.78\n\nStep:  AIC=199.83\nabund ~ yr.isol + graze + logarea + yr.isol:graze + yr.isol:logarea + \n    graze:logarea\n\n                  Df Sum of Sq    RSS    AIC\n<none>                         1546.4 199.83\n+ alt              1     32.19 1514.2 200.65\n+ logldist         1      6.57 1539.8 201.59\n+ logdist          1      0.01 1546.4 201.83\n- yr.isol:logarea  1    239.74 1786.1 205.90\n- yr.isol:graze    1    272.21 1818.6 206.91\n- graze:logarea    1    334.74 1881.1 208.80\n\n\n\n\n# 13. Estimate the coefficients of the best-fitting model.\nsummary(z2)\n\n\nCall:\nlm(formula = abund ~ yr.isol + graze + logarea + yr.isol:graze + \n    yr.isol:logarea + graze:logarea, data = birds2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.1445  -2.8572  -0.3756   2.6846  11.6416 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)   \n(Intercept)     1012.42110  344.55122   2.938  0.00502 **\nyr.isol           -0.50272    0.17527  -2.868  0.00607 **\ngraze           -204.03353   67.93366  -3.003  0.00420 **\nlogarea         -145.27312   53.17222  -2.732  0.00873 **\nyr.isol:graze      0.10176    0.03465   2.937  0.00504 **\nyr.isol:logarea    0.07438    0.02699   2.756  0.00819 **\ngraze:logarea      1.22433    0.37593   3.257  0.00205 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.618 on 49 degrees of freedom\nMultiple R-squared:  0.756, Adjusted R-squared:  0.7261 \nF-statistic:  25.3 on 6 and 49 DF,  p-value: 1.938e-13\n\n\n\n\n# 14. AIC comparison\nAIC(z1, z2)\n\n   df      AIC\nz1  5 371.1092\nz2  8 360.7471"
  },
  {
    "objectID": "lab15-model-selection.html#harper-adams-data-science",
    "href": "lab15-model-selection.html#harper-adams-data-science",
    "title": "Lab 15 Model selection",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The material in this module is designed to be experienced in an intensive one week format followed by an assessment meant to showcase reproducible statistical analysis skills. For enrolled students, the work will be supported with several live sessions during the main week of delivery.\n\n\n\n\nDay\nTopics\nLabs\n\n\n\n\nMon\nam\npm\n 00 Module overview\n 01 Introduction part 1 \n 02 Graph basics \n 03 Describing data \n 04 Estimation \n 05 Probability \nLab 01 Graphing I\nLab 02 Describing data\nLab 03 Frequency data\n\n\nTues\nam\npm\n 06 Hypothesis testing \n 07 Analyzing proportion data \n 08 Discrete data \n 09 Contingency tables \n 10 Gaussian distribution \n 11 One-sample tests \nLab 04 Contingency analysis\nLab 05 The Gaussian dist\nLab 06 2 group tests\n\n\nWed\nam\npm\n 12 Two sample tests  vid1  vid2\n 13 Testing assumptions  vid1  vid2\n 14 Experimental design I \n 15 ANOVA  vid1  vid2  vid3\n 16 Correlation \nLab 07 ANOVA\nLab 08 Cor and reg\nLab 09 Graphing II\n\n\nThurs\nam\npm\n 17 Regression  vid1  vid2\n 18 Data and reproducibility \n 19 Graphing II \n 20 Experimental design II \nLab 10 Sample size\nLab 11 Linear models\nLab 12 Mixed effects\n\n\nFri\nam\npm\n 21 Linear models  vid1  vid2\n 22 Mixed effects models \n 23 Likelihood \n 24 Generalized linear model \n 25 Model selection \nLab 13 Likelihood\nLab 14 GLM\nLab 15 Model selection"
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  }
]