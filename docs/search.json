[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The material in this module is designed to be experienced in an intensive one week format followed by an assessment meant to showcase reproducible statistical analysis skills. For enrolled students, the work will be supported with several live sessions during the main week of delivery.\n\n\n\n\nDay\nTopics\nLabs\n\n\n\n\nMon\nam\npm\n 00 Module overview\n 01 Introduction part 1 \n 02 Graph basics \n 03 Describing data \n 04 Estimation \n 05 Probability \nLab 01 Graphing I\nLab 02 Describing data\nLab 03 Frequency data\n\n\nTues\nam\npm\n 06 Hypothesis testing \n 07 Analyzing proportion data \n 08 Discrete data \n 09 Contingency tables \n 10 Gaussian distribution \n 11 One-sample tests \nLab 04 Contingency analysis\nLab 05 The Gaussian dist\nLab 06 2 group tests\n\n\nWed\nam\npm\n 12 Two sample tests  vid1  vid2\n 13 Testing assumptions  vid1  vid2\n 14 Experimental design I \n 15 ANOVA  vid1  vid2  vid3\n 16 Correlation \nLab 07 ANOVA\nLab 08 Cor and reg\nLab 09 Graphing II\n\n\nThurs\nam\npm\n 17 Regression  vid1  vid2\n 18 Data and reproducibility \n 19 Graphing II \n 20 Experimental design II \nLab 10 Sample size\nLab 11 Linear models\nLab 12 Mixed effects\n\n\nFri\nam\npm\n 21 Linear models  vid1  vid2\n 22 Mixed effects models \n 23 Likelihood \n 24 Generalized linear model \n 25 Model selection \nLab 13 Likelihood\nLab 14 GLM\nLab 15 Model selection"
  },
  {
    "objectID": "schedule.html#harper-adams-data-science",
    "href": "schedule.html#harper-adams-data-science",
    "title": "Schedule",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab01-graph1.html#harper-adams-data-science",
    "href": "lab01-graph1.html#harper-adams-data-science",
    "title": "Lab 01 Graphs I",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab02-descriptives.html#harper-adams-data-science",
    "href": "lab02-descriptives.html#harper-adams-data-science",
    "title": "Lab 02 Describing data",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab01-graph1.html",
    "href": "lab01-graph1.html",
    "title": "Lab 01 Graphs I",
    "section": "",
    "text": "{width = “400px”}"
  },
  {
    "objectID": "lab01-graph1.html#objectives",
    "href": "lab01-graph1.html#objectives",
    "title": "Lab 01 Graphs I",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nReview some common graph formats and when they are used\nMake graphs in R, such as histograms, bar charts, box plots, and scatter plots\nCritique basic graphs to improve readability and accurate communication\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab01-graph1.html#start-a-script",
    "href": "lab01-graph1.html#start-a-script",
    "title": "Lab 01 Graphs I",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab01-graph1.html#learning-the-tools",
    "href": "lab01-graph1.html#learning-the-tools",
    "title": "Lab 01 Graphs I",
    "section": "3 Learning the tools",
    "text": "3 Learning the tools\n\n3.1 {ggplot2} package\nThe function ggplot() allows us to graph most kinds of data relatively simply. The syntax is slightly odd but very flexible. We’ll practice specific commands for several types of plots below.\nTo begin, remember to install ggplot2 if need to and then load ggplot2 with the library() function:\n\n# Neat trick for checking whether a package is installed\n# and installing it if it is not, loading it if it is...\n\nif(!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n# The \"hard\" way\n# install.packages(\"ggplot2\", dep = T) # run if needed...\n\nlibrary(ggplot2)\n\n\nGraphing in {ggplot2} is slightly different than with the base R plot functions (like plot(), boxplot(), hist(), etc.). To make a graph with ggplot(), you need to specify at least two elements in your command.\n\nFirst part: Use the function ggplot() itself, to specify which data frame you want to use and also which variables are to be plotted.\nSecond part: Tell R what kind of graph to make using a geom() function.\n\nThe odd part is that these two parts are put together with a + sign. It’s simplest to see this with an example. We’ll draw a histogram with ggplot() in the next section.\n\n\n\n3.2 Histograms\nA histogram represents the frequency distribution of a numerical variable in a sample.\nLet’s see how to make a basic histogram using the age data from the Titanic data set. Make sure you have loaded the data (using read.csv()) into a data frame called titanicData.\n\n# Example code to load the data\n# NB you will probably have to supply your own file path\ntitanic <- read.csv(\"data/titanic.csv\")\n\n# Code to make a simple histogram of age:\n\nggplot(titanic, aes(x=age)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 680 rows containing non-finite values (stat_bin).\n\n\n\n\n# There are a couple of informative warnings, but we can ignore them\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n## Warning: Removed 680 rows containing non-finite values (stat_bin).\n\n\nggplot()\nNotice that there are two functions called here, put together in a single command with a plus sign. The first function is ggplot(), and it has two input arguments. Listed first is titanicData; this is the name of the data frame containing the variables that we want to graph. The second input to ggplot is an aes() function. In this case, the aes() function tells R that we want Age to be the x-variable (i.e. the variable that is displayed along the x-axis). (The “aes” stands for “aesthetics”,” but if you’re like me it won’t necessarily help you remember it any better)\ngeom_histogram()\nThe second function in this command is geom_histogram(). This is the part that tells R that the “geometry” of our plot should be a histogram.\nThis is not the most beautiful graph in the world, but it conveys the information. Later, we’ll see a couple of options that can make a ggplot graph look a little better.\n\n\n\n3.3 Bar graphs\nA bar graph plots the frequency distribution of a categorical variable.\nIn ggplot(), the syntax for a bar graph is very similar to that for a histogram. For example, here is a bar graph for the categorical variable Sex in the titanic data set. Aside from specifying a different variable for x, we use a different geom function here, geom_bar.\n\nggplot(titanic, aes(x=sex)) + geom_bar(stat=\"count\")\n\n\n\n\n\n\n\n3.4 Boxplots\nA boxplot is a convenient way of showing the “frequency distribution”central tendency” of data for a numerical variable in one or more groups. Here’s the code to draw a boxplot for age in the titanic data set, separately for each sex:\n\nggplot(titanic, aes(x=sex, y=age)) + geom_boxplot()\n\nWarning: Removed 680 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nNotice that the y variable here is age, and x is the categorical variable sex that winds up on the x-axis. See the result below, and look at where the variables are. The other new feature here is the new geom function, geom_boxplot().\nHere the thick bar in the middle of each boxplot is the median of that group. The upper and lower bounds of the box extend from the first to the third quartile. The vertical lines are called whiskers, and they cover most of the range of the data (except when data points are pretty far from the median, when they are plotted as individual dots, as on the male boxplot).\n\n\n\n3.5 Scatterplots\nThe last graphical style that we will mention here is the scatter plot, which shows the relationship between two numerical variables.\nWe will use a dataset showing the relationship between the ornamentation of father guppies and the sexual attractiveness of their sons, using the file guppy-attractiveness.csv.\n\n# Example code to load the data\n# NB you will probably have to supply your own file path\nguppy <-read.csv(\"data/guppy-attractiveness.csv\", header =T)\n\nhead(guppy)\n\n  fatherOrnamentation sonAttractiveness\n1                0.35             -0.32\n2                0.03             -0.03\n3                0.14              0.11\n4                0.10              0.28\n5                0.22              0.31\n6                0.23              0.18\n\n\n\nTo make a scatter plot of the variables fatherOrnamentation and sonAttractiveness with ggplot, you need to specify the x and y variables, and use geom_point():\n\n\nggplot(guppy,  \n      aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n  geom_point()"
  },
  {
    "objectID": "lab01-graph1.html#better-looking-graphics-with-options",
    "href": "lab01-graph1.html#better-looking-graphics-with-options",
    "title": "Lab 01 Graphs I",
    "section": "4 Better looking graphics with options",
    "text": "4 Better looking graphics with options\n\n4.1 Sexing up the sexual selection scatterplot\nThe code we tried above for {ggplot2} does not begin to scratch the surface of what ggplot and R are capable of. Not only are there far more choices about the kinds of plots available, but there are many, many options for customizing the look and feel of each graph. You can choose the font, the font size, the colors, the style of the axes labels, etc., and you can customize the legends and axes legends nearly as much as you want.\nLet’s dig a little deeper into just a few options that you can add to any of the graphs to improve them. For example, you can change the text of the x-axis label or the y-axis label by using xlab or ylab. Let’s do that for the scatterplot, to make the labels a little nicer to read for humans.\n\n\n# better axes\nggplot(guppy,   \n    aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n    geom_point() +\n    xlab(\"Father ornamentation\") + \n    ylab(\"Son attractiveness\")\n\n\n\n\n\nThe can see the labels included in quotes inside the xlab and ylab arguments now appear on your graph.\nIt can also be nice to remove the default gray background, to make what some feel is a cleaner graph. Try adding + theme_classic() to the end of one of your lines of code making a graph, to see whether you prefer the result to the default design. Also, you could add a title by adding the labs function and setting the main argument to a character string + labs(title = \"your title\").\n\n\nggplot(guppy,   \n    aes(x = fatherOrnamentation, y = sonAttractiveness)) +\n    geom_point() +\n    xlab(\"Father ornamentation\") + \n    ylab(\"Son attractiveness\") +\n    theme_classic() +\n    labs(title = \"Ooooo, now that is classic!\")"
  },
  {
    "objectID": "lab01-graph1.html#getting-help",
    "href": "lab01-graph1.html#getting-help",
    "title": "Lab 01 Graphs I",
    "section": "5 Getting help",
    "text": "5 Getting help\nThe help pages in R (using help() with the name of the function you would like help with) are the main source of help, but the amount of detail takes getting used to. For example, to explore the options for ggplot(), enter the following into the R Console.\n\nhelp(ggplot)\nThis will cause the contents of the help page for this function to appear in the Help window in RStudio. These manual pages are often technical, but it is useful with a little practice."
  },
  {
    "objectID": "lab01-graph1.html#exercises",
    "href": "lab01-graph1.html#exercises",
    "title": "Lab 01 Graphs I",
    "section": "6 Exercises",
    "text": "6 Exercises\n\nMake a script in RStudio that collects all your R code required to answer the following questions. Include answers to the qualitative questions using comments.\n\n)\n\n6.1\nFor each of the following pairs of graphs, identify features that communicate better on one version than the other.\nSurvivorship as a function of sex for passengers of the RMS Titanic\n\n\n\n\n\n\nEar length in male humans as a function of age\n\n\n\n\n6.2\nLet’s use the data from countries.csv to practice making some graphs.\nRead the data from the file “countries.csv” in the Data folder.\nUse ggplot() (Make sure that you have run library(ggplot2)).\nMake a histogram to show the frequency distribution of values for measles_immunization_oneyearolds, a numerical variable. (This variable gives the percentage of 1-year-olds that have been vaccinated against measles.) Describe the pattern that you see.\nMake a bar graph to show the numbers of countries in each of the continents. (The categorical variable continent indicates the continent to which countries belong.)\nDraw a scatterplot that shows the relationship between the two numerical variables life_expectancy_at_birth_male and life_expectancy_at_birth_female.\n\n\n\n6.3\nThe ecological footprint is a widely-used measure of the impact a person has on the planet. It measures the area of land (in hectares) required to generate the food, shelter, and other resources used by a typical person and required to dispose of that person’s wastes. Larger values of the ecological footprint indicate that the typical person from that country uses more resources.\nThe countries data set has two variables for many countries showing the ecological footprint of an average person in each country. ecological_footprint_2000 and ecological_footprint_2012 show the ecological footprints for the years 2000 and 2012, respectively.\nMake a scatterplot to plot the relationship between the ecological footprint of 2000 and of 2012.\nDescribe the relationship between the footprints for the two years. Does the value of ecological footprint of 2000 seem to predict anything about its value in 2012?\nFrom this graph, does the ecological footprint tend to go up or down in the years between 2000 and 2012? Did the countries with high or low ecological footprint change the most over this time? (Hint: you can add a one-to-one line to your graph by adding + geom_abline(intercept = 0, slope = 1) to your ggplot() command. This will make it easier to see when your points are above or below the “line of equivalence”).\n\n\n\n6.4\nUse the countries data again. Plot the relationship between continent and female life expectancy at birth. Describe the patterns that you see.\n\n\n\n6.5 Muchhala (2006) measured the length of the tongues of eleven different species of South American bats, as well as the length of their palates (to get an indication of the size of their mouths). All of these bats use their tongues to feed on nectar from flowers. Data from the article are given in the file BatTongues.csv. In this file, both Tongue Length and Palette Length are given in millimeters.\nImport the data and inspect it using summary(). You can call the data set whatever you like, but in one of the later steps we’ll assume it is called bat_tongues. Each value for tongue length and palate length is a species mean, calculated from a sample of individuals per species.\nDraw a scatter plot to show the association between palate length and tongue length, with tongue length as the response variable. Describe the association: is it positive or negative? Is it strong or weak?\nAll of the data points that went into this graph have been double checked and verified. With that in mind, what conclusion can you draw from the outlier (the point with an unusual value) on the scatter plot?\nLet’s figure out which species is the outlier. To do this, we’ll use a useful function called filter() from the package {dplyr}. Use library() to load dplyr to your R session.\nThe function filter() gives us the row (or rows) of a data frame that has a certain property. Looking at the graph, we can tell that the point we are interested in has a very long tongue length, at least over 80 mm long! The following command will pull out the rows of the data frame bat_tongues that have tongue_length greater than 80 mm:\n\nbat_tongues <- read.csv('data/BatTongues.csv')\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# useful\nfilter(bat_tongues,tongue_length>80)\n\n           species palate_length tongue_length\n1 Anoura fistulata          12.4          85.2\n\n\n The unusual species is Anoura fistulata See a photo here. This species has an outrageously long tongue, which it uses to collect nectar from a particular flower (can you guess what feature of the flower has led to the evolution of such a long tongue?). See the article by Muchhala (2006) to learn more about the biology of this strange bat.\n\n\n\n6.6 Pick one of the plots you made using R today. What could be improved about this graph to make it a more effective presentation of the data?"
  },
  {
    "objectID": "lab02-descriptives.html#objectives",
    "href": "lab02-descriptives.html#objectives",
    "title": "Lab 02 Describing data",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nInvestigate sampling error; see that larger samples have less sampling error\nVisualize confidence intervals\nCalculate basic summary statistics using R\nCalculate confidence intervals for the mean with R\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab02-descriptives.html#start-a-script",
    "href": "lab02-descriptives.html#start-a-script",
    "title": "Lab 02 Describing data",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab02-descriptives.html#missing-data",
    "href": "lab02-descriptives.html#missing-data",
    "title": "Lab 02 Describing data",
    "section": "3 Missing data",
    "text": "3 Missing data\nSometimes we do not have all variables measured on all individuals in the data set. When this happens, we need a space holder in our data files so that R knows that the data is missing. The standard way of doing this in R is to put NA in the location that the data would have gone. NA is short for “not available”.\nFor example, in the Titanic data set, we do not know the age of several passengers. Let’s look at it. Load the Titanic data set:\n\n# Load data\n# NB your file path may be different than mine\ntitanicData <- read.csv(\"data/titanic.csv\" )\n\n\nHave R print out the vector of the age variable, which you can do easily by just typing its name:\n\n# print the variable age\ntitanicData$age\n\n   [1] 29.0000  2.0000 30.0000 25.0000  0.9167 47.0000 63.0000 39.0000 58.0000\n  [10] 71.0000 47.0000 19.0000      NA      NA      NA 50.0000 24.0000 36.0000\n  [19] 37.0000 47.0000 26.0000 25.0000 25.0000 19.0000 28.0000 45.0000 39.0000\n  [28] 30.0000 58.0000      NA 45.0000 22.0000      NA 41.0000 48.0000      NA\n  [37] 44.0000 59.0000 60.0000 45.0000      NA 53.0000 58.0000 36.0000 33.0000\n  [46]      NA      NA 36.0000 36.0000 14.0000 11.0000 49.0000      NA 36.0000\n  [55]      NA 46.0000 47.0000 27.0000 31.0000      NA      NA      NA      NA\n  [64] 27.0000 26.0000      NA      NA 64.0000 37.0000 39.0000 55.0000      NA\n  [73] 70.0000 69.0000 36.0000 39.0000 38.0000      NA 27.0000 31.0000 27.0000\n  [82]      NA 31.0000 17.0000      NA      NA  4.0000 27.0000 50.0000 48.0000\n  [91] 49.0000 48.0000 39.0000 23.0000 53.0000 36.0000      NA      NA 30.0000\n [100] 24.0000 19.0000 28.0000 23.0000 64.0000 60.0000      NA 49.0000      NA\n [109] 44.0000 22.0000 60.0000 48.0000 37.0000 35.0000 47.0000 22.0000 45.0000\n [118] 49.0000      NA 71.0000 54.0000 38.0000 19.0000 58.0000 45.0000 23.0000\n [127] 46.0000 25.0000 21.0000 48.0000 49.0000 45.0000 36.0000      NA 55.0000\n [136] 52.0000 24.0000      NA      NA      NA 16.0000 44.0000 51.0000 42.0000\n [145] 35.0000 35.0000 38.0000 35.0000      NA 50.0000 49.0000 46.0000      NA\n [154] 58.0000 41.0000      NA 42.0000 40.0000      NA      NA      NA 42.0000\n [163] 55.0000 50.0000 16.0000      NA 29.0000 21.0000 30.0000 15.0000 30.0000\n [172]      NA      NA      NA 46.0000 54.0000 36.0000 28.0000      NA 65.0000\n [181] 33.0000 44.0000 37.0000      NA 55.0000 47.0000 36.0000 58.0000 31.0000\n [190] 23.0000 19.0000 64.0000      NA 64.0000 22.0000 28.0000      NA      NA\n [199] 22.0000      NA      NA 18.0000 17.0000 52.0000 46.0000 56.0000      NA\n [208]      NA 43.0000 31.0000      NA      NA 33.0000      NA 27.0000 55.0000\n [217] 54.0000      NA 61.0000 48.0000 18.0000 13.0000 21.0000      NA      NA\n [226]      NA 34.0000 40.0000 36.0000 50.0000 39.0000 56.0000 28.0000 56.0000\n [235] 56.0000 24.0000 18.0000      NA 24.0000 23.0000 45.0000 40.0000  6.0000\n [244] 57.0000      NA 32.0000 62.0000 54.0000 43.0000 52.0000      NA 62.0000\n [253] 67.0000 63.0000 61.0000 46.0000 52.0000 39.0000 18.0000 48.0000      NA\n [262] 49.0000 39.0000 17.0000 46.0000      NA 31.0000      NA 61.0000 47.0000\n [271] 64.0000 60.0000 60.0000 55.0000 54.0000 21.0000 57.0000 45.0000 31.0000\n [280] 50.0000 50.0000 27.0000 20.0000 51.0000      NA 21.0000      NA      NA\n [289] 36.0000      NA      NA      NA      NA      NA      NA      NA      NA\n [298]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [307] 40.0000      NA      NA 32.0000      NA      NA      NA      NA      NA\n [316]      NA 33.0000      NA      NA      NA      NA      NA 30.0000 28.0000\n [325] 18.0000      NA 34.0000 32.0000 57.0000 18.0000 23.0000 36.0000 28.0000\n [334] 51.0000 32.0000 19.0000 28.0000 36.0000  4.0000  1.0000 12.0000 34.0000\n [343] 19.0000 23.0000 26.0000      NA 27.0000 15.0000 45.0000 40.0000 20.0000\n [352] 25.0000 36.0000 25.0000      NA 42.0000 26.0000 26.0000  0.8333 31.0000\n [361]      NA 19.0000 54.0000 44.0000 52.0000 30.0000 30.0000      NA      NA\n [370] 29.0000      NA 29.0000 27.0000 24.0000 35.0000 31.0000  8.0000 22.0000\n [379] 30.0000      NA 20.0000      NA 21.0000 49.0000  8.0000 28.0000 18.0000\n [388]      NA 28.0000 22.0000 25.0000 18.0000 32.0000 18.0000      NA 42.0000\n [397] 34.0000  8.0000      NA      NA 23.0000 21.0000 19.0000      NA      NA\n [406]      NA 38.0000      NA 38.0000 35.0000 35.0000 38.0000 24.0000 16.0000\n [415] 26.0000 45.0000 24.0000 21.0000 22.0000      NA 34.0000 30.0000 50.0000\n [424] 30.0000 23.0000  1.0000 44.0000 28.0000  6.0000 30.0000      NA 43.0000\n [433] 45.0000  7.0000 24.0000 24.0000 49.0000 48.0000      NA 34.0000 32.0000\n [442] 21.0000 18.0000 53.0000 23.0000 21.0000      NA 52.0000 42.0000 36.0000\n [451] 21.0000 41.0000      NA      NA 33.0000 17.0000      NA      NA      NA\n [460]      NA      NA      NA 23.0000 34.0000      NA 22.0000      NA      NA\n [469] 45.0000      NA      NA 31.0000 30.0000 26.0000      NA 34.0000 26.0000\n [478] 22.0000  1.0000  3.0000      NA      NA      NA 25.0000      NA 48.0000\n [487]      NA 57.0000      NA      NA      NA  2.0000      NA 27.0000 19.0000\n [496] 30.0000 20.0000 45.0000      NA 46.0000 41.0000 13.0000 19.0000 30.0000\n [505] 48.0000 71.0000 54.0000      NA      NA 64.0000 32.0000 18.0000  2.0000\n [514] 32.0000  3.0000 26.0000 19.0000      NA 20.0000 29.0000 39.0000 22.0000\n [523]      NA 24.0000      NA 28.0000      NA 50.0000 20.0000 40.0000 42.0000\n [532] 21.0000 32.0000 34.0000      NA      NA 33.0000  2.0000  8.0000 36.0000\n [541] 34.0000 30.0000 28.0000 23.0000  0.8333 25.0000  3.0000 50.0000      NA\n [550] 21.0000      NA      NA 25.0000 18.0000 20.0000 30.0000 59.0000 30.0000\n [559] 35.0000 22.0000      NA 25.0000 41.0000 25.0000 14.0000 50.0000 22.0000\n [568]      NA 27.0000 29.0000 27.0000 30.0000 22.0000 35.0000 30.0000 28.0000\n [577] 23.0000      NA 12.0000 40.0000 36.0000 28.0000 32.0000 29.0000  4.0000\n [586]  2.0000      NA      NA 36.0000 33.0000      NA      NA      NA 32.0000\n [595]      NA      NA 26.0000      NA 30.0000 24.0000      NA 18.0000 42.0000\n [604] 13.0000 16.0000 35.0000 16.0000 25.0000 18.0000 20.0000 30.0000 26.0000\n [613] 40.0000 24.0000 41.0000 18.0000  0.8333 23.0000 20.0000 25.0000 35.0000\n [622] 17.0000 32.0000 20.0000 39.0000 39.0000  6.0000  2.0000 17.0000 38.0000\n [631]  9.0000 26.0000 11.0000  4.0000 20.0000 26.0000 25.0000 18.0000 24.0000\n [640] 35.0000 40.0000 38.0000  5.0000  9.0000  3.0000 13.0000 23.0000  5.0000\n [649]      NA 45.0000 23.0000 17.0000 27.0000 23.0000 20.0000 32.0000 33.0000\n [658]  3.0000      NA      NA      NA 18.0000 40.0000 26.0000 15.0000 45.0000\n [667] 18.0000 27.0000 22.0000 19.0000 26.0000 22.0000 20.0000 32.0000 21.0000\n [676] 18.0000 26.0000  6.0000      NA      NA  9.0000 40.0000 32.0000      NA\n [685] 26.0000 18.0000 20.0000      NA 29.0000 22.0000 22.0000 35.0000 21.0000\n [694] 20.0000 19.0000 18.0000 18.0000 38.0000      NA 30.0000 17.0000 21.0000\n [703] 21.0000 21.0000      NA      NA 24.0000 33.0000 33.0000 28.0000 16.0000\n [712] 37.0000 28.0000      NA 24.0000 21.0000      NA 32.0000 29.0000 26.0000\n [721] 18.0000 20.0000 19.0000 24.0000 24.0000 36.0000 31.0000 31.0000 30.0000\n [730] 22.0000      NA 43.0000 35.0000 27.0000 19.0000 30.0000 36.0000  3.0000\n [739]  9.0000 59.0000 19.0000 44.0000 17.0000      NA 45.0000 22.0000 19.0000\n [748] 29.0000 30.0000 34.0000 28.0000  0.3333 27.0000 25.0000 24.0000 22.0000\n [757] 21.0000 17.0000      NA      NA 26.0000 33.0000  1.0000  0.1667 25.0000\n [766] 36.0000 36.0000 30.0000      NA 23.0000 26.0000 19.0000 65.0000      NA\n [775] 42.0000 43.0000 32.0000 19.0000 30.0000 24.0000 23.0000      NA 24.0000\n [784] 24.0000 23.0000 22.0000      NA 18.0000 16.0000 45.0000      NA      NA\n [793]      NA 47.0000  5.0000      NA      NA      NA      NA      NA      NA\n [802]      NA      NA      NA      NA      NA 21.0000 18.0000  9.0000 48.0000\n [811] 16.0000      NA      NA 25.0000      NA      NA 22.0000 16.0000      NA\n [820] 33.0000      NA  9.0000 41.0000 38.0000 40.0000 43.0000 14.0000 16.0000\n [829]  9.0000 10.0000  6.0000 11.0000 40.0000 32.0000      NA 20.0000 37.0000\n [838] 28.0000 19.0000      NA      NA      NA      NA      NA      NA      NA\n [847]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [856]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [865]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [874]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [883]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [892]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [901]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [910]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [919]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [928]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [937]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [946]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [955]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [964]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [973]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [982]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n [991]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1000]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1009]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1018]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1027]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1036]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1045]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1054]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1063]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1072]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1081]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1090]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1099]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1108]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1117]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1126]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1135]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1144]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1153]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1162]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1171]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1180]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1189]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1198]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1207]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1216]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1225]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1234]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1243]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1252]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1261]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1270]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1279]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1288]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1297]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[1306]      NA      NA      NA      NA      NA      NA      NA      NA\n\n\n\nIf you look through the results, you will see that most individuals have numbers in this list, but some have NA. These NA values are the people for which we do not have age information - missing data.\nBy the way, the titanic.csv file simply has nothing in the places where there is missing data. When R loaded it, it replaced the empty spots with NA automatically."
  },
  {
    "objectID": "lab02-descriptives.html#measures-of-location",
    "href": "lab02-descriptives.html#measures-of-location",
    "title": "Lab 02 Describing data",
    "section": "4 Measures of location",
    "text": "4 Measures of location\nFor this lab we will use R to give some basic descriptive statistics for numerical data.\n\n4.1 mean()\nWe have already seen in lab 1 how to calculate the mean of a vector of data using mean(). Unfortunately, if there are missing data we need to tell R how to deal with it.\nA (somewhat annoying) quirk of R is that if we try to take the mean of a list of numbers that include missing data, we get an NA for the result!\n\n# mean with missing data - doh!\nmean(titanicData$age)\n\n[1] NA\n\n\n\n\n\n4.2 na.rm = TRUE\nTo get the mean of all the numbers that we do have, we have to add an argument to the mean() function, na.rm = TRUE.\n\n# tell R to exclude NAs\nmean(titanicData$age, na.rm = TRUE)\n\n[1] 31.19418\n\n\n\nThis tells R to remove (“rm”) the NAs (“na”) before calculating the mean. It turns out that the mean age of passengers that we have information for was about 31.2.\nna.rm = TRUE can be added to many functions in R, including median(), as we shall see next.\n\n\n\n4.3 median()\nThe median of a series of numbers is the “middle” number – half of the numbers in the list are greater the median and half are below it. It can be calculated in R by using median().\n\n# calculate median\nmedian(titanicData$age, na.rm = TRUE)\n\n[1] 30\n\n\n\n\n\n4.4 summary()\nA handy function that will return both the mean and median at the same time, along with other information such as the first and third quartiles, is summary().\n\n# automatic summary information\nsummary(titanicData$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1667 21.0000 30.0000 31.1942 41.0000 71.0000     680 \n\n\n\nFrom left to right, this output gives us\n\nthe smallest (minimum) value in the list (“Min.”)\nthe first quartile (“1st Qu.” - the value where 25% of the values are smaller)\nthe median\nthe mean\nthe third quartile (“3rd Qu.” - 75% of values are smaller)\nthe largest (maximum) value (“Max.”)\nthe number of individuals with missing values (”NA’s”)\n\n\nThe first quartile is the value in the data that is larger than a quarter of the data points. The third quartile is larger than 3/4 of the data. These are also called the 25th percentile and the 75th percentile, respectively. (You may remember these from boxplots, where the top and bottom of the box mark the 75th and 25th percentiles, respectively.)"
  },
  {
    "objectID": "lab02-descriptives.html#measures-of-variability",
    "href": "lab02-descriptives.html#measures-of-variability",
    "title": "Lab 02 Describing data",
    "section": "5 Measures of variability",
    "text": "5 Measures of variability\nR can also calculate measures of the variability of a sample. In this section we’ll learn how to calculate the variance, standard deviation, coefficient of variation and interquartile range of a set of data.\n\n\n5.1 var()\nTo calculate the variance of a list of numbers, use var().\n\n# variance is big here!\nvar(titanicData$age, na.rm = TRUE)\n\n[1] 217.4895\n\n\n\nNote that var(), as well as sd() below, have the same need for na.rm = TRUE when analyzing data that include missing values.\n\n\n\n5.2 sd()\nThe standard deviation can be calculated by sd().\n\n\n# standard deviation\nsd(titanicData$age, na.rm = TRUE)\n\n[1] 14.74753\n\n\n\nOf course, the standard deviation is the same as the square root of the variance.\n\nsqrt(var(titanicData$age, na.rm = TRUE))\n\n[1] 14.74753\n\n\n\n\n\n5.3 Coefficient of variation\nThe variance of a measure is positively correlated with it’s mean. We can standardize the measure of variation according to the mean (so that variation can be compared between variables with different means) using the coefficient of variation, the `c.v.’.\n\\(c.v. = (standard\\ deviation) / mean * 100\\)\nThere is no standard function in R to calculate the coefficient of variation. You can do this yourself, though, directly from the definition:\n\n# calculate the cv\nsd(titanicData$age, na.rm = TRUE) / mean(titanicData$age, na.rm = TRUE) * 100\n\n[1] 47.27653\n\n\n\n\n\n5.4 IQR()\nThe interquartile range (or IQR) is the difference between the third quartile and the first quartile; in other words the range covered by the middle half of the data. It can be calculated easily with IQR().\n\n# Interquartile range\nIQR(titanicData$age, na.rm = TRUE)\n\n[1] 20\n\n\n\nNote that this is the same as we could calculate by the results from summary() above. The third quartile is 41 and the first quartile is 21, so the difference is 41 – 21 = 20.\n\n\n\n5.5 Confidence intervals (CI) of the mean\nThe confidence interval for an estimate tells us a range of values that is likely to contain the true value of the parameter. For example, in 95% of random samples from a population, the 95% confidence interval of the mean will contain the true value of the population mean.\nR does not have a built-in function to calculate only the confidence interval of the mean, but the function that calculates t-tests will give us this information. The function t.test() has many results in its output. By adding $conf.int to this function we can slice out only the confidence interval for the mean. By default it gives us the low and high values for the 95% confidence interval of a variable.\n\n# 95 CI\nt.test(titanicData$age)$conf.int\n\n[1] 30.04312 32.34524\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\nAs the result above shows, the 95% confidence interval of the mean of age in the titanicData data set is from about 30.0 to 32.3. R also tells us that it used a 95% confidence level for its calculation. (The confidence interval is not so useful in this case, because we actually have information for nearly all the individuals on the Titanic.)\nTo calculate confidence intervals with a different level of confidence, we can add the option conf.level to the t.test function. For example, for a 99% confidence interval we can use the following.\n\n\n# Specify confidence interval with conf.level argument\nt.test(titanicData$age, conf.level = 0.99)$conf.int\n\n[1] 29.67976 32.70861\nattr(,\"conf.level\")\n[1] 0.99"
  },
  {
    "objectID": "lab02-descriptives.html#sampling-exercise",
    "href": "lab02-descriptives.html#sampling-exercise",
    "title": "Lab 02 Describing data",
    "section": "6 Sampling exercise",
    "text": "6 Sampling exercise\n\n6.1 Distribution of sample means\nGo to the web and open the page at Gaussian sampling exercise. This page contains some interactive visualizations that let you play around with sampling to see the distribution of sampling means. Click the button that says “Tutorial” near the bottom of the page and follow along with the instructions.\n\n\n\n6.2 Confidence intervals\nGo back to the web, and open the page at Confidence interval exercise. This page draws confidence intervals for the mean of a known population. Click “Tutorial” again, and follow along."
  },
  {
    "objectID": "lab02-descriptives.html#challenge-questions",
    "href": "lab02-descriptives.html#challenge-questions",
    "title": "Lab 02 Describing data",
    "section": "7 Challenge questions",
    "text": "7 Challenge questions\n\nMake a script in RStudio that collects all your R code required to answer the following questions. Include answers to the qualitative questions using comments.\n\n\n\n7.1\nUse the data file MancHeight.csv for this question - it contains height, gender and the number of siblings for university statistics students from Manchester. These data were collected using measuring tapes in units of cm. Open that file in R.\nPlot the distribution of heights in the class. Describe the shape of the distribution. Is it symmetric or skewed? Is it unimodal (one distinct peak), bimodal (two discernible peaks) or something else? ? Key point: A distribution is skewed if it is asymmetric. A distribution is skewed right if there is a long tail to the right, and skewed left if there is a long tail to the left.\nAre there any large or small outliers that look as though a student used the wrong units for their height measurement. (e.g., are there any that are more plausibly a height given in inches rather than the requested centimeters?) If so, and if this is not likely to be an accurate description of an individual in your class, use filter() from the package dplyr to create a new data set without those rows.\nUse R to calculate the mean height of all students in the class.\nUse sd() to calculate the standard deviation of height.\n\n\n\n7.2\nThe file caffeine.csv contains data on the amount of caffeine in a 16 oz. cup of coffee obtained from various vendors. For context, doses of caffeine over 25 mg are enough to increase anxiety in some people, and doses over 300 to 360 mg are enough to significantly increase heart rate in most people. A can of Red Bull contains 80mg of caffeine.\nWhat is the mean amount of caffeine in 16 oz. coffees?\nWhat is the 95% confidence interval for the mean?\nPlot the frequency distribution of caffeine levels for these data in a histogram. Comment on whether you think the amount of caffeine in a cup of coffee is relatively consistent from one vendor to another? What is the standard deviation of caffeine level? What is the coefficient of variation?\nThe file caffeineCosta.csv has data on six 16 oz. cups of Breakfast Blend coffee sampled on six different days from a Costa location. Calculate the mean (and the 95% confidence interval for the mean) for these data. Compare these results to the data taken on the broader sample of vendors in the first file. Describe the difference.\n\n\n\n7.3\nA confidence interval is a range of values that are likely to contain the true value of a parameter. Consider the caffeine.csv data again.\nCalculate the 99% confidence interval for the mean caffeine level.\nCompare this 99% confidence interval to the 95% confidence interval you calculate in question 2 above. Which confidence interval is wider (i.e., spans a broader range)? Why should this one be wider?\nLet’s compare the quantiles of the distribution of caffeine to this confidence interval. Approximately 95% of the data values should fall between the 2.5% and 97.5% quantiles of the distribution of caffeine. (Explain why this is true.) We can use R to calculate the 2.5% and 97.5% quantiles with a command like the following. (Replace “datavector” with the name of the vector of your caffeine data.)\nquantile(datavector, c(0.025, 0.975), na.rm =TRUE)\nAre these the same as the boundaries of the 95% confidence interval? If not, why not? Which should bound a smaller region, the quantile or the confidence interval of the mean?\n\n\n\n7.4\nReturn to the class data set Mancheight.csv. Find the mean value of sibs. Add one to this to find the mean number of children per family in the class.\nThe mean number of offspring per family twenty years ago was about 2. Is the value for this class similar, greater, or smaller? If different, think of reasons for the difference.\nAre the families represented in this class systematically different from the population at large? Is there a potential sampling bias?\nConsider the way in which the data were collected. How many families with zero children are represented? Why? What effect does this have on the estimated mean family size of all couples?\n\n\n\n7.5\nReturn to the data on countries of the world, in countries.csv. Plot the distributions for ecological_footprint_2000, cell_phone_subscriptions_per_100_people_2012, and life_expectancy_at_birth_female.\nFor each variable, plot a histogram of the distribution using hist(). Is the variable skewed? If so, in which direction?\nFor each variable, calculate the mean and median and add them to their respective plots using the function abline() using the v argument. Are the mean and median similar for each? Match the difference in mean and median to the direction of skew on the histogram. Do you see a pattern?"
  },
  {
    "objectID": "lab03-frequency.html#objectives",
    "href": "lab03-frequency.html#objectives",
    "title": "Lab 03 Frequency data",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nCalculate a confidence interval for a proportion\nMake a hypothesis test about proportions\nFit frequency data to a model\nTest for a fit to a Poisson distribution\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab03-frequency.html#start-a-script",
    "href": "lab03-frequency.html#start-a-script",
    "title": "Lab 03 Frequency data",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab03-frequency.html#confidence-interval-for-a-proportion",
    "href": "lab03-frequency.html#confidence-interval-for-a-proportion",
    "title": "Lab 03 Frequency data",
    "section": "3 Confidence interval for a proportion",
    "text": "3 Confidence interval for a proportion\nTo calculate a confidence interval for an estimate of a proportion, we suggest using the Agresti-Coull method. This method is available in the R package “binom”, which you can install and load with the following:\n\n# If needed, install {binom}\n# install.packages(\"binom\", dependencies = TRUE)\nlibrary(binom)\n\nWarning: package 'binom' was built under R version 4.1.1\n\n\n\nLike any package, you only need to install it once, but you need to run the library() command in each session before you can use it. Once the package is installed, calculating a confidence interval for a proportion is fairly straightforward. The function you need is called binom.confint(). You need to specify:\n\nx: the number of successes\nn: the total sample size\nmethod: “ac” for “Agresti-Coull”\n\n\nFor example, if we have 87 data points and 30 of them are “successes”, we can find the 95% confidence interval for the proportion of successes with the following command.\n\nbinom.confint(x = 30, n = 87, method = \"ac\")\n\n         method  x  n      mean     lower     upper\n1 agresti-coull 30 87 0.3448276 0.2532164 0.4495625\n\n\n\nThis tells us that the observed proportion (called “mean” here) is 0.3448, and the lower and upper bounds of the 95% confidence interval are 0.2532 and 0.4496."
  },
  {
    "objectID": "lab03-frequency.html#the-binomial-tests",
    "href": "lab03-frequency.html#the-binomial-tests",
    "title": "Lab 03 Frequency data",
    "section": "4 The Binomial tests",
    "text": "4 The Binomial tests\nDoing a binomial test in R is much easieR (see what I did theRe!?) than doing one by hand. You use the binomial test to decide whther some observed count of data with 2 possible outcomes is different than expected (e.g. if you count 100 coin flips you might expect 50:50)\n\n\n4.1 binom.test()\nThe function binom.test() will do an exact binomial test. It requires three pieces of information in the input:\nx for the number of “successes” observed in the data,\nn for the total number of data points, and\np for the proportion given by the null hypothesis.\n\nFor example, if we have 18 toads that have been measured for a left-right preference and 14 are right-handed, we can test the null hypothesis of equal probabilities of left- and right-handedness with a binomial test. In this case, n = 18, x = 14, and p = 0.5.\n\nbinom.test(x = 14, n = 18, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  14 and 18\nnumber of successes = 14, number of trials = 18, p-value = 0.03088\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5236272 0.9359080\nsample estimates:\nprobability of success \n             0.7777778 \n\n\n\nIn this case, the output of the function gives quite a bit of information. One key element that we will be looking for is the P-value; in this case R tells us that the P-value is 0.03088. This is the P-value that corresponds to a two-tailed test.\nThe binom.test() function also gives an estimate of the proportion of successes (in this case 0.7777778). (It also gives an approximate 95% confidence interval for the proportion using a different method than the Agresti-Coull method that we recommend.)"
  },
  {
    "objectID": "lab03-frequency.html#goodness-of-fit-test",
    "href": "lab03-frequency.html#goodness-of-fit-test",
    "title": "Lab 03 Frequency data",
    "section": "5 Goodness of fit test",
    "text": "5 Goodness of fit test\nA chi-squared (\\(\\chi^2\\)) goodness-of-fit test compares the frequencies of values of a categorical variable to the frequencies predicted by a null hypothesis. For example, the file MandMlist.csv contains a list of all the colors of M&M candies from one package, under the variable “color”. Let’s ask whether this fits the proportions of colors advertised by the candy company.\n\n# Read in the data, NB your file path may be different\nMMlist <- read.csv(\"data/MandMlist.csv\")\n\n# MMlist$color contains the color of each of 55 M&Ms.\nMMlist$color\n\n [1] \"brown\"  \"blue\"   \"blue\"   \"green\"  \"yellow\" \"yellow\" \"orange\" \"yellow\"\n [9] \"orange\" \"yellow\" \"red\"    \"green\"  \"green\"  \"orange\" \"orange\" \"yellow\"\n[17] \"green\"  \"blue\"   \"red\"    \"brown\"  \"green\"  \"orange\" \"blue\"   \"yellow\"\n[25] \"green\"  \"yellow\" \"orange\" \"brown\"  \"green\"  \"blue\"   \"brown\"  \"red\"   \n[33] \"green\"  \"orange\" \"blue\"   \"blue\"   \"green\"  \"orange\" \"red\"    \"yellow\"\n[41] \"red\"    \"blue\"   \"brown\"  \"yellow\" \"blue\"   \"orange\" \"green\"  \"green\" \n[49] \"green\"  \"blue\"   \"orange\" \"yellow\" \"yellow\" \"orange\" \"red\"   \n\n\n\n\n5.1 table()\nSummarizing frequency data in a table is useful to see the data more concisely, and such a table is also necessary as input to the χ2 test function. We can summarize the counts (frequencies) of a categorical variable with table().\n\n# create table\nMMtable <- table(MMlist$color)\n\n# print table\nMMtable\n\n\n  blue  brown  green orange    red yellow \n    10      5     12     11      6     11 \n\n\nThis shows that in this list of M&M colors, 10 were blue, 5 were brown, etc.\n\n\n\n5.2 chisq.test()\nWe can use a χ2 goodness-of-fit test to compare the frequencies in a set of data to a null hypothesis that specifies the probabilities of each category. In R, this can be calculated using the function chisq.test().\nThe company says that the percentages are 24% blue, 14% brown, 16% green, 20% orange, 13% red and 13% yellow. This is our null hypothesis. Let’s test whether these proportions are consistent with the frequencies of the colors in this bag. R requires you to proivide the proportions expected by the null hypothesis in a vector, like this:\n\nexpected_proportions <- c(0.24, 0.14, 0.16, 0.20, 0.13, 0.13)\n\nThese expected proportions will use to calculate the expected frequencies.\n\nThe first thing we need to do is check whether the expected frequencies (expected by the null hypothesis) are large enough to justify using a χ2 goodness of fit test. (In other words, that no more than 25% of the expected frequencies are less than 5 and none is less than 1.)\nTo obtain these expected counts, multiply the vector of the expected proportions by the total sample size. We can find the total sample size by taking the sum() of the frequency table:\n\nsum(MMtable)\n\n[1] 55\n\n\n\nSo for this example, the expected frequencies from our null hypothesis are 55 multiplied by the list of expected proportions:\n\n55 * expected_proportions\n\n[1] 13.20  7.70  8.80 11.00  7.15  7.15\n\n\nAll of these expected values are greater than 5, so we have no problem with the assumptions of the χ2 goodness of fit test. (If there were a problem here, we’d have to combine categories. We’ll see an example of that in the next section.)\n\nThe function chisq.test() requires two arguments in its input: a frequency table from the data and a vector with the expected proportions from the null hypothesis. The name of the vector of expected proportions in this function is p. So here is the format of the R command to do a χ2 test:\n\nchisq.test(MMtable, p = expected_proportions)\n\n\n    Chi-squared test for given probabilities\n\ndata:  MMtable\nX-squared = 5.1442, df = 5, p-value = 0.3985\n\n\n“X–squared” here in the output means χ2. The χ2 value for this test is 5.1442. There are 5 degrees of freedom (df) for this test, and the P-value turned out to be 0.3985. We wouldn’t reject the null hypothesis with these data."
  },
  {
    "objectID": "lab03-frequency.html#fit-to-a-poisson-distribution",
    "href": "lab03-frequency.html#fit-to-a-poisson-distribution",
    "title": "Lab 03 Frequency data",
    "section": "6 Fit to a Poisson distribution",
    "text": "6 Fit to a Poisson distribution\nA χ2 goodness of fit test can be used to ask whether a frequency distribution of a variable is consistent with a specific distribution, like the Poisson distribution. This is a useful way to determine if the count of events in space or time are not “random” but instead are clumped or dispersed.\nIt turns out that this is not a straightforward process in R. In this section, we’ll highlight a couple of functions in R that will streamline doing the calculations by hand. We’ll use the data on the numbers of historical extinctions in the data file MassExtinctions.csv.\n\nMass extinctions Do extinctions occur randomly in Earth’s history, or are there some periods when extinctions occur at a rate higher than we would expect at random, i.e., is there evidence of “mass extinctions”? Fossil marine invertebrates constitute what is considered the best fossil record of extinctions, because they have structures that tend to preserve well. The data we examine here is a survey of the recorded number of extinctions of marine invertebrate families in 76 time blocks of similar duration through the fossil record (Raup & Seposki 1982).\n\n\n\n# Load the data, you will probably need to supply your own file path\nextinctData <- read.csv(\"data/MassExtinctions.csv\")\n\nnumber_of_extinctions <- extinctData$numberOfExtinctions\n\ntable(number_of_extinctions)\n\nnumber_of_extinctions\n 1  2  3  4  5  6  7  8  9 10 11 14 16 20 \n13 15 16  7 10  4  2  1  2  1  1  1  2  1 \n\n\n\nThe first row lists each possible number of extinctions in a time period, ranging from 1 to 20. The second row shows the number of time periods that had each number of extinctions. (so there were 13 time periods with one extinction, 15 time periods with 2 extinctions, etc.)\nThe mean number of extinctions per unit of time is not specified by the null hypothesis, so we need to estimate it from the data. This mean turns out to be 4.21 extinctions per time period.\n\nmean(number_of_extinctions)\n\n[1] 4.210526\n\n\n\nWe can use this mean to calculate the probability according to a Poisson distribution of getting each specific number of extinctions.\n\n\n6.1 dpois()\nFortunately, R has a function to calculate the probability of a given value under the Poisson distribution. This function is dpois(), which stands for “density of the Poisson”. dpois() requires two values for input, the mean (which is called lambda in this function) and x, the value of the outcome we are interested in (here, the number of extinctions). For example, the probability of getting x = 3 from a Poisson distribution with mean 4.21 can be found from the following:\n\ndpois(x = 3, lambda = 4.21)\n\n[1] 0.1846355\n\n\n\nWe see that the probability of getting exactly 3 successes from a Poisson distribution with mean 4.21 is about 18.5%.\ndpois() will accept a vector of xs as input, and return a vector of the probabilities of those xs. A convenient short hand to know here is that R will create a vector with a range of values with a colon. For example, 0:20 is a vector with the integers from 0 to 20.\n\n0:20\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\nWe’ll need the probabilities for all possible values of x from 0 to 20 (because this was the largest value observed in the data). We can create a vector with the Poisson probabilities of each possibility from 0 to 20 like this:\n\nexpected_probability <- dpois(x = 0:20, lambda = 4.21)\n\nexpected_probability\n\n [1] 1.484637e-02 6.250321e-02 1.315693e-01 1.846355e-01 1.943289e-01\n [6] 1.636249e-01 1.148102e-01 6.905011e-02 3.633762e-02 1.699793e-02\n[11] 7.156129e-03 2.738846e-03 9.608784e-04 3.111768e-04 9.357530e-05\n[16] 2.626347e-05 6.910575e-06 1.711384e-06 4.002736e-07 8.869220e-08\n[21] 1.866971e-08\n\nround(expected_probability, 8)\n\n [1] 0.01484637 0.06250321 0.13156926 0.18463553 0.19432889 0.16362493\n [7] 0.11481016 0.06905011 0.03633762 0.01699793 0.00715613 0.00273885\n[13] 0.00096088 0.00031118 0.00009358 0.00002626 0.00000691 0.00000171\n[19] 0.00000040 0.00000009 0.00000002\n\n\n\n\nNB! The odd numerical notation here is a shorthand for scientific notation. 1.48e-02 means 1.48 x 10-2, which is equal to 0.0148. The e stands for exponent.\n\n\nRemember that we asked R to output the probability of values from 0 to 20. So the first value in this output corresponds to the probability of getting a zero, the second to the probability of getting a 1, etc.\n\n\n6.2 Example chi squared test\nIn order to convert these probabilities into expected values for a χ2 test, we need to multiply them by the total sample size.\n\n# the expected number of extinctions\nround(length(number_of_extinctions) * expected_probability, 8)\n\n [1]  1.12832399  4.75024401  9.99926365 14.03229999 14.76899574 12.43549441\n [7]  8.72557191  5.24780825  2.76165909  1.29184275  0.54386580  0.20815227\n[13]  0.07302676  0.02364943  0.00711172  0.00199602  0.00052520  0.00013007\n[19]  0.00003042  0.00000674  0.00000142\n\n\n\nMost of these values would cause problems for the χ2 test because they are too small (less than 1). We need to combine categories in some reasonable way so that the expected frequencies match the assumptions of the χ2 test. Let’s combine the categories for 0 and 1 successes, and combine everything greater than or equal to 8 into one category. It is possible to do this all in R, but you can also do it by hand, using the expected frequencies we just calculated. Here is a table of the expected frequencies for these new combined categories, for both the observed and expected, summing over all the possibilities within each of these groupings. (E.g., the expected frequency for the category “0 or 1” is the sum of 1.128324 and 4.750244.)\n\n\n\n\n\nExpected frequencies\n\n\n\n\nNote that these expected values are large enough to match the conditions of the χ2 test.\n\n\n\n6.3 Do the test\nMake vectors (using c()) for the observed and expected values for these new groups:\n\nexpected_combined <- c(5.878568, 9.999264, 14.032300, 14.768996, 12.435494,  8.725572, 5.247808, 4.911998)\n\nobserved_combined <- c(13, 15, 16, 7, 10, 4, 2, 9)\n\nFrom these we can calculate the χ2 test statistic using chisq.test()$statistic.\n\nFirst we give it the list of observed values, then the expected values as \\(p\\). Because we are giving the list of expected frequencies rather the expected probabilities, we need to give it the option rescale.p = TRUE. Finally, by adding $statistic at the end, R will only give us the χ2 value as output. (We don’t yet want the full results of the test because R does not know to correct for the reduction in degrees of freedom caused by estimating the mean.)\n\nchisq.test(observed_combined, p = expected_combined, rescale.p = TRUE)$statistic\n\nWarning in chisq.test(observed_combined, p = expected_combined, rescale.p =\nTRUE): Chi-squared approximation may be incorrect\n\n\nX-squared \n 23.93919 \n\n\n\nThis correctly calculates our χ2 value as 23.939. The warning message is because one of the expected values is under 5, but because this occurred in less than 20% of the categories, it is fine to proceed.\nThe degrees of freedom for this test are the number of categories minus the number of parameters estimated from the data minus one. We ended up using 8 categories and estimated one parameter from the data, so we have df = 8 – 1 – 1 = 6.\n\n\n\n6.4 pchisq()\nTo calculate the correct P-value, we need to know probability of getting a χ2 value greater than 23.939 from a distribution that has 6 degrees of freedom. R has a function we can use for this, called pchisq(). We are interested here in the probability above the observed χ2 value, so we want the probability in the right (or upper) tail. To use this function, we need to specify three values: our observed χ2 (confusingly called \\(q\\) in this function), the degrees of freedom (called df, thank goodness), and an option that tells us to use the upper tail (lower.tail = FALSE).\n\npchisq(q = 23.939, df = 6, lower.tail = FALSE)\n\n[1] 0.0005359236\n\n\nThus, our P-value for this test of the fit of these data to a Poisson distribution is approximately 0.0005. We reject the null hypothesis that these extinction data follow a Poisson distribution."
  },
  {
    "objectID": "lab03-frequency.html#r-commands-summary",
    "href": "lab03-frequency.html#r-commands-summary",
    "title": "Lab 03 Frequency data",
    "section": "7 R commands summary",
    "text": "7 R commands summary\n\n\n\n\ncommands summary"
  },
  {
    "objectID": "lab03-frequency.html#exercises",
    "href": "lab03-frequency.html#exercises",
    "title": "Lab 03 Frequency data",
    "section": "8 Exercises",
    "text": "8 Exercises\n\nMake a script in RStudio that collects all your R code required to answer the following questions. Include answers to the qualitative questions using comments.\n\n\n\n8.1\nMany hospitals (and airplanes) have signs posted banning mobile phone use. These bans originated from studies on early versions of mobile phones. In one such experiment, out of 510 tests with cell phones operating at near-maximum power, six disrupted a piece of medical equipment enough to hinder interpretation of data or cause equipment to malfunction. A more recent study found zero instances of disruption of medical equipment out of 300 tests.\nFor the older data, use binom.confint() with the Agresti-Coull method to calculate the estimated proportion of equipment disruption. What is the 95% confidence interval for this proportion?\nFor the data on the newer cell phones, use R to calculate the estimate of the proportion and its 95% confidence interval.\n\n\n\n8.2\nIt is difficult to tell what other people are thinking, and it may even be impossible to find out what they are thinking by asking them. A series of studies shows that we do not always know how our own thought processes are carried out.\nA classic experiment by Wilson and Nisbett (1978) addressed this issue in a clever way. Participants were asked to decide which of four pairs of silk stockings were better, but the four stockings that they were shown side-by-side were in fact identical. Nearly all participants were able, however, to offer reasons that they had chosen one pair over the other.\nThe four pairs of stockings were presented to the participants randomly with respect to which pair was in which position. However, of the 52 subjects who selected a pair of stockings, 6 chose the pair on the far left, 9 chose the pair in the left-middle, 16 chose the pair in the right-middle, and 21 chose the pair on the far right. None admitted later that the position had any role in their selection. These data are in a file called stockings.csv.\n\n\nWhat are the expected frequencies for this scenario, under the null hypothesis that all four pairs of stockings are equally likely to be chosen?\n\n\nUse chisq.test() to test the null hypothesis that the selection of the stockings was independent of position.\n\nThe function chisq.test() can take the data either as a data frame, as above, or as a vector of the observed counts, as a parameter called x as input:\n\nchisq.test(x = c(6,9,16,21), p = c(0.25,0.25,0.25,0.25))\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(6, 9, 16, 21)\nX-squared = 10.615, df = 3, p-value = 0.014\n\n\nTry it using the specification of the counts, to see that you get the same answer as above.\n\n\n\n8.3\nMany people believe that the month in which a person is born predicts significant attributes of that person in later life. Such astrological beliefs have no scientific support, but are there circumstances in which birth month could have a strong effect on later life? One prediction is that elite athletes will disproportionately have been born in the months just after the age cutoff used to separate levels for young players of the sport. The prediction is that those athletes that are oldest within an age group will do better by being relatively older, and therefore will gain more confidence and attract more coaching attention than the relatively younger players in their same groups. As a result, they may be more likely to dedicate themselves to the sport and do well later. In the case of football, the cutoff for different age groups is generally August.\nThe birth months (by three month interval) of football players competing in the Under-20’s World Tournament are recorded in the data file football_birth_quarter.csv (from Barnsley et al. 1992). Plot these data. Do you see a pattern?\nThe numbers of people born calculated from a large sample is recorded in the file births.csv. Compare the distribution of birth months of the football players to what would be expected by chance, assuming that the birth data for the same in “births.csv” is a good approximation for the population from which recorded players are drawn. Use the appropriate statistical test. Describe the pattern.\n\n\n\n8.4\nIs cardiac arrest equally likely to occur throughout the year? Or are some weeks more likely than others to produce cardiac arrests? One way to look at this issue is to ask whether cardiac arrests occur according to a probability model in which such events are independent of each other and equally likely at all times. If so, then the number of cardiac arrests per week will follow a Poisson distribution.\nThe data file “cardiacArrest.csv” contains data on the number of cardiac arrests per week from one hospital over five years. It records the number of cardiac arrests that occurred to individuals outside of the hospital who were then brought in for treatment (from Skogvoll and Lindqvist 1999).\n\nCalculate a table of the observed frequencies of cardiac arrests per week.\nWhat is the mean number of cardiac arrests per week?\nFor the mean you just calculated, use dpois() to calculate the probability of 0 cardiac arrests in a week assuming a Poisson distribution. Multiply that probability by the number of data points to calculate the expected frequency of 0 in these data under the null hypothesis of a Poisson distribution.\nHere is a table of the expected frequencies under the null hypothesis. (The expected frequency for zero cardiac arrests should match your calculation above.) Are these frequencies acceptable for use in a χ2 goodness of fit test?\n\n\n\n\nNumber of cardiac arrests\nExpected\n\n\n\n\n0\n34.785295\n\n\n1\n70.103698\n\n\n2\n70.640891\n\n\n3\n47.454800\n\n\n4\n23.909219\n\n\n5\n9.636973\n\n\n6 or more\n4.469124\n\n\n\n\n\nCreate vectors in R for the observed and expected frequencies.\nCalculate the χ2 for this hypothesis test, using chisq.test()$statistic.\nHow many degrees of freedom should this χ2 goodness of fit test have?\nCalculate the P-value for this test, using pchisq().\nSummarize the results of this test. Does the frequency distribution of out-of-hospital cardiac events follow a Poisson distribution?"
  },
  {
    "objectID": "lab03-frequency.html#harper-adams-data-science",
    "href": "lab03-frequency.html#harper-adams-data-science",
    "title": "Lab 03 Frequency data",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  },
  {
    "objectID": "lab04-contingency.html#objectives",
    "href": "lab04-contingency.html#objectives",
    "title": "Lab 04 Contingency",
    "section": "1 Objectives",
    "text": "1 Objectives\n\nDisplay associations between two categorical variables in contingency tables and mosaic plots\nCalculate odds ratios and their confidence intervals\nTest for the independence of two categorical variables\n\n\nData for all labs (unzip to your working directory)\nTemplate script"
  },
  {
    "objectID": "lab04-contingency.html#start-a-script",
    "href": "lab04-contingency.html#start-a-script",
    "title": "Lab 04 Contingency",
    "section": "2 Start a script!",
    "text": "2 Start a script!\nFor this and every lab or project, begin by:\n\nstarting a new script\ncreate a good header section and table of contents\nsave the script file with an informative name\nset your working directory\n\nAim to make the script useful as a future reference for doing things in R - this will come in handy for projects and assessments!"
  },
  {
    "objectID": "lab04-contingency.html#categorical-variables",
    "href": "lab04-contingency.html#categorical-variables",
    "title": "Lab 04 Contingency",
    "section": "3 Categorical variables",
    "text": "3 Categorical variables\nIn this lab, we will learn how to look at, and test for, the association between two categorical variables. We’ll use the Titanic data set as an example, looking at the association between the sex of passengers and whether they survived the accident. Let’s load the data from the .csv file:\n\n# Load data\n# NB your file path may be different than mine\ntitanicData <- read.csv(\"data/titanic.csv\" )\n\nThe variables we’ll use are sex and survive. The variable survive contains a yes if the individual survived the sinking and a no for those that did not."
  },
  {
    "objectID": "lab04-contingency.html#frequency-table",
    "href": "lab04-contingency.html#frequency-table",
    "title": "Lab 04 Contingency",
    "section": "4 Frequency table",
    "text": "4 Frequency table\nA contingency table is an effective method to see the association between two categorical variables. Moreover, other R functions we will use in this exercise require a contingency table as input.\nA frequency table can be created using a function we saw in the last lab, called table(). To create a contingency table that shows the relationship between two categorical variables, we simply give table() the vectors that contain those two variables. Put the explanatory variable first, and then the response variable after a comma.\n\nsex_survive_table <- table(titanicData$sex, titanicData$survive)\nsex_survive_table\n\n        \n          no yes\n  female 156 307\n  male   708 142\n\n\n\nThis shows us that in the Titanic data set, there are 156 female passengers who did not survive, 307 females who did survive, and similar information for the males.\nIt is useful to keep this frequency table as a named object, as we have done here (sex_survive_table). We shall use this table several times in later analyses.\nSometimes for contingency analysis we have already summarized the counts for each case. In these cases it is useful to be able to create a data table directly from these counts. The following syntax will allow you to create a data table directly:\n\n# Make a table \"manually\"\"\n\nsex_survive_table_direct <- data.frame(no = c(156, 708),\n                                       yes = c(307, 142), \n                                       row.names = c(\"female\", \"male\"))\nsex_survive_table_direct\n\n        no yes\nfemale 156 307\nmale   708 142"
  },
  {
    "objectID": "lab04-contingency.html#mosaic-plots",
    "href": "lab04-contingency.html#mosaic-plots",
    "title": "Lab 04 Contingency",
    "section": "5 Mosaic plots",
    "text": "5 Mosaic plots\nWe often want show associations between categorical variables, and we have options, e.g. using a grouped bar or the he mosaic plot. Mosaic plots are nice because each combination of the variables is represented by a rectangle, and the size of the rectangle is proportional to the number of individuals in that combination.\nR has a function to calculate mosaic plots, with the sensible name mosaicplot(). In its most basic form, you just give it a frequency table as input.\n\n# ugly but functional default mosaic plot\nmosaicplot(sex_survive_table)\n\n\n\n\n\nThis shows the basic pattern. However, this plot can be greatly improved by adding a couple of extra options, to specify color and axes labels. We can add the option color = c(“darkred”, “gold”) to tell R which colors to use for the different response variables. This is a vector of color names that R assigns in the same order as the order of the categories of the variable plotted on the vertical axis (the response variable) starting with the topmost. (R has many named colors, including all the basics like “red”, “orange”, “blue”, etc.)\nWe would also like the axes to have good labels (rather than merely having the category level names). We can specify these with xlab and ylab as options. Let’s simply call the x-axis “Sex” and the y-axis “Survival”. Here’s what the command would now look like:\n\n# better but still slighty ugly\nmosaicplot(sex_survive_table, \n          color = c(\"blue4\", \"gold\"), \n          xlab = \"Sex\", \n          ylab = \"Survival\")\n\n\n\n\n\nIt is much easier now to see in the graph that the majority of females survived whereas the majority of males did not."
  },
  {
    "objectID": "lab04-contingency.html#odds-ratios",
    "href": "lab04-contingency.html#odds-ratios",
    "title": "Lab 04 Contingency",
    "section": "6 Odds ratios",
    "text": "6 Odds ratios\nOne of the ways to measure the strength of the association between two categorical variables is an odds ratio.\nIn R, the simplest way to estimate an odds ratio is to use the command fisher.test(). This function will also perform a Fisher’s exact test (more on that later). The input to this function is a contingency table like the one we calculated above. We’ll use the results in a couple of ways, so let’s save the results in an object. (Here we called it sex_survive_fisher)\n\nsex_survive_fisher <- fisher.test(sex_survive_table)\nsex_survive_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  sex_survive_table\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07759301 0.13384845\nsample estimates:\nodds ratio \n 0.1021212 \n\n\n\nThe output of this function has several parts, two of which we’ll want to look at now for estimating an odds ratio. We can see the specific elements of the output by using the $ character, in a similar way to how we specify variables inside of data frames.\nAdd $estimate after the results of the fisher.test() function call to get the odds ratio estimate. For example, if we want to know the odds ratio for survival as a function of sex for the Titanic data, we write:\n\nsex_survive_fisher$estimate\n\nodds ratio \n 0.1021212 \n\n\n\nThis shows that the odds ratio is about 0.10. Thus, the odds of a male surviving were only about a tenth of the odds of a female surviving. This is:\n(female death / female survival) / (male death / male survival)\n\nThe order of the values in the odds ratios is determined by the order of the values of each variable; by default R uses alphabetical order.\nThis fisher.test() function also calculates the 95% confidence interval for the odds ratio, and assigns it to an output variable called conf.int. We can see the 95% confidence interval for the odds ratio with a command like:\n\nsex_survive_fisher$conf.int\n\n[1] 0.07759301 0.13384845\nattr(,\"conf.level\")\n[1] 0.95\n\n\nThus, the confidence interval for this odds ratio ranges from about 0.078 to about 0.134."
  },
  {
    "objectID": "lab04-contingency.html#χ2-contingency-test",
    "href": "lab04-contingency.html#χ2-contingency-test",
    "title": "Lab 04 Contingency",
    "section": "7 χ2 contingency test",
    "text": "7 χ2 contingency test\nA χ2 contingency analysis allows us to test the null hypothesis that two categorical variables are independent of each other.\nBecause this is simple to interpret and to calculate by hand, it is possibly the most common test used in science. However, the χ2 has assumptions requiring that all of the expected values are greater than 1 and that at least 80% are greater than 5. When doing such a test of independence on a computer, it may sometimes be better to use Fisher’s exact test, which doesn’t have this restriction.\nThe χ2 contingency test can be done with a function we have seen before, chisq.test(). If we give a frequency table as input, this function will calculate the χ2 test for us.\nBefore we do the test, though, we need to make sure that the assumptions of the χ2 test are met by our data. Fortunately, the chisq.test() function also provides a way for us to look at the expected values. If we give a frequency table as input, and then add $expected at the end of the function, it will show us the expected values for a test of independence, like this:\n\nchisq.test(sex_survive_table)$expected\n\n        \n               no      yes\n  female 304.6702 158.3298\n  male   559.3298 290.6702\n\n\n\nIn this case all the expected values are greater than 5, so we have no problem meeting this assumption. Therefore, it is appropriate to do a χ2 contingency test. Just give a frequency table as input to the chisq.test() function to do this test. We’ve added the option correct = FALSE to tell R to not do a Yate’s correction, which can be overly conservative.\n\nchisq.test(sex_survive_table, correct=FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  sex_survive_table\nX-squared = 327.7, df = 1, p-value < 2.2e-16\n\n\nThis output shows that the χ2 value for this test is 325.5, with 1 degree of freedom and a P-value less than 0.00000000000000022. So we can reject the null hypothesis of no association between sex and survival on the Titanic."
  },
  {
    "objectID": "lab04-contingency.html#fishers-exact-test",
    "href": "lab04-contingency.html#fishers-exact-test",
    "title": "Lab 04 Contingency",
    "section": "8 Fisher’s exact test",
    "text": "8 Fisher’s exact test\nAnother, more exact, option for testing for the independence of two categorical variables is Fisher’s exact test. This is a test that is tedious to calculate by hand, but R can do it in a flash. This test makes no approximations and sets no minimum threshold for the sizes of the expected values.\nTo implement Fisher’s exact test in R, use fisher.test(). This function is easy to use; just give it a frequency table as input.\n\nfisher.test(sex_survive_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  sex_survive_table\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07759301 0.13384845\nsample estimates:\nodds ratio \n 0.1021212 \n\n\n\nHere there is little uncertainty about the presence of an association between sex and survival; the P-value is less than 0.00000000000000022, which is very, very small and much, much less than 0.05 (This strange value is the smallest number that can be recorded using the default numerical precision in R).\nAlso given in the output here is information about the estimate of the odds ratio and its 95% confidence interval.\nfisher.test() is able to calculate contingency tests even when there are more than two possible values for each variable. In such cases, though, it cannot calculate the odds ratios."
  },
  {
    "objectID": "lab04-contingency.html#r-commands-summary",
    "href": "lab04-contingency.html#r-commands-summary",
    "title": "Lab 04 Contingency",
    "section": "9 R commands summary",
    "text": "9 R commands summary\n\n\n\n\nSurvivors"
  },
  {
    "objectID": "lab04-contingency.html#activity",
    "href": "lab04-contingency.html#activity",
    "title": "Lab 04 Contingency",
    "section": "10 Activity",
    "text": "10 Activity\n\n10.1 Sampling and contingency tables\nThe χ2 contingency test uses a test statistic called χ2. When the null hypothesis is true the distribution of possible values of that χ2 is approximately a χ2distribution. We’ll use an online exercise to visualize how well the χ2 distribution fits the distribution of the test statistic when the null hypothesis is true.\nOpen a web browser and navigate to this page\nThis page is based on a hypothetical example in which we compare the proportion of people who get sick after either receiving a vaccination or not. When you first open this app, you will see that the true probability of getting sick has been set to 60%, regardless of whether the person is vaccinated or not. Under these conditions, the null hypothesis of a contingency analysis is true: whether or not a person is vaccinated has no effect on whether they get sick. The two variables are independent.\nNotice the graph at the bottom right. Before we do anything else, this graph simply shows the χ2 distribution with one degree of freedom, as would be appropriate to use for a 2 x 2 contingency analysis as will be performed here.\n\nSTEP 1 Click the “MAKE ONE SAMPLE” button once. This will cause the computer to take one sample of n = 120 individuals from the population. A frequency table for that sample is shown on the right side of the page, and under that the χ2 and P-value from a χ2 contingency test of those data is shown. Also, a vertical blue bar appears on the graph at that value of χ2, and the area under the curve to the left of this χ2 is shown in red. The area under the curve to the right is the P-value.\nVisually compare the P-value to the area under the curve in the graph. Did your sample cause the test to reject the null hypothesis? Or did that sample give the correct answer about the population (i.e., that there is no difference in the sickness rates between vaccinated and unvaccinated people)?\n\nSTEP 2 The button “SIMULATE DISTRIBUTION” will take repeated samples from the population and calculate a χ2 statistic for each one. In the yellow box on the right, it will keep a tally of how many samples led to hypothesis tests that rejected the null hypothesis of independence. (Rejecting a true null hypothesis is a Type I error.) The program will also make a histogram of the χ2 values from all the tests to allow comparison with the χ2 distribution.\nCheck that probability of getting sick is still set to be the same for vaccinated and unvaccinated people in the population parameters. If so, the null hypothesis is in fact true. With a true null hypothesis, the distribution of calculated χ2 values (in the histogram) ought to match the theoretical distribution (shown by the thin curve). Do they match? (Note: you won’t get a proportion that exactly matches what we expect, by chance, because we haven’t been able to take an infinite number of samples.)\nThe calculations in this app assume a significance level of α = 0.05. What fraction of samples should we expect to result in a Type I error?\nNotice that there is a “FASTER!” button on the left. If you press this it will speed up the intervals between new samples taken in the simulation.\nRun the simulation until there are 1000 or more replicates. What Type I error rate do you observe?\n\nSTEP 3 Finally, let’s investigate the distribution of results that you get when the null hypothesis is NOT true. Before we go any further, think about what you would expect to see in the distribution of test statistics when the null hypothesis is false. Should the distribution of test statistics stay the same? Or will it shift towards the right or left?\nLet’s have the app simulate a case in which the null hypothesis is false. For example, let’s imagine that the vaccine actually works to some extent, and the probability of vaccinated people getting sick is only 0.35. Change the small white box in the table at the top of the page to 0.35 under “Vaccinated”, but leave the value of 0.6 under the “Unvaccinated” column. Now vaccination status and illness are NOT independent—the probability of getting sick differs between vaccination groups.\nTo see what the distribution of test statistics is now under this case, let’s simulate samples from this new population. Click “SIMULATE DISTRIBUTION” again. What is the shape of the distribution of test statistics in the histogram? Does it still match the theoretical χ2 distribution (shown by the curve)? Remember, the theoretical distribution is what we would get assuming that the null hypothesis is true. When the null hypothesis is false, we expect on average larger values of χ2.\nNow that the null hypothesis is false, a hypothesis test gets the correct answer when it rejects H0. The probability that a test correctly rejects a false null hypothesis is called the power of a test. The yellow box on the right tracks the power of the χ2 test. This is not a value that we can easily calculate ahead of time in a real setting. Power should be increased by having larger sample sizes or by having larger true deviations from the null hypothesis in the population. You can explore these features of power by changing the sample size (with the slider at the top right) or by making the difference between the vaccinated and unvaccinated groups larger or smaller.\n\n\n\n10.2 (optional) Collecting some data\nWe’ll do an experiment on ourselves. The point of the experiment needs to remain obscure until after the data is collected, so as to not bias the data collection process.\nAfter this paragraph, is reprinted the last paragraph of Darwin’s Origin of Species. You can download a page with only this one paragraph to print out. (e.g., consider printing a paper version if you are reading this electronically and are able to do so)\nPlease read through this paragraph, and circle every letter “t”. Please proceed at a normal reading speed. If you ever realize that you missed a “t” in a previous word, do not retrace your steps to encircle the “t”. You are not expected to get every “t”, so don’t slow down your reading to get the letter “t”s.\n\nIt is interesting to contemplate an entangled bank, clothed with many plants of many kinds, with birds singing on the bushes, with various insects flitting about, and with worms crawling through the damp earth, and to reflect that these elaborately constructed forms, so different from each other, and dependent on each other in so complex a manner, have all been produced by laws acting around us. These laws, taken in the largest sense, being Growth with Reproduction; inheritance which is almost implied by reproduction; Variability from the indirect and direct action of the external conditions of life, and from use and disuse; a Ratio of Increase so high as to lead to a Struggle for Life, and as a consequence to Natural Selection, entailing Divergence of Character and the Extinction of less-improved forms. Thus, from the war of nature, from famine and death, the most exalted object which we are capable of conceiving, namely, the production of the higher animals, directly follows. There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved.\n\nQuestion number 4 below will return to this exercise. Please don’t read Question 4 until you have completed this procedure."
  },
  {
    "objectID": "lab04-contingency.html#exercises",
    "href": "lab04-contingency.html#exercises",
    "title": "Lab 04 Contingency",
    "section": "11 Exercises",
    "text": "11 Exercises\n\n11.1\nOn April 18, 1940, most of the participants at a church supper in Oswego County, NY, developed gastroenteritis (a.k.a. food poisoning). In what has become a classic of epidemiology, researchers interviewed most of the people at the supper to discover which dishes they ate (CDC-EIS 2003). Their answers are in the file oswego.csv. In this file, the variable ill records whether that person got sick (Y for yes, N for no), and there are also columns for whether each person ate the baked ham, spinach, mashed potatoes, etc., for a total of 14 foods and drinks.\n\nLoad the data set into R from the .csv file.\nUsing table() and chisq.test()$expected, calculate the expected values for a χ2 contingency test of the relationship between fruit salad and illness. Would it be legitimate to use a χ2 contingency analysis to test this association? Why or why not? What test would be best to use?\nYou want to know which food is most strongly associated with illness. For the sake of simplicity, let’s imagine that we have ruled out all the other foods except for spinach, baked ham, vanilla ice cream and chocolate ice cream. Use fisher.test() to calculate an odds ratio for the illness for each these foods. Which is the most likely vehicle of the disease?\nUsing the food you think is the likely vehicle, what is the 95% confidence interval of the odds ratio for illness?\nFor the food you decided in part c is the most likely vehicle, draw a mosaic plot to illustrate how many people got sick as a function of whether they ate this food.\n\n(Researchers later determined that the person who had prepared the specific food that was associated with the gastroenteritis had a Staphylococcus infection, including a lesion on her hand [omg gross!]. The food in question had been left to sit overnight at room temperature, which allowed the Staphylococcus to grow to dangerous numbers before the supper.)\n\n\n\n11.2\nHuman names are often of obscure origin, but many have fairly obvious sources. For example, “Johnson” means “son of John,” “Smith” refers to an occupation, and “Whitlock” means “white-haired” (from “white locks”). In Lancashire, U.K., a fair number of people are named “Shufflebottom,” a name whose origins remain obscure.\nBefore children learn to walk, they move around in a variety of ways, with most infants preferring a particular mode of transportation. Some crawl on hands and knees, some belly crawl commando-style, and some shuffle around on their bottoms.\nA group of researchers decided to ask whether the name “Shufflebottom” might be associated with a propensity to bottom-shuffle. To test this, the compared the frequency of bottom-shufflers among infants with the last name “Shufflebottom” to the frequency of infants named “Walker.” (See Fox et al. 2002.)\nThey found that 9 out of 41 Walkers moved by bottom-shuffling, while 9 out of 43 Shufflebottoms did. You can find these data in the file shufflebottoms.csv.\nWhat is the odds ratio for the association between name and mode of movement? Give a 95% confidence interval for this odds ratio.\nBased on the confidence interval for the odds ratio, would you expect that a hypothesis test would find a significant association between name and movement type? Why or why not?\nIs there a significant difference between the name groups for mode of movement?\n\n\n\n11.3\nFalls are extremely dangerous for the elderly; in fact many deaths are associated with such falls. Some preventative measures are possible, and it would be very useful to have ways to predict which people are at greatest risks for falls.\nOne doctor noticed that some patients stopped walking when they started to talk, and she thought that the reason might be that it is a challenge for these people to do more than one thing at once. She hypothesized that this might be a cue for future risks, such as for falling, and this led to a study of 58 elderly patients in a nursing home (Lundin-Olsson et al. 1997).\nOf these 58 people, 12 stopped walking when talking, while the rest did not. Of the people who stopped walking to talk, 10 had a fall in the next six months. Of the other 46 who did not stop walking to talk, 11 had a fall in that same time period. These data are available in stopping_falls.csv.\nDraw a mosaic plot of the relationship between stopping to talk and falls.\nCarry out an appropriate hypothesis test of the relationship between “stops walking while talking” and falls.\nWhat is the odds ratio of this relationship? Give a 95% confidence interval. Which group has the higher odds of falling?\n\n\n\n11.4\nReturn to the page from the activities section where you circled the letter “t” in the paragraph from Darwin’s Origin of Species. (If you haven’t already done this, please stop reading here now and go back to do that first.)\nThe point of this exercise is to collect data on whether our brains perceive words merely as a collection of letters or if sometimes our brains process words as entities. The logic of this test is that, if words are perceived as units, rather than as collections of letters, then this should be especially true for common words. Hence we will look at the errors made in scanning this paragraph, and ask whether we are more (or less) likely to miss finding a “t” when it is part of a common word.\nCompare your results to the answer key that marks all the instances of the letter “t”. Note that the answer key marks all “t”s in red, but it also puts boxes around some words. The boxes are drawn around all instances of the letter “t” occurring in common words. “Common” is defined here as among the top-twenty words in terms of frequency of use in English; of these six contain one or more “t”s: the, to, it, that, with, and at. In this passage there are 94 “t”s, and 29 are in common words.\nCount how many mistakes you made finding “t”s in common words and in less common words.\nUse the appropriate test to ask whether the commonness of a word affects your probability of noticing the “t”s in it. You can create the table needed for the test by a command like:\ntCountTable = data.frame(Common = c (25, 4), Uncommon = c(55, 10), row.names = c(\"Found\", \"Not found\"))\n\nReplace 25 and 4 with the number of t’s in common words that you found or didn’t find, respectively. Replace 55 and 10 with the numbers of t’s you found or didn’t find in uncommon words."
  },
  {
    "objectID": "lab04-contingency.html#harper-adams-data-science",
    "href": "lab04-contingency.html#harper-adams-data-science",
    "title": "Lab 04 Contingency",
    "section": "Harper Adams Data Science",
    "text": "Harper Adams Data Science\n\nThis module is a part of the MSc in Data Science for Global Agriculture, Food, and Environment at Harper Adams University, led by Ed Harris."
  }
]